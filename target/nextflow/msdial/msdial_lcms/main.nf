// msdial_lcms main_build
// 
// This wrapper script is auto-generated by viash 0.6.6 and is thus a derivative
// work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
// Intuitive.
// 
// The component may contain files which fall under a different license. The
// authors of this component should specify the license in the header of such
// files, or include a separate license file detailing the licenses of all included
// files.
// 
// Component authors:
//  * Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid:
// 0000-0003-3641-729X}

nextflow.enable.dsl=2

// Required imports
import groovy.json.JsonSlurper

// initialise slurper
def jsonSlurper = new JsonSlurper()

// DEFINE CUSTOM CODE

// functionality metadata
thisConfig = processConfig(jsonSlurper.parseText('''{
  "functionality" : {
    "name" : "msdial_lcms",
    "namespace" : "msdial",
    "version" : "main_build",
    "authors" : [
      {
        "name" : "Robrecht Cannoodt",
        "email" : "rcannood@gmail.com",
        "roles" : [
          "maintainer"
        ],
        "props" : {
          "github" : "rcannood",
          "orcid" : "0000-0003-3641-729X"
        }
      }
    ],
    "argument_groups" : [
      {
        "name" : "Inputs",
        "arguments" : [
          {
            "type" : "file",
            "name" : "--input",
            "description" : "One or more input files.",
            "example" : [
              "input1.abf",
              "input2.abf",
              "input3.abf"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : true,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--name",
            "description" : "Sample name, must be of same length as the input files.",
            "example" : [
              "foo",
              "bar",
              "baz"
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--type",
            "description" : "Sample type, must be of same length as the input files.",
            "example" : [
              "Sample"
            ],
            "required" : false,
            "choices" : [
              "Sample",
              "Standard",
              "QC",
              "Blank"
            ],
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--class_id",
            "description" : "Sample class, must be of same length as the input files.",
            "example" : [
              "1",
              "2",
              "3"
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--batch",
            "description" : "Sample batch, must be of same length as the input files.",
            "example" : [
              1,
              2,
              3
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--analytical_order",
            "description" : "Sample analytical order, must be of same length as the input files.",
            "example" : [
              2,
              1,
              3
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--inject_volume",
            "description" : "Sample injection volume in ÂµL, must be of same length as the input files.",
            "example" : [
              1.0,
              0.9,
              1.1
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ";",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Outputs",
        "arguments" : [
          {
            "type" : "file",
            "name" : "--output",
            "description" : "An output directory to store the '*.msdial' outputs.",
            "example" : [
              "output_dir"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : true,
            "direction" : "output",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Data type arguments",
        "arguments" : [
          {
            "type" : "string",
            "name" : "--ms1_data_type",
            "default" : [
              "Profile"
            ],
            "required" : false,
            "choices" : [
              "Centroid",
              "Profile"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--ms2_data_type",
            "default" : [
              "Profile"
            ],
            "required" : false,
            "choices" : [
              "Centroid",
              "Profile"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--ion_mode",
            "default" : [
              "Positive"
            ],
            "required" : false,
            "choices" : [
              "Positive",
              "Negative"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "file",
            "name" : "--dia_file",
            "example" : [
              "path/to/file.dia"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Data correction arguments",
        "arguments" : [
          {
            "type" : "double",
            "name" : "--retention_time_begin",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--retention_time_end",
            "default" : [
              100.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms1_mass_range_begin",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms1_mass_range_end",
            "default" : [
              2000.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms2_mass_range_begin",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms2_mass_range_end",
            "default" : [
              2000.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Centroid arguments",
        "arguments" : [
          {
            "type" : "double",
            "name" : "--ms1_tolerance_for_centroid",
            "default" : [
              0.01
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms2_tolerance_for_centroid",
            "default" : [
              0.025
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Isotope arguments",
        "arguments" : [
          {
            "type" : "integer",
            "name" : "--max_charged_number",
            "default" : [
              2
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Retention time correction arguments",
        "arguments" : [
          {
            "type" : "boolean",
            "name" : "--execute_rt_correction",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--rt_correction_smoothing",
            "description" : "RT correction with smoothing for RT diff.",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--user_setting_intercept",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--rt_diff_calc_method",
            "example" : [
              "SampleMinusSampleAverage"
            ],
            "required" : false,
            "choices" : [
              "SampleMinusSampleAverage",
              "SampleMinusReference"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--extrapolation_method_begin",
            "example" : [
              "UserSetting"
            ],
            "required" : false,
            "choices" : [
              "UserSetting",
              "FirstPoint",
              "LinearExtrapolation"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "string",
            "name" : "--extrapolation_method_end",
            "example" : [
              "lastpoint"
            ],
            "required" : false,
            "choices" : [
              "lastpoint",
              "LinearExtrapolation"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "file",
            "name" : "--istd_file",
            "example" : [
              "path/to/file.istd"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Peak detection arguments",
        "arguments" : [
          {
            "type" : "string",
            "name" : "--smoothing_method",
            "default" : [
              "LinearWeightedMovingAverage"
            ],
            "required" : false,
            "choices" : [
              "SimpleMovingAverage",
              "LinearWeightedMovingAverage",
              "SavitzkyGolayFilter",
              "BinomialFilter"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--smoothing_level",
            "default" : [
              3
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--minimum_peak_width",
            "default" : [
              5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--minimum_peak_height",
            "default" : [
              1000
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--mass_slice_width",
            "default" : [
              0.1
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Deconvolution arguments",
        "arguments" : [
          {
            "type" : "double",
            "name" : "--sigma_window_value",
            "default" : [
              0.5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--amplitude_cutoff",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--exclude_after_precursor",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--keep_isotope_until",
            "default" : [
              0.5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--keep_original_precursor_isotopes",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Adduct arguments",
        "arguments" : [
          {
            "type" : "string",
            "name" : "--adduct_list",
            "example" : [
              "[M+H]+,[M+Na]+,[M+NH4]+"
            ],
            "required" : true,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ",",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Identification arguments",
        "arguments" : [
          {
            "type" : "file",
            "name" : "--msp_file",
            "example" : [
              "file.msp"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--retention_time_tolerance_for_identification",
            "default" : [
              100.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--accurate_ms1_tolerance_for_identification",
            "default" : [
              0.01
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--accurate_ms2_tolerance_for_identification",
            "default" : [
              0.05
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--identification_score_cutoff",
            "default" : [
              80.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--use_retention_information_for_identification_scoring",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--use_retention_information_for_identification_filtering",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Post-identification arguments",
        "arguments" : [
          {
            "type" : "file",
            "name" : "--post_identification_library_file",
            "example" : [
              "file.txt"
            ],
            "must_exist" : true,
            "create_parent" : true,
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--retention_time_tolerance_for_post_identification",
            "default" : [
              0.1
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--accurate_ms1_tolerance_for_post_identification",
            "default" : [
              0.01
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--post_identification_score_cutoff",
            "default" : [
              85.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Alignment arguments",
        "arguments" : [
          {
            "type" : "double",
            "name" : "--retention_time_tolerance_for_alignment",
            "default" : [
              0.05
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms1_tolerance_for_alignment",
            "default" : [
              0.015
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--retention_time_factor_for_alignment",
            "default" : [
              0.5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ms1_factor_for_alignment",
            "default" : [
              0.5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--peak_count_filter",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--gap_filling_by_compulsion",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--alignment_reference_file_id",
            "default" : [
              0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--remove_feature_based_on_peak_height_fold_change",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--pct_detected_in_at_least_one_group",
            "default" : [
              0.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--sample_max_over_blank_average",
            "default" : [
              5.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--sample_average_over_blank_average",
            "default" : [
              5.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--keep_identified_metabolites",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--keep_removable_features",
            "description" : "Keep removable features and assign the tag for checking.",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--replace_true_zero",
            "description" : "Replace true zero values with 1/10 of minimum peak height over all samples",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Isotope tracking arguments",
        "arguments" : [
          {
            "type" : "boolean",
            "name" : "--tracking_isotope_label",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--set_fully_labeled_reference_file",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--non_labeled_reference_id",
            "default" : [
              0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--fully_labeled_reference_id",
            "default" : [
              0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "integer",
            "name" : "--isotope_tracking_dictionary_id",
            "default" : [
              0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Corrdec arguments",
        "arguments" : [
          {
            "type" : "boolean",
            "name" : "--corrdec_execute",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_ms2_tolerance",
            "default" : [
              0.01
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_minimum_ms2_peak_height",
            "default" : [
              1000.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_min_detected_samples",
            "default" : [
              3.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_exclude_highly_correlated_spots",
            "description" : "To ignore highly correlated precursor ions (related ions e.g. adducts and isotopes); 0.8-0.9",
            "default" : [
              0.9
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_min_corr_ms2",
            "description" : "A cutoff value to remove not correlated MS2 peaks",
            "default" : [
              0.7
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_margin_1",
            "description" : "Minimum correlation cutoff value: max correlation (vs. a target precursor) - this value. Criterion 2 in the method paper, it should be published soon.",
            "default" : [
              0.2
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_margin_2",
            "description" : "Minimum correlation cutoff value: max correlation (vs. coeluted precursors) - this value. Criterion 3 in the method paper, it should be published soon.",
            "default" : [
              0.7
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_min_detected_rate",
            "default" : [
              0.5
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--corrdec_min_ms2_relative_intensity",
            "default" : [
              2.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--corrdec_remove_peaks_larger_than_precursor",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      },
      {
        "name" : "Ion mobility arguments",
        "arguments" : [
          {
            "type" : "double",
            "name" : "--accumulated_rt_range",
            "default" : [
              0.2
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--ccs_search_tolerance",
            "default" : [
              10.0
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "double",
            "name" : "--mobility_axis_alignment_tolerance",
            "default" : [
              0.02
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--use_ccs_for_identification_scoring",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          },
          {
            "type" : "boolean",
            "name" : "--use_ccs_for_identification_filtering",
            "default" : [
              true
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":",
            "dest" : "par"
          }
        ]
      }
    ],
    "resources" : [
      {
        "type" : "python_script",
        "path" : "script.py",
        "is_executable" : true,
        "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/msdial/msdial_lcms/config.vsh.yaml"
      }
    ],
    "description" : "MS-DIAL LC/MS",
    "test_resources" : [
      {
        "type" : "bash_script",
        "path" : "run_test.sh",
        "is_executable" : true,
        "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/msdial/msdial_lcms/config.vsh.yaml"
      },
      {
        "type" : "file",
        "path" : "../../../resources_test/msdial_demo_files/raw/LCMS_DDA",
        "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/msdial/msdial_lcms/config.vsh.yaml"
      },
      {
        "type" : "file",
        "path" : "../../../resources_test/msdial_demo_files/raw/LCMS_DIA",
        "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/msdial/msdial_lcms/config.vsh.yaml"
      }
    ],
    "status" : "enabled",
    "set_wd_to_resources_dir" : false
  },
  "platforms" : [
    {
      "type" : "docker",
      "id" : "docker",
      "image" : "python:3.10",
      "target_organization" : "czbiohub/mspipelines",
      "target_registry" : "ghcr.io",
      "namespace_separator" : "_",
      "resolve_volume" : "Automatic",
      "chown" : true,
      "setup_strategy" : "ifneedbepullelsecachedbuild",
      "target_image_source" : "https://github.com/czbiohub/mspipelines",
      "setup" : [
        {
          "type" : "apt",
          "packages" : [
            "libnetcdf-dev"
          ],
          "interactive" : false
        },
        {
          "type" : "docker",
          "run" : [
            "mkdir /tmp/msdial && cd /tmp/msdial && wget -q https://www.dropbox.com/s/6fn2tjfyudbrg3o/foo.zip?dl=1 -O \\"msdial.zip\\" && unzip \\"msdial.zip\\" -d /msdial && rm -rf /tmp/msdial && chmod +x /msdial/MsdialConsoleApp"
          ]
        },
        {
          "type" : "python",
          "user" : false,
          "pypi" : [
            "pandas"
          ],
          "upgrade" : true
        }
      ]
    },
    {
      "type" : "nextflow",
      "id" : "nextflow",
      "variant" : "vdsl3",
      "directives" : {
        "label" : [
          "midmem",
          "midcpu"
        ],
        "tag" : "$id"
      },
      "auto" : {
        "simplifyInput" : true,
        "simplifyOutput" : true,
        "transcript" : false,
        "publish" : false
      },
      "debug" : false,
      "container" : "docker"
    }
  ],
  "info" : {
    "config" : "/home/runner/work/mspipelines/mspipelines/src/msdial/msdial_lcms/config.vsh.yaml",
    "platform" : "nextflow",
    "viash_version" : "0.6.6",
    "git_commit" : "4fef684fb8dcb2578f7009350f834d836d01d90e",
    "git_remote" : "https://github.com/czbiohub/mspipelines"
  }
}'''))

thisScript = '''set -e
tempscript=".viash_script.sh"
cat > "$tempscript" << VIASHMAIN

"""Module to run the msdial algorithm for lcms"""
import os
import tempfile
import shutil
import subprocess
import pandas as pd

MSDIAL_PATH="/msdial"
## VIASH START
# The following code has been auto-generated by Viash.
par = {
  'input': $( if [ ! -z ${VIASH_PAR_INPUT+x} ]; then echo "r'${VIASH_PAR_INPUT//\\'/\\'\\"\\'\\"r\\'}'.split(';')"; else echo None; fi ),
  'name': $( if [ ! -z ${VIASH_PAR_NAME+x} ]; then echo "r'${VIASH_PAR_NAME//\\'/\\'\\"\\'\\"r\\'}'.split(';')"; else echo None; fi ),
  'type': $( if [ ! -z ${VIASH_PAR_TYPE+x} ]; then echo "r'${VIASH_PAR_TYPE//\\'/\\'\\"\\'\\"r\\'}'.split(';')"; else echo None; fi ),
  'class_id': $( if [ ! -z ${VIASH_PAR_CLASS_ID+x} ]; then echo "r'${VIASH_PAR_CLASS_ID//\\'/\\'\\"\\'\\"r\\'}'.split(';')"; else echo None; fi ),
  'batch': $( if [ ! -z ${VIASH_PAR_BATCH+x} ]; then echo "list(map(int, r'${VIASH_PAR_BATCH//\\'/\\'\\"\\'\\"r\\'}'.split(';')))"; else echo None; fi ),
  'analytical_order': $( if [ ! -z ${VIASH_PAR_ANALYTICAL_ORDER+x} ]; then echo "list(map(int, r'${VIASH_PAR_ANALYTICAL_ORDER//\\'/\\'\\"\\'\\"r\\'}'.split(';')))"; else echo None; fi ),
  'inject_volume': $( if [ ! -z ${VIASH_PAR_INJECT_VOLUME+x} ]; then echo "list(map(float, r'${VIASH_PAR_INJECT_VOLUME//\\'/\\'\\"\\'\\"r\\'}'.split(';')))"; else echo None; fi ),
  'output': $( if [ ! -z ${VIASH_PAR_OUTPUT+x} ]; then echo "r'${VIASH_PAR_OUTPUT//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'ms1_data_type': $( if [ ! -z ${VIASH_PAR_MS1_DATA_TYPE+x} ]; then echo "r'${VIASH_PAR_MS1_DATA_TYPE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'ms2_data_type': $( if [ ! -z ${VIASH_PAR_MS2_DATA_TYPE+x} ]; then echo "r'${VIASH_PAR_MS2_DATA_TYPE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'ion_mode': $( if [ ! -z ${VIASH_PAR_ION_MODE+x} ]; then echo "r'${VIASH_PAR_ION_MODE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'dia_file': $( if [ ! -z ${VIASH_PAR_DIA_FILE+x} ]; then echo "r'${VIASH_PAR_DIA_FILE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'retention_time_begin': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_BEGIN+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_BEGIN//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'retention_time_end': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_END+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_END//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms1_mass_range_begin': $( if [ ! -z ${VIASH_PAR_MS1_MASS_RANGE_BEGIN+x} ]; then echo "float(r'${VIASH_PAR_MS1_MASS_RANGE_BEGIN//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms1_mass_range_end': $( if [ ! -z ${VIASH_PAR_MS1_MASS_RANGE_END+x} ]; then echo "float(r'${VIASH_PAR_MS1_MASS_RANGE_END//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms2_mass_range_begin': $( if [ ! -z ${VIASH_PAR_MS2_MASS_RANGE_BEGIN+x} ]; then echo "float(r'${VIASH_PAR_MS2_MASS_RANGE_BEGIN//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms2_mass_range_end': $( if [ ! -z ${VIASH_PAR_MS2_MASS_RANGE_END+x} ]; then echo "float(r'${VIASH_PAR_MS2_MASS_RANGE_END//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms1_tolerance_for_centroid': $( if [ ! -z ${VIASH_PAR_MS1_TOLERANCE_FOR_CENTROID+x} ]; then echo "float(r'${VIASH_PAR_MS1_TOLERANCE_FOR_CENTROID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms2_tolerance_for_centroid': $( if [ ! -z ${VIASH_PAR_MS2_TOLERANCE_FOR_CENTROID+x} ]; then echo "float(r'${VIASH_PAR_MS2_TOLERANCE_FOR_CENTROID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'max_charged_number': $( if [ ! -z ${VIASH_PAR_MAX_CHARGED_NUMBER+x} ]; then echo "int(r'${VIASH_PAR_MAX_CHARGED_NUMBER//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'execute_rt_correction': $( if [ ! -z ${VIASH_PAR_EXECUTE_RT_CORRECTION+x} ]; then echo "r'${VIASH_PAR_EXECUTE_RT_CORRECTION//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'rt_correction_smoothing': $( if [ ! -z ${VIASH_PAR_RT_CORRECTION_SMOOTHING+x} ]; then echo "r'${VIASH_PAR_RT_CORRECTION_SMOOTHING//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'user_setting_intercept': $( if [ ! -z ${VIASH_PAR_USER_SETTING_INTERCEPT+x} ]; then echo "float(r'${VIASH_PAR_USER_SETTING_INTERCEPT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'rt_diff_calc_method': $( if [ ! -z ${VIASH_PAR_RT_DIFF_CALC_METHOD+x} ]; then echo "r'${VIASH_PAR_RT_DIFF_CALC_METHOD//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'extrapolation_method_begin': $( if [ ! -z ${VIASH_PAR_EXTRAPOLATION_METHOD_BEGIN+x} ]; then echo "r'${VIASH_PAR_EXTRAPOLATION_METHOD_BEGIN//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'extrapolation_method_end': $( if [ ! -z ${VIASH_PAR_EXTRAPOLATION_METHOD_END+x} ]; then echo "r'${VIASH_PAR_EXTRAPOLATION_METHOD_END//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'istd_file': $( if [ ! -z ${VIASH_PAR_ISTD_FILE+x} ]; then echo "r'${VIASH_PAR_ISTD_FILE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'smoothing_method': $( if [ ! -z ${VIASH_PAR_SMOOTHING_METHOD+x} ]; then echo "r'${VIASH_PAR_SMOOTHING_METHOD//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'smoothing_level': $( if [ ! -z ${VIASH_PAR_SMOOTHING_LEVEL+x} ]; then echo "int(r'${VIASH_PAR_SMOOTHING_LEVEL//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'minimum_peak_width': $( if [ ! -z ${VIASH_PAR_MINIMUM_PEAK_WIDTH+x} ]; then echo "int(r'${VIASH_PAR_MINIMUM_PEAK_WIDTH//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'minimum_peak_height': $( if [ ! -z ${VIASH_PAR_MINIMUM_PEAK_HEIGHT+x} ]; then echo "int(r'${VIASH_PAR_MINIMUM_PEAK_HEIGHT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'mass_slice_width': $( if [ ! -z ${VIASH_PAR_MASS_SLICE_WIDTH+x} ]; then echo "float(r'${VIASH_PAR_MASS_SLICE_WIDTH//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'sigma_window_value': $( if [ ! -z ${VIASH_PAR_SIGMA_WINDOW_VALUE+x} ]; then echo "float(r'${VIASH_PAR_SIGMA_WINDOW_VALUE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'amplitude_cutoff': $( if [ ! -z ${VIASH_PAR_AMPLITUDE_CUTOFF+x} ]; then echo "float(r'${VIASH_PAR_AMPLITUDE_CUTOFF//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'exclude_after_precursor': $( if [ ! -z ${VIASH_PAR_EXCLUDE_AFTER_PRECURSOR+x} ]; then echo "r'${VIASH_PAR_EXCLUDE_AFTER_PRECURSOR//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'keep_isotope_until': $( if [ ! -z ${VIASH_PAR_KEEP_ISOTOPE_UNTIL+x} ]; then echo "float(r'${VIASH_PAR_KEEP_ISOTOPE_UNTIL//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'keep_original_precursor_isotopes': $( if [ ! -z ${VIASH_PAR_KEEP_ORIGINAL_PRECURSOR_ISOTOPES+x} ]; then echo "r'${VIASH_PAR_KEEP_ORIGINAL_PRECURSOR_ISOTOPES//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'adduct_list': $( if [ ! -z ${VIASH_PAR_ADDUCT_LIST+x} ]; then echo "r'${VIASH_PAR_ADDUCT_LIST//\\'/\\'\\"\\'\\"r\\'}'.split(',')"; else echo None; fi ),
  'msp_file': $( if [ ! -z ${VIASH_PAR_MSP_FILE+x} ]; then echo "r'${VIASH_PAR_MSP_FILE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'retention_time_tolerance_for_identification': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_IDENTIFICATION+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_IDENTIFICATION//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'accurate_ms1_tolerance_for_identification': $( if [ ! -z ${VIASH_PAR_ACCURATE_MS1_TOLERANCE_FOR_IDENTIFICATION+x} ]; then echo "float(r'${VIASH_PAR_ACCURATE_MS1_TOLERANCE_FOR_IDENTIFICATION//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'accurate_ms2_tolerance_for_identification': $( if [ ! -z ${VIASH_PAR_ACCURATE_MS2_TOLERANCE_FOR_IDENTIFICATION+x} ]; then echo "float(r'${VIASH_PAR_ACCURATE_MS2_TOLERANCE_FOR_IDENTIFICATION//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'identification_score_cutoff': $( if [ ! -z ${VIASH_PAR_IDENTIFICATION_SCORE_CUTOFF+x} ]; then echo "float(r'${VIASH_PAR_IDENTIFICATION_SCORE_CUTOFF//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'use_retention_information_for_identification_scoring': $( if [ ! -z ${VIASH_PAR_USE_RETENTION_INFORMATION_FOR_IDENTIFICATION_SCORING+x} ]; then echo "r'${VIASH_PAR_USE_RETENTION_INFORMATION_FOR_IDENTIFICATION_SCORING//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'use_retention_information_for_identification_filtering': $( if [ ! -z ${VIASH_PAR_USE_RETENTION_INFORMATION_FOR_IDENTIFICATION_FILTERING+x} ]; then echo "r'${VIASH_PAR_USE_RETENTION_INFORMATION_FOR_IDENTIFICATION_FILTERING//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'post_identification_library_file': $( if [ ! -z ${VIASH_PAR_POST_IDENTIFICATION_LIBRARY_FILE+x} ]; then echo "r'${VIASH_PAR_POST_IDENTIFICATION_LIBRARY_FILE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'retention_time_tolerance_for_post_identification': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_POST_IDENTIFICATION+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_POST_IDENTIFICATION//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'accurate_ms1_tolerance_for_post_identification': $( if [ ! -z ${VIASH_PAR_ACCURATE_MS1_TOLERANCE_FOR_POST_IDENTIFICATION+x} ]; then echo "float(r'${VIASH_PAR_ACCURATE_MS1_TOLERANCE_FOR_POST_IDENTIFICATION//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'post_identification_score_cutoff': $( if [ ! -z ${VIASH_PAR_POST_IDENTIFICATION_SCORE_CUTOFF+x} ]; then echo "float(r'${VIASH_PAR_POST_IDENTIFICATION_SCORE_CUTOFF//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'retention_time_tolerance_for_alignment': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_ALIGNMENT+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_TOLERANCE_FOR_ALIGNMENT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms1_tolerance_for_alignment': $( if [ ! -z ${VIASH_PAR_MS1_TOLERANCE_FOR_ALIGNMENT+x} ]; then echo "float(r'${VIASH_PAR_MS1_TOLERANCE_FOR_ALIGNMENT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'retention_time_factor_for_alignment': $( if [ ! -z ${VIASH_PAR_RETENTION_TIME_FACTOR_FOR_ALIGNMENT+x} ]; then echo "float(r'${VIASH_PAR_RETENTION_TIME_FACTOR_FOR_ALIGNMENT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ms1_factor_for_alignment': $( if [ ! -z ${VIASH_PAR_MS1_FACTOR_FOR_ALIGNMENT+x} ]; then echo "float(r'${VIASH_PAR_MS1_FACTOR_FOR_ALIGNMENT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'peak_count_filter': $( if [ ! -z ${VIASH_PAR_PEAK_COUNT_FILTER+x} ]; then echo "float(r'${VIASH_PAR_PEAK_COUNT_FILTER//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'gap_filling_by_compulsion': $( if [ ! -z ${VIASH_PAR_GAP_FILLING_BY_COMPULSION+x} ]; then echo "r'${VIASH_PAR_GAP_FILLING_BY_COMPULSION//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'alignment_reference_file_id': $( if [ ! -z ${VIASH_PAR_ALIGNMENT_REFERENCE_FILE_ID+x} ]; then echo "int(r'${VIASH_PAR_ALIGNMENT_REFERENCE_FILE_ID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'remove_feature_based_on_peak_height_fold_change': $( if [ ! -z ${VIASH_PAR_REMOVE_FEATURE_BASED_ON_PEAK_HEIGHT_FOLD_CHANGE+x} ]; then echo "r'${VIASH_PAR_REMOVE_FEATURE_BASED_ON_PEAK_HEIGHT_FOLD_CHANGE//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'pct_detected_in_at_least_one_group': $( if [ ! -z ${VIASH_PAR_PCT_DETECTED_IN_AT_LEAST_ONE_GROUP+x} ]; then echo "float(r'${VIASH_PAR_PCT_DETECTED_IN_AT_LEAST_ONE_GROUP//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'sample_max_over_blank_average': $( if [ ! -z ${VIASH_PAR_SAMPLE_MAX_OVER_BLANK_AVERAGE+x} ]; then echo "float(r'${VIASH_PAR_SAMPLE_MAX_OVER_BLANK_AVERAGE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'sample_average_over_blank_average': $( if [ ! -z ${VIASH_PAR_SAMPLE_AVERAGE_OVER_BLANK_AVERAGE+x} ]; then echo "float(r'${VIASH_PAR_SAMPLE_AVERAGE_OVER_BLANK_AVERAGE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'keep_identified_metabolites': $( if [ ! -z ${VIASH_PAR_KEEP_IDENTIFIED_METABOLITES+x} ]; then echo "r'${VIASH_PAR_KEEP_IDENTIFIED_METABOLITES//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'keep_removable_features': $( if [ ! -z ${VIASH_PAR_KEEP_REMOVABLE_FEATURES+x} ]; then echo "r'${VIASH_PAR_KEEP_REMOVABLE_FEATURES//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'replace_true_zero': $( if [ ! -z ${VIASH_PAR_REPLACE_TRUE_ZERO+x} ]; then echo "r'${VIASH_PAR_REPLACE_TRUE_ZERO//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'tracking_isotope_label': $( if [ ! -z ${VIASH_PAR_TRACKING_ISOTOPE_LABEL+x} ]; then echo "r'${VIASH_PAR_TRACKING_ISOTOPE_LABEL//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'set_fully_labeled_reference_file': $( if [ ! -z ${VIASH_PAR_SET_FULLY_LABELED_REFERENCE_FILE+x} ]; then echo "r'${VIASH_PAR_SET_FULLY_LABELED_REFERENCE_FILE//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'non_labeled_reference_id': $( if [ ! -z ${VIASH_PAR_NON_LABELED_REFERENCE_ID+x} ]; then echo "int(r'${VIASH_PAR_NON_LABELED_REFERENCE_ID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'fully_labeled_reference_id': $( if [ ! -z ${VIASH_PAR_FULLY_LABELED_REFERENCE_ID+x} ]; then echo "int(r'${VIASH_PAR_FULLY_LABELED_REFERENCE_ID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'isotope_tracking_dictionary_id': $( if [ ! -z ${VIASH_PAR_ISOTOPE_TRACKING_DICTIONARY_ID+x} ]; then echo "int(r'${VIASH_PAR_ISOTOPE_TRACKING_DICTIONARY_ID//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_execute': $( if [ ! -z ${VIASH_PAR_CORRDEC_EXECUTE+x} ]; then echo "r'${VIASH_PAR_CORRDEC_EXECUTE//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'corrdec_ms2_tolerance': $( if [ ! -z ${VIASH_PAR_CORRDEC_MS2_TOLERANCE+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MS2_TOLERANCE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_minimum_ms2_peak_height': $( if [ ! -z ${VIASH_PAR_CORRDEC_MINIMUM_MS2_PEAK_HEIGHT+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MINIMUM_MS2_PEAK_HEIGHT//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_min_detected_samples': $( if [ ! -z ${VIASH_PAR_CORRDEC_MIN_DETECTED_SAMPLES+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MIN_DETECTED_SAMPLES//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_exclude_highly_correlated_spots': $( if [ ! -z ${VIASH_PAR_CORRDEC_EXCLUDE_HIGHLY_CORRELATED_SPOTS+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_EXCLUDE_HIGHLY_CORRELATED_SPOTS//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_min_corr_ms2': $( if [ ! -z ${VIASH_PAR_CORRDEC_MIN_CORR_MS2+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MIN_CORR_MS2//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_margin_1': $( if [ ! -z ${VIASH_PAR_CORRDEC_MARGIN_1+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MARGIN_1//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_margin_2': $( if [ ! -z ${VIASH_PAR_CORRDEC_MARGIN_2+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MARGIN_2//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_min_detected_rate': $( if [ ! -z ${VIASH_PAR_CORRDEC_MIN_DETECTED_RATE+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MIN_DETECTED_RATE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_min_ms2_relative_intensity': $( if [ ! -z ${VIASH_PAR_CORRDEC_MIN_MS2_RELATIVE_INTENSITY+x} ]; then echo "float(r'${VIASH_PAR_CORRDEC_MIN_MS2_RELATIVE_INTENSITY//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'corrdec_remove_peaks_larger_than_precursor': $( if [ ! -z ${VIASH_PAR_CORRDEC_REMOVE_PEAKS_LARGER_THAN_PRECURSOR+x} ]; then echo "r'${VIASH_PAR_CORRDEC_REMOVE_PEAKS_LARGER_THAN_PRECURSOR//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'accumulated_rt_range': $( if [ ! -z ${VIASH_PAR_ACCUMULATED_RT_RANGE+x} ]; then echo "float(r'${VIASH_PAR_ACCUMULATED_RT_RANGE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'ccs_search_tolerance': $( if [ ! -z ${VIASH_PAR_CCS_SEARCH_TOLERANCE+x} ]; then echo "float(r'${VIASH_PAR_CCS_SEARCH_TOLERANCE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'mobility_axis_alignment_tolerance': $( if [ ! -z ${VIASH_PAR_MOBILITY_AXIS_ALIGNMENT_TOLERANCE+x} ]; then echo "float(r'${VIASH_PAR_MOBILITY_AXIS_ALIGNMENT_TOLERANCE//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'use_ccs_for_identification_scoring': $( if [ ! -z ${VIASH_PAR_USE_CCS_FOR_IDENTIFICATION_SCORING+x} ]; then echo "r'${VIASH_PAR_USE_CCS_FOR_IDENTIFICATION_SCORING//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi ),
  'use_ccs_for_identification_filtering': $( if [ ! -z ${VIASH_PAR_USE_CCS_FOR_IDENTIFICATION_FILTERING+x} ]; then echo "r'${VIASH_PAR_USE_CCS_FOR_IDENTIFICATION_FILTERING//\\'/\\'\\"\\'\\"r\\'}'.lower() == 'true'"; else echo None; fi )
}
meta = {
  'functionality_name': $( if [ ! -z ${VIASH_META_FUNCTIONALITY_NAME+x} ]; then echo "r'${VIASH_META_FUNCTIONALITY_NAME//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'resources_dir': $( if [ ! -z ${VIASH_META_RESOURCES_DIR+x} ]; then echo "r'${VIASH_META_RESOURCES_DIR//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'executable': $( if [ ! -z ${VIASH_META_EXECUTABLE+x} ]; then echo "r'${VIASH_META_EXECUTABLE//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'config': $( if [ ! -z ${VIASH_META_CONFIG+x} ]; then echo "r'${VIASH_META_CONFIG//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'temp_dir': $( if [ ! -z ${VIASH_META_TEMP_DIR+x} ]; then echo "r'${VIASH_META_TEMP_DIR//\\'/\\'\\"\\'\\"r\\'}'"; else echo None; fi ),
  'cpus': $( if [ ! -z ${VIASH_META_CPUS+x} ]; then echo "int(r'${VIASH_META_CPUS//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_b': $( if [ ! -z ${VIASH_META_MEMORY_B+x} ]; then echo "int(r'${VIASH_META_MEMORY_B//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_kb': $( if [ ! -z ${VIASH_META_MEMORY_KB+x} ]; then echo "int(r'${VIASH_META_MEMORY_KB//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_mb': $( if [ ! -z ${VIASH_META_MEMORY_MB+x} ]; then echo "int(r'${VIASH_META_MEMORY_MB//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_gb': $( if [ ! -z ${VIASH_META_MEMORY_GB+x} ]; then echo "int(r'${VIASH_META_MEMORY_GB//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_tb': $( if [ ! -z ${VIASH_META_MEMORY_TB+x} ]; then echo "int(r'${VIASH_META_MEMORY_TB//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi ),
  'memory_pb': $( if [ ! -z ${VIASH_META_MEMORY_PB+x} ]; then echo "int(r'${VIASH_META_MEMORY_PB//\\'/\\'\\"\\'\\"r\\'}')"; else echo None; fi )
}

## VIASH END

MODE = "lcmsdia" if par["dia_file"] else "lcmsdda"

print(f"Running MS-DIAL {MODE}", flush=True)

assert len(par["input"]) > 0, "Need to specify at least one --input."

# Create input csv file
csv_vars = {
  'file_path': 'input',
  'file_name': 'name',
  'type': 'type',
  'class_id': 'class_id',
  'batch': 'batch',
  'analytical_order': 'analytical_order',
  'inject_volume': 'inject_volume',
}

csv_file = os.path.join(par["output"], "input.csv")
for par_key, par_name in csv_vars.items():
    assert par.get(par_name) is None or len(par["input"]) == len(par[par_name]), f"--{par_name} should be of same length as --input"

# Create params file
param_file = os.path.join(par["output"], "params.txt")
ri_index_file = os.path.join(par["output"], "ri_index_paths.txt")

param_content = f"""# Data type
Ms1 data type: {par["ms1_data_type"]}
Ms2 data type: {par["ms2_data_type"]}
Ion mode: {par["ion_mode"]}
{ "Dia file: " + par["dia_file"] if par["dia_file"] else "# Dia file: none"}

# Data correction parameters
Retention time begin: {par["retention_time_begin"]}
Retention time end: {par["retention_time_end"]}
Mass range begin: {par["ms1_mass_range_begin"]}
Mass range end: {par["ms1_mass_range_end"]}
Ms2 mass range begin: {par["ms2_mass_range_begin"]}
Ms2 mass range end: {par["ms2_mass_range_end"]}

# Centroid arguments
ms1 tolerance for centroid: {par["ms1_tolerance_for_centroid"]}
ms2 tolerance for centroid: {par["ms2_tolerance_for_centroid"]}

# Isotope arguments
maximum charged number: {par["max_charged_number"]}

# Retention time correction arguments
excute rt correction: {par["execute_rt_correction"]}
rt correction with smoothing for rt diff: {par["rt_correction_smoothing"]}
user setting intercept: {par["user_setting_intercept"]}
{ "rd diff calc method: " + par["rt_diff_calc_method"] if par["rt_diff_calc_method"] else "# rd diff calc method: none"}
{ "extrapolation method (begin): " + par["extrapolation_method_begin"] if par["extrapolation_method_begin"] else "# extrapolation method (begin): none"}
{ "extrapolation method (end): " + par["extrapolation_method_end"] if par["extrapolation_method_end"] else "# extrapolation method (end): "}
{ "istd file: " + par["istd_file"] if par["istd_file"] else "# istd file: none"}

# Peak detection parameters
Smoothing method: {par["smoothing_method"]}
Smoothing level: {par["smoothing_level"]}
Minimum peak width: {par["minimum_peak_width"]}
Minimum peak height: {par["minimum_peak_height"]}
Mass slice width: {par["mass_slice_width"]}

# Deconvolution parameters
Sigma window value: {par["sigma_window_value"]}
Amplitude cut off: {par["amplitude_cutoff"]}
exclude after precursor: {par["exclude_after_precursor"]}
keep isotope until: {par["amplitude_cutoff"]}
keep original precursor isotopes: {par["amplitude_cutoff"]}

# Adduct list
Adduct list: {','.join(par["adduct_list"])}

# Identification
{"MSP file: " + par["msp_file"] if par["msp_file"] else "# MSP file: none"}
Retention time tolerance for identification: {par["retention_time_tolerance_for_identification"]}
accurate ms1 tolerance for identification: {par["accurate_ms1_tolerance_for_identification"]}
accurate ms2 tolerance for identification: {par["accurate_ms2_tolerance_for_identification"]}
Identification score cut off: {par["identification_score_cutoff"]}
Use retention information for identification scoring: {par["use_retention_information_for_identification_scoring"]}
Use retention information for identification filtering: {par["use_retention_information_for_identification_filtering"]}

# Post identification
{ "text file: " + par["post_identification_library_file"] if par["post_identification_library_file"] else "# text file: none"}
retention time tolerance for post identification: {par["retention_time_tolerance_for_post_identification"]}
accurate ms1 tolerance for post identification: {par["accurate_ms1_tolerance_for_post_identification"]}
post identification score cut off: {par["post_identification_score_cutoff"]}

# Alignment arguments
Retention time tolerance for alignment: {par["retention_time_tolerance_for_alignment"]}
Ms1 tolerance for alignment: {par["ms1_tolerance_for_alignment"]}
Retention time factor for alignment: {par["retention_time_factor_for_alignment"]}
Ms1 factor for alignment: {par["ms1_factor_for_alignment"]}
Peak count filter: {par["peak_count_filter"]}
Gap filling by compulsion: {par["gap_filling_by_compulsion"]}
alignment reference file id: {par["alignment_reference_file_id"]}
Remove feature based on peak height fold-change: {par["remove_feature_based_on_peak_height_fold_change"]}
N% detected in at least one group: {par["pct_detected_in_at_least_one_group"]}
Sample max / blank average: {par["sample_max_over_blank_average"]}
Sample average / blank average: {par["sample_average_over_blank_average"]}
Keep identified and annotated metabolites: {par["keep_identified_metabolites"]}
Keep removable features and assign the tag for checking: {par["keep_removable_features"]}
Replace true zero values with 1/10 of minimum peak height over all samples: {par["replace_true_zero"]}

# isotope tracking arguments
tracking isotope label: {par["tracking_isotope_label"]}
set fully labeled reference file: {par["set_fully_labeled_reference_file"]}
non labeled reference id: {par["non_labeled_reference_id"]}
fully labeled reference id: {par["fully_labeled_reference_id"]}
isotope tracking dictionary id: {par["isotope_tracking_dictionary_id"]}

# corrdec arguments
corrdec excute: {par["corrdec_execute"]}
corrdec ms2 tolerance: {par["corrdec_ms2_tolerance"]}
corrdec minimum ms2 peak height: {par["corrdec_minimum_ms2_peak_height"]}
corrdec minimum number of detected samples: {par["corrdec_min_detected_samples"]}
corrdec exclude highly correlated spots: {par["corrdec_exclude_highly_correlated_spots"]}
corrdec minimum correlation coefficient (ms2): {par["corrdec_min_corr_ms2"]}
corrdec margin 1 (target precursor): {par["corrdec_margin_1"]}
corrdec margin 2 (coeluted precursor): {par["corrdec_margin_2"]}
corrdec minimum detected rate: {par["corrdec_min_detected_rate"]}
corrdec minimum ms2 relative intensity: {par["corrdec_min_ms2_relative_intensity"]}
corrdec remove peaks larger than precursor: {par["corrdec_remove_peaks_larger_than_precursor"]}

# ion mobility arguments
accumulated rt ragne: {par["accumulated_rt_range"]}
ccs search tolerance: {par["ccs_search_tolerance"]}
mobility axis alignment tolerance: {par["mobility_axis_alignment_tolerance"]}
use ccs for identification scoring: {par["use_ccs_for_identification_scoring"]}
use ccs for identification filtering: {par["use_ccs_for_identification_filtering"]}
"""

with tempfile.TemporaryDirectory() as temp_dir:
    # copy input files to tempdir
    # because MSDial otherwise generates a lot
    # of temporary files in the input dir.
    sources = par["input"]
    dests = [ os.path.join(temp_dir, os.path.basename(file)) for file in par["input"] ]

    for src,dst in zip(par["input"], dests):
        print(f"Copying {src} to {dst}", flush=True)
        shutil.copyfile(src, dst)

    par["input"] = dests

    # create output dir if not exists
    if not os.path.exists(par["output"]):
        os.makedirs(par["output"])
    # write input csv file
    data = {new: par[key] for new, key in csv_vars.items() if par.get(key) is not None}
    data_df = pd.DataFrame(data)
    data_df.to_csv(csv_file, index=False)

    # write params file
    with open(param_file, "w",encoding='utf-8') as f:
        f.write(param_content)

    # run msdial
    args =  [
            f"{MSDIAL_PATH}/MsdialConsoleApp",
            MODE,
            "-i", csv_file,
            "-o", par["output"],
            "-m", param_file,
            "-p"
        ]
    with subprocess.Popen(args) as p:
        p.wait()

if p.returncode != 0:
    raise Exception(f"MS-DIAL finished with exit code {p.returncode}")

VIASHMAIN
python "$tempscript"
'''

thisDefaultProcessArgs = [
  // key to be used to trace the process and determine output names
  key: thisConfig.functionality.name,
  // fixed arguments to be passed to script
  args: [:],
  // default directives
  directives: jsonSlurper.parseText('''{
  "container" : {
    "registry" : "ghcr.io",
    "image" : "czbiohub/mspipelines/msdial_msdial_lcms",
    "tag" : "main_build"
  },
  "label" : [
    "midmem",
    "midcpu"
  ],
  "tag" : "$id"
}'''),
  // auto settings
  auto: jsonSlurper.parseText('''{
  "simplifyInput" : true,
  "simplifyOutput" : true,
  "transcript" : false,
  "publish" : false
}'''),
  // apply a map over the incoming tuple
  // example: { tup -> [ tup[0], [input: tup[1].output], tup[2] ] }
  map: null,
  // apply a map over the ID element of a tuple (i.e. the first element)
  // example: { id -> id + "_foo" }
  mapId: null,
  // apply a map over the data element of a tuple (i.e. the second element)
  // example: { data -> [ input: data.output ] }
  mapData: null,
  // apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements)
  // example: { pt -> pt.drop(1) }
  mapPassthrough: null,
  // filter the channel
  // example: { tup -> tup[0] == "foo" }
  filter: null,
  // rename keys in the data field of the tuple (i.e. the second element)
  // example: [ "new_key": "old_key" ]
  renameKeys: null,
  // whether or not to print debug messages
  debug: false
]

// END CUSTOM CODE

/////////////////////////////////////
// Viash Workflow helper functions //
/////////////////////////////////////

import java.util.regex.Pattern
import java.io.BufferedReader
import java.io.FileReader
import java.nio.file.Paths
import groovy.json.JsonSlurper
import groovy.text.SimpleTemplateEngine
import org.yaml.snakeyaml.Yaml

// param helpers //
def paramExists(name) {
  return params.containsKey(name) && params[name] != ""
}

def assertParamExists(name, description) {
  if (!paramExists(name)) {
    exit 1, "ERROR: Please provide a --${name} parameter ${description}"
  }
}

// helper functions for reading params from file //
def getChild(parent, child) {
  if (child.contains("://") || Paths.get(child).isAbsolute()) {
    child
  } else {
    parent.replaceAll('/[^/]*$', "/") + child
  }
}

def readCsv(file) {
  def output = []
  def inputFile = file !instanceof File ? new File(file) : file

  // todo: allow escaped quotes in string
  // todo: allow single quotes?
  def splitRegex = Pattern.compile(''',(?=(?:[^"]*"[^"]*")*[^"]*$)''')
  def removeQuote = Pattern.compile('''"(.*)"''')

  def br = new BufferedReader(new FileReader(inputFile))

  def row = -1
  def header = null
  while (br.ready() && header == null) {
    def line = br.readLine()
    row++
    if (!line.startsWith("#")) {
      header = splitRegex.split(line, -1).collect{field ->
        m = removeQuote.matcher(field)
        m.find() ? m.replaceFirst('$1') : field
      }
    }
  }
  assert header != null: "CSV file should contain a header"

  while (br.ready()) {
    def line = br.readLine()
    row++
    if (!line.startsWith("#")) {
      def predata = splitRegex.split(line, -1)
      def data = predata.collect{field ->
        if (field == "") {
          return null
        }
        m = removeQuote.matcher(field)
        if (m.find()) {
          return m.replaceFirst('$1')
        } else {
          return field
        }
      }
      assert header.size() == data.size(): "Row $row should contain the same number as fields as the header"
      
      def dataMap = [header, data].transpose().collectEntries().findAll{it.value != null}
      output.add(dataMap)
    }
  }

  output
}

def readJsonBlob(str) {
  def jsonSlurper = new JsonSlurper()
  jsonSlurper.parseText(str)
}

def readJson(file) {
  def inputFile = file !instanceof File ? new File(file) : file
  def jsonSlurper = new JsonSlurper()
  jsonSlurper.parse(inputFile)
}

def readYamlBlob(str) {
  def yamlSlurper = new Yaml()
  yamlSlurper.load(str)
}

def readYaml(file) {
  def inputFile = file !instanceof File ? new File(file) : file
  def yamlSlurper = new Yaml()
  yamlSlurper.load(inputFile)
}

// helper functions for reading a viash config in groovy //

// based on how Functionality.scala is implemented
def processArgument(arg) {
  arg.multiple = arg.multiple ?: false
  arg.required = arg.required ?: false
  arg.direction = arg.direction ?: "input"
  arg.multiple_sep = arg.multiple_sep ?: ":"
  arg.plainName = arg.name.replaceAll("^-*", "")

  if (arg.type == "file") {
    arg.must_exist = arg.must_exist ?: true
    arg.create_parent = arg.create_parent ?: true
  }

  if (arg.type == "file" && arg.direction == "output") {
    def mult = arg.multiple ? "_*" : ""
    def extSearch = ""
    if (arg.default != null) {
      extSearch = arg.default
    } else if (arg.example != null) {
      extSearch = arg.example
    }
    if (extSearch instanceof List) {
      extSearch = extSearch[0]
    }
    def ext = extSearch.find("\\.[^\\.]+\$") ?: ""
    arg.default = "\$id.\$key.${arg.plainName}${mult}${ext}"
  }

  if (!arg.multiple) {
    if (arg.default != null && arg.default instanceof List) {
      arg.default = arg.default[0]
    }
    if (arg.example != null && arg.example instanceof List) {
      arg.example = arg.example[0]
    }
  }

  if (arg.type == "boolean_true") {
    arg.default = false
  }
  if (arg.type == "boolean_false") {
    arg.default = true
  }

  arg
}

// based on how Functionality.scala is implemented
def processArgumentGroup(argumentGroups, name, arguments) {
  def argNamesInGroups = argumentGroups.collectMany{it.arguments.findAll{it instanceof String}}.toSet()

  // Check if 'arguments' is in 'argumentGroups'. 
  def argumentsNotInGroup = arguments.findAll{arg -> !(argNamesInGroups.contains(arg.plainName))}

  // Check whether an argument group of 'name' exists.
  def existing = argumentGroups.find{gr -> name == gr.name}

  // if there are no arguments missing from the argument group, just return the existing group (if any)
  if (argumentsNotInGroup.isEmpty()) {
    return existing == null ? [] : [existing]
  
  // if there are missing arguments and there is an existing group, add the missing arguments to it
  } else if (existing != null) {
    def newEx = existing.clone()
    newEx.arguments.addAll(argumentsNotInGroup.findAll{it !instanceof String})
    return [newEx]

  // else create a new group
  } else {
    def newEx = [name: name, arguments: argumentsNotInGroup.findAll{it !instanceof String}]
    return [newEx]
  }
}

// based on how Functionality.scala is implemented
def processConfig(config) {
  // TODO: assert .functionality etc.
  if (config.functionality.inputs) {
    System.err.println("Warning: .functionality.inputs is deprecated. Please use .functionality.arguments instead.")
  }
  if (config.functionality.outputs) {
    System.err.println("Warning: .functionality.outputs is deprecated. Please use .functionality.arguments instead.")
  }

  // set defaults for inputs
  config.functionality.inputs = 
    (config.functionality.inputs ?: []).collect{arg ->
      arg.type = arg.type ?: "file"
      arg.direction = "input"
      processArgument(arg)
    }
  // set defaults for outputs
  config.functionality.outputs = 
    (config.functionality.outputs ?: []).collect{arg ->
      arg.type = arg.type ?: "file"
      arg.direction = "output"
      processArgument(arg)
    }
  // set defaults for arguments
  config.functionality.arguments = 
    (config.functionality.arguments ?: []).collect{arg ->
      processArgument(arg)
    }
  // set defaults for argument_group arguments
  config.functionality.argument_groups =
    (config.functionality.argument_groups ?: []).collect{grp ->
      grp.arguments = (grp.arguments ?: []).collect{arg ->
        arg instanceof String ? arg.replaceAll("^-*", "") : processArgument(arg)
      }
      grp
    }

  // create combined arguments list
  config.functionality.allArguments = 
    config.functionality.inputs +
    config.functionality.outputs +
    config.functionality.arguments +
    config.functionality.argument_groups.collectMany{ group ->
      group.arguments.findAll{ it !instanceof String }
    }
  
  // add missing argument groups (based on Functionality::allArgumentGroups())
  def argGroups = config.functionality.argument_groups
  def inputGroup = processArgumentGroup(argGroups, "Inputs", config.functionality.inputs)
  def outputGroup = processArgumentGroup(argGroups, "Outputs", config.functionality.outputs)
  def defaultGroup = processArgumentGroup(argGroups, "Arguments", config.functionality.arguments)
  def groupsFiltered = argGroups.findAll(gr -> !(["Inputs", "Outputs", "Arguments"].contains(gr.name)))
  config.functionality.allArgumentGroups = inputGroup + outputGroup + defaultGroup + groupsFiltered

  config
}

def readConfig(file) {
  def config = readYaml(file ?: "$projectDir/config.vsh.yaml")
  processConfig(config)
}

// recursively merge two maps
def mergeMap(Map lhs, Map rhs) {
  return rhs.inject(lhs.clone()) { map, entry ->
    if (map[entry.key] instanceof Map && entry.value instanceof Map) {
      map[entry.key] = mergeMap(map[entry.key], entry.value)
    } else if (map[entry.key] instanceof Collection && entry.value instanceof Collection) {
      map[entry.key] += entry.value
    } else {
      map[entry.key] = entry.value
    }
    return map
  }
}

def addGlobalParams(config) {
  def localConfig = [
    "functionality" : [
      "argument_groups": [
        [
          "name": "Nextflow input-output arguments",
          "description": "Input/output parameters for Nextflow itself. Please note that both publishDir and publish_dir are supported but at least one has to be configured.",
          "arguments" : [
            [
              'name': '--publish_dir',
              'required': true,
              'type': 'string',
              'description': 'Path to an output directory.',
              'example': 'output/',
              'multiple': false
            ],
            [
              'name': '--param_list',
              'required': false,
              'type': 'string',
              'description': '''Allows inputting multiple parameter sets to initialise a Nextflow channel. A `param_list` can either be a list of maps, a csv file, a json file, a yaml file, or simply a yaml blob.
              |
              |* A list of maps (as-is) where the keys of each map corresponds to the arguments of the pipeline. Example: in a `nextflow.config` file: `param_list: [ ['id': 'foo', 'input': 'foo.txt'], ['id': 'bar', 'input': 'bar.txt'] ]`.
              |* A csv file should have column names which correspond to the different arguments of this pipeline. Example: `--param_list data.csv` with columns `id,input`.
              |* A json or a yaml file should be a list of maps, each of which has keys corresponding to the arguments of the pipeline. Example: `--param_list data.json` with contents `[ {'id': 'foo', 'input': 'foo.txt'}, {'id': 'bar', 'input': 'bar.txt'} ]`.
              |* A yaml blob can also be passed directly as a string. Example: `--param_list "[ {'id': 'foo', 'input': 'foo.txt'}, {'id': 'bar', 'input': 'bar.txt'} ]"`.
              |
              |When passing a csv, json or yaml file, relative path names are relativized to the location of the parameter file. No relativation is performed when `param_list` is a list of maps (as-is) or a yaml blob.'''.stripMargin(),
              'example': 'my_params.yaml',
              'multiple': false,
              'hidden': true
            ],
          ]
        ]
      ]
    ]
  ]

  return processConfig(mergeMap(config, localConfig))
}

// helper functions for generating help // 

// based on io.viash.helpers.Format.wordWrap
def formatWordWrap(str, maxLength) {
  def words = str.split("\\s").toList()

  def word = null
  def line = ""
  def lines = []
  while(!words.isEmpty()) {
    word = words.pop()
    if (line.length() + word.length() + 1 <= maxLength) {
      line = line + " " + word
    } else {
      lines.add(line)
      line = word
    }
    if (words.isEmpty()) {
      lines.add(line)
    }
  }
  return lines
}

// based on Format.paragraphWrap
def paragraphWrap(str, maxLength) {
  def outLines = []
  str.split("\n").each{par ->
    def words = par.split("\\s").toList()

    def word = null
    def line = words.pop()
    while(!words.isEmpty()) {
      word = words.pop()
      if (line.length() + word.length() + 1 <= maxLength) {
        line = line + " " + word
      } else {
        outLines.add(line)
        line = word
      }
    }
    if (words.isEmpty()) {
      outLines.add(line)
    }
  }
  return outLines
}

def generateArgumentHelp(param) {
  // alternatives are not supported
  // def names = param.alternatives ::: List(param.name)

  def unnamedProps = [
    ["required parameter", param.required],
    ["multiple values allowed", param.multiple],
    ["output", param.direction.toLowerCase() == "output"],
    ["file must exist", param.type == "file" && param.must_exist]
  ].findAll{it[1]}.collect{it[0]}
  
  def dflt = null
  if (param.default != null) {
    if (param.default instanceof List) {
      dflt = param.default.join(param.multiple_sep ?: ", ")
    } else {
      dflt = param.default.toString()
    }
  }
  def example = null
  if (param.example != null) {
    if (param.example instanceof List) {
      example = param.example.join(param.multiple_sep ?: ", ")
    } else {
      example = param.example.toString()
    }
  }
  def min = param.min?.toString()
  def max = param.max?.toString()

  def escapeChoice = { choice ->
    def s1 = choice.replaceAll("\\n", "\\\\n")
    def s2 = s1.replaceAll("\"", """\\\"""")
    s2.contains(",") || s2 != choice ? "\"" + s2 + "\"" : s2
  }
  def choices = param.choices == null ? 
    null : 
    "[ " + param.choices.collect{escapeChoice(it.toString())}.join(", ") + " ]"

  def namedPropsStr = [
    ["type", ([param.type] + unnamedProps).join(", ")],
    ["default", dflt],
    ["example", example],
    ["choices", choices],
    ["min", min],
    ["max", max]
  ]
    .findAll{it[1]}
    .collect{"\n        " + it[0] + ": " + it[1].replaceAll("\n", "\\n")}
    .join("")
  
  def descStr = param.description == null ?
    "" :
    paragraphWrap("\n" + param.description.trim(), 80 - 8).join("\n        ")
  
  "\n    --" + param.plainName +
    namedPropsStr +
    descStr
}

// Based on Helper.generateHelp() in Helper.scala
def generateHelp(config) {
  def fun = config.functionality

  // PART 1: NAME AND VERSION
  def nameStr = fun.name + 
    (fun.version == null ? "" : " " + fun.version)

  // PART 2: DESCRIPTION
  def descrStr = fun.description == null ? 
    "" :
    "\n\n" + paragraphWrap(fun.description.trim(), 80).join("\n")

  // PART 3: Usage
  def usageStr = fun.usage == null ? 
    "" :
    "\n\nUsage:\n" + fun.usage.trim()

  // PART 4: Options
  def argGroupStrs = fun.allArgumentGroups.collect{argGroup ->
    def name = argGroup.name
    def descriptionStr = argGroup.description == null ?
      "" :
      "\n    " + paragraphWrap(argGroup.description.trim(), 80-4).join("\n    ") + "\n"
    def arguments = argGroup.arguments.collect{arg -> 
      arg instanceof String ? fun.allArguments.find{it.plainName == arg} : arg
    }.findAll{it != null}
    def argumentStrs = arguments.collect{param -> generateArgumentHelp(param)}
    
    "\n\n$name:" +
      descriptionStr +
      argumentStrs.join("\n")
  }

  // FINAL: combine
  def out = nameStr + 
    descrStr +
    usageStr + 
    argGroupStrs.join("")

  return out
}

def helpMessage(config) {
  if (paramExists("help")) {
    def mergedConfig = addGlobalParams(config)
    def helpStr = generateHelp(mergedConfig)
    println(helpStr)
    exit 0
  }
}

def guessMultiParamFormat(params) {
  if (!params.containsKey("param_list") || params.param_list == null) {
    "none"
  } else {
    def param_list = params.param_list

    if (param_list !instanceof String) {
      "asis"
    } else if (param_list.endsWith(".csv")) {
      "csv"
    } else if (param_list.endsWith(".json") || param_list.endsWith(".jsn")) {
      "json"
    } else if (param_list.endsWith(".yaml") || param_list.endsWith(".yml")) {
      "yaml"
    } else {
      "yaml_blob"
    }
  }
}

def paramsToList(params, config) {
  // fetch default params from functionality
  def defaultArgs = config.functionality.allArguments
    .findAll { it.containsKey("default") }
    .collectEntries { [ it.plainName, it.default ] }

  // fetch overrides in params
  def paramArgs = config.functionality.allArguments
    .findAll { params.containsKey(it.plainName) }
    .collectEntries { [ it.plainName, params[it.plainName] ] }
  
  // check multi input params
  // objects should be closures and not functions, thanks to FunctionDef
  def multiParamFormat = guessMultiParamFormat(params)

  def multiOptionFunctions = [ 
    "csv": {[it, readCsv(it)]},
    "json": {[it, readJson(it)]},
    "yaml": {[it, readYaml(it)]},
    "yaml_blob": {[null, readYamlBlob(it)]},
    "asis": {[null, it]},
    "none": {[null, [[:]]]}
  ]
  assert multiOptionFunctions.containsKey(multiParamFormat): 
    "Format of provided --param_list not recognised.\n" +
    "You can use '--param_list_format' to manually specify the format.\n" +
    "Found: '$multiParamFormat'. Expected: one of 'csv', 'json', 'yaml', 'yaml_blob', 'asis' or 'none'"

  // fetch multi param inputs
  def multiOptionFun = multiOptionFunctions.get(multiParamFormat)
  // todo: add try catch
  def multiOptionOut = multiOptionFun(params.containsKey("param_list") ? params.param_list : "")
  def paramList = multiOptionOut[1]
  def multiFile = multiOptionOut[0]

  // data checks
  assert paramList instanceof List: "--param_list should contain a list of maps"
  for (value in paramList) {
    assert value instanceof Map: "--param_list should contain a list of maps"
  }
  
  // combine parameters
  def processedParams = paramList.collect{ multiParam ->
    // combine params
    def combinedArgs = defaultArgs + paramArgs + multiParam

    if (workflow.stubRun) {
      // if stub run, explicitly add an id if missing
      combinedArgs = [id: "stub"] + combinedArgs
    } else {
      // else check whether required arguments exist
      config.functionality.allArguments
        .findAll { it.required }
        .forEach { par ->
          assert combinedArgs.containsKey(par.plainName): "Argument ${par.plainName} is required but does not have a value"
        }
    }
    
    // process arguments
    def inputs = config.functionality.allArguments
      .findAll{ par -> combinedArgs.containsKey(par.plainName) }
      .collectEntries { par ->
        // split on 'multiple_sep'
        if (par.multiple) {
          parData = combinedArgs[par.plainName]
          if (parData instanceof List) {
            parData = parData.collect{it instanceof String ? it.split(par.multiple_sep) : it }
          } else if (parData instanceof String) {
            parData = parData.split(par.multiple_sep)
          } else if (parData == null) {
            parData = []
          } else {
            parData = [ parData ]
          }
        } else {
          parData = [ combinedArgs[par.plainName] ]
        }

        // flatten
        parData = parData.flatten()

        // cast types
        if (par.type == "file" && ((par.direction ?: "input") == "input")) {
          parData = parData.collect{path ->
            if (path !instanceof String) {
              path
            } else if (multiFile) {
              file(getChild(multiFile, path))
            } else {
              file(path)
            }
          }.flatten()
        } else if (par.type == "integer") {
          parData = parData.collect{it as Integer}
        } else if (par.type == "double") {
          parData = parData.collect{it as Double}
        } else if (par.type == "boolean" || par.type == "boolean_true" || par.type == "boolean_false") {
          parData = parData.collect{it as Boolean}
        }
        // simplify list to value if need be
        if (!par.multiple) {
          assert parData.size() == 1 : 
            "Error: argument ${par.plainName} has too many values.\n" +
            "  Expected amount: 1. Found: ${parData.size()}"
          parData = parData[0]
        }

        // return pair
        [ par.plainName, parData ]
      }
      // remove parameters which were explicitly set to null
      .findAll{ par -> par != null }
    }
    
  
  // check processed params
  processedParams.forEach { args ->
    assert args.containsKey("id"): "Each argument set should have an 'id'. Argument set: $args"
  }
  def ppIds = processedParams.collect{it.id}
  assert ppIds.size() == ppIds.unique().size() : "All argument sets should have unique ids. Detected ids: $ppIds"

  processedParams
}

def paramsToChannel(params, config) {
  Channel.fromList(paramsToList(params, config))
}

def viashChannel(params, config) {
  paramsToChannel(params, config)
    | map{tup -> [tup.id, tup]}
}

////////////////////////////
// VDSL3 helper functions //
////////////////////////////

import nextflow.Nextflow
import nextflow.script.IncludeDef
import nextflow.script.ScriptBinding
import nextflow.script.ScriptMeta
import nextflow.script.ScriptParser

// retrieve resourcesDir here to make sure the correct path is found
resourcesDir = ScriptMeta.current().getScriptPath().getParent()

def assertMapKeys(map, expectedKeys, requiredKeys, mapName) {
  assert map instanceof Map : "Expected argument '$mapName' to be a Map. Found: class ${map.getClass()}"
  map.forEach { key, val -> 
    assert key in expectedKeys : "Unexpected key '$key' in ${mapName ? mapName + " " : ""}map"
  }
  requiredKeys.forEach { requiredKey -> 
    assert map.containsKey(requiredKey) : "Missing required key '$key' in ${mapName ? mapName + " " : ""}map"
  }
}

// TODO: unit test processDirectives
def processDirectives(Map drctv) {
  // remove null values
  drctv = drctv.findAll{k, v -> v != null}

  /* DIRECTIVE accelerator
    accepted examples:
    - [ limit: 4, type: "nvidia-tesla-k80" ]
  */
  if (drctv.containsKey("accelerator")) {
    assertMapKeys(drctv["accelerator"], ["type", "limit", "request", "runtime"], [], "accelerator")
  }

  /* DIRECTIVE afterScript
    accepted examples:
    - "source /cluster/bin/cleanup"
  */
  if (drctv.containsKey("afterScript")) {
    assert drctv["afterScript"] instanceof CharSequence
  }

  /* DIRECTIVE beforeScript
    accepted examples:
    - "source /cluster/bin/setup"
  */
  if (drctv.containsKey("beforeScript")) {
    assert drctv["beforeScript"] instanceof CharSequence
  }

  /* DIRECTIVE cache
    accepted examples:
    - true
    - false
    - "deep"
    - "lenient"
  */
  if (drctv.containsKey("cache")) {
    assert drctv["cache"] instanceof CharSequence || drctv["cache"] instanceof Boolean
    if (drctv["cache"] instanceof CharSequence) {
      assert drctv["cache"] in ["deep", "lenient"] : "Unexpected value for cache"
    }
  }

  /* DIRECTIVE conda
    accepted examples:
    - "bwa=0.7.15"
    - "bwa=0.7.15 fastqc=0.11.5"
    - ["bwa=0.7.15", "fastqc=0.11.5"]
  */
  if (drctv.containsKey("conda")) {
    if (drctv["conda"] instanceof List) {
      drctv["conda"] = drctv["conda"].join(" ")
    }
    assert drctv["conda"] instanceof CharSequence
  }

  /* DIRECTIVE container
    accepted examples:
    - "foo/bar:tag"
    - [ registry: "reg", image: "im", tag: "ta" ]
      is transformed to "reg/im:ta"
    - [ image: "im" ] 
      is transformed to "im:latest"
  */
  if (drctv.containsKey("container")) {
    assert drctv["container"] instanceof Map || drctv["container"] instanceof CharSequence
    if (drctv["container"] instanceof Map) {
      def m = drctv["container"]
      assertMapKeys(m, [ "registry", "image", "tag" ], ["image"], "container")
      def part1 = 
        System.getenv('OVERRIDE_CONTAINER_REGISTRY') ? System.getenv('OVERRIDE_CONTAINER_REGISTRY') + "/" : 
        params.containsKey("override_container_registry") ? params["override_container_registry"] + "/" : // todo: remove?
        m.registry ? m.registry + "/" : 
        ""
      def part2 = m.image
      def part3 = m.tag ? ":" + m.tag : ":latest"
      drctv["container"] = part1 + part2 + part3
    }
  }

  /* DIRECTIVE containerOptions
    accepted examples:
    - "--foo bar"
    - ["--foo bar", "-f b"]
  */
  if (drctv.containsKey("containerOptions")) {
    if (drctv["containerOptions"] instanceof List) {
      drctv["containerOptions"] = drctv["containerOptions"].join(" ")
    }
    assert drctv["containerOptions"] instanceof CharSequence
  }

  /* DIRECTIVE cpus
    accepted examples:
    - 1
    - 10
  */
  if (drctv.containsKey("cpus")) {
    assert drctv["cpus"] instanceof Integer
  }

  /* DIRECTIVE disk
    accepted examples:
    - "1 GB"
    - "2TB"
    - "3.2KB"
    - "10.B"
  */
  if (drctv.containsKey("disk")) {
    assert drctv["disk"] instanceof CharSequence
    // assert drctv["disk"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
    // ^ does not allow closures
  }

  /* DIRECTIVE echo
    accepted examples:
    - true
    - false
  */
  if (drctv.containsKey("echo")) {
    assert drctv["echo"] instanceof Boolean
  }

  /* DIRECTIVE errorStrategy
    accepted examples:
    - "terminate"
    - "finish"
  */
  if (drctv.containsKey("errorStrategy")) {
    assert drctv["errorStrategy"] instanceof CharSequence
    assert drctv["errorStrategy"] in ["terminate", "finish", "ignore", "retry"] : "Unexpected value for errorStrategy"
  }

  /* DIRECTIVE executor
    accepted examples:
    - "local"
    - "sge"
  */
  if (drctv.containsKey("executor")) {
    assert drctv["executor"] instanceof CharSequence
    assert drctv["executor"] in ["local", "sge", "uge", "lsf", "slurm", "pbs", "pbspro", "moab", "condor", "nqsii", "ignite", "k8s", "awsbatch", "google-pipelines"] : "Unexpected value for executor"
  }

  /* DIRECTIVE machineType
    accepted examples:
    - "n1-highmem-8"
  */
  if (drctv.containsKey("machineType")) {
    assert drctv["machineType"] instanceof CharSequence
  }

  /* DIRECTIVE maxErrors
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxErrors")) {
    assert drctv["maxErrors"] instanceof Integer
  }

  /* DIRECTIVE maxForks
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxForks")) {
    assert drctv["maxForks"] instanceof Integer
  }

  /* DIRECTIVE maxRetries
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxRetries")) {
    assert drctv["maxRetries"] instanceof Integer
  }

  /* DIRECTIVE memory
    accepted examples:
    - "1 GB"
    - "2TB"
    - "3.2KB"
    - "10.B"
  */
  if (drctv.containsKey("memory")) {
    assert drctv["memory"] instanceof CharSequence
    // assert drctv["memory"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
    // ^ does not allow closures
  }

  /* DIRECTIVE module
    accepted examples:
    - "ncbi-blast/2.2.27"
    - "ncbi-blast/2.2.27:t_coffee/10.0"
    - ["ncbi-blast/2.2.27", "t_coffee/10.0"]
  */
  if (drctv.containsKey("module")) {
    if (drctv["module"] instanceof List) {
      drctv["module"] = drctv["module"].join(":")
    }
    assert drctv["module"] instanceof CharSequence
  }

  /* DIRECTIVE penv
    accepted examples:
    - "smp"
  */
  if (drctv.containsKey("penv")) {
    assert drctv["penv"] instanceof CharSequence
  }

  /* DIRECTIVE pod
    accepted examples:
    - [ label: "key", value: "val" ]
    - [ annotation: "key", value: "val" ]
    - [ env: "key", value: "val" ]
    - [ [label: "l", value: "v"], [env: "e", value: "v"]]
  */
  if (drctv.containsKey("pod")) {
    if (drctv["pod"] instanceof Map) {
      drctv["pod"] = [ drctv["pod"] ]
    }
    assert drctv["pod"] instanceof List
    drctv["pod"].forEach { pod ->
      assert pod instanceof Map
      // TODO: should more checks be added?
      // See https://www.nextflow.io/docs/latest/process.html?highlight=directives#pod
      // e.g. does it contain 'label' and 'value', or 'annotation' and 'value', or ...?
    }
  }

  /* DIRECTIVE publishDir
    accepted examples:
    - []
    - [ [ path: "foo", enabled: true ], [ path: "bar", enabled: false ] ]
    - "/path/to/dir" 
      is transformed to [[ path: "/path/to/dir" ]]
    - [ path: "/path/to/dir", mode: "cache" ]
      is transformed to [[ path: "/path/to/dir", mode: "cache" ]]
  */
  // TODO: should we also look at params["publishDir"]?
  if (drctv.containsKey("publishDir")) {
    def pblsh = drctv["publishDir"]
    
    // check different options
    assert pblsh instanceof List || pblsh instanceof Map || pblsh instanceof CharSequence
    
    // turn into list if not already so
    // for some reason, 'if (!pblsh instanceof List) pblsh = [ pblsh ]' doesn't work.
    pblsh = pblsh instanceof List ? pblsh : [ pblsh ]

    // check elements of publishDir
    pblsh = pblsh.collect{ elem ->
      // turn into map if not already so
      elem = elem instanceof CharSequence ? [ path: elem ] : elem

      // check types and keys
      assert elem instanceof Map : "Expected publish argument '$elem' to be a String or a Map. Found: class ${elem.getClass()}"
      assertMapKeys(elem, [ "path", "mode", "overwrite", "pattern", "saveAs", "enabled" ], ["path"], "publishDir")

      // check elements in map
      assert elem.containsKey("path")
      assert elem["path"] instanceof CharSequence
      if (elem.containsKey("mode")) {
        assert elem["mode"] instanceof CharSequence
        assert elem["mode"] in [ "symlink", "rellink", "link", "copy", "copyNoFollow", "move" ]
      }
      if (elem.containsKey("overwrite")) {
        assert elem["overwrite"] instanceof Boolean
      }
      if (elem.containsKey("pattern")) {
        assert elem["pattern"] instanceof CharSequence
      }
      if (elem.containsKey("saveAs")) {
        assert elem["saveAs"] instanceof CharSequence //: "saveAs as a Closure is currently not supported. Surround your closure with single quotes to get the desired effect. Example: '\{ foo \}'"
      }
      if (elem.containsKey("enabled")) {
        assert elem["enabled"] instanceof Boolean
      }

      // return final result
      elem
    }
    // store final directive
    drctv["publishDir"] = pblsh
  }

  /* DIRECTIVE queue
    accepted examples:
    - "long"
    - "short,long"
    - ["short", "long"]
  */
  if (drctv.containsKey("queue")) {
    if (drctv["queue"] instanceof List) {
      drctv["queue"] = drctv["queue"].join(",")
    }
    assert drctv["queue"] instanceof CharSequence
  }

  /* DIRECTIVE label
    accepted examples:
    - "big_mem"
    - "big_cpu"
    - ["big_mem", "big_cpu"]
  */
  if (drctv.containsKey("label")) {
    if (drctv["label"] instanceof CharSequence) {
      drctv["label"] = [ drctv["label"] ]
    }
    assert drctv["label"] instanceof List
    drctv["label"].forEach { label ->
      assert label instanceof CharSequence
      // assert label.matches("[a-zA-Z0-9]([a-zA-Z0-9_]*[a-zA-Z0-9])?")
      // ^ does not allow closures
    }
  }

  /* DIRECTIVE scratch
    accepted examples:
    - true
    - "/path/to/scratch"
    - '$MY_PATH_TO_SCRATCH'
    - "ram-disk"
  */
  if (drctv.containsKey("scratch")) {
    assert drctv["scratch"] == true || drctv["scratch"] instanceof CharSequence
  }

  /* DIRECTIVE storeDir
    accepted examples:
    - "/path/to/storeDir"
  */
  if (drctv.containsKey("storeDir")) {
    assert drctv["storeDir"] instanceof CharSequence
  }

  /* DIRECTIVE stageInMode
    accepted examples:
    - "copy"
    - "link"
  */
  if (drctv.containsKey("stageInMode")) {
    assert drctv["stageInMode"] instanceof CharSequence
    assert drctv["stageInMode"] in ["copy", "link", "symlink", "rellink"]
  }

  /* DIRECTIVE stageOutMode
    accepted examples:
    - "copy"
    - "link"
  */
  if (drctv.containsKey("stageOutMode")) {
    assert drctv["stageOutMode"] instanceof CharSequence
    assert drctv["stageOutMode"] in ["copy", "move", "rsync"]
  }

  /* DIRECTIVE tag
    accepted examples:
    - "foo"
    - '$id'
  */
  if (drctv.containsKey("tag")) {
    assert drctv["tag"] instanceof CharSequence
  }

  /* DIRECTIVE time
    accepted examples:
    - "1h"
    - "2days"
    - "1day 6hours 3minutes 30seconds"
  */
  if (drctv.containsKey("time")) {
    assert drctv["time"] instanceof CharSequence
    // todo: validation regex?
  }

  return drctv
}

// TODO: unit test processAuto
def processAuto(Map auto) {
  // remove null values
  auto = auto.findAll{k, v -> v != null}

  expectedKeys = ["simplifyInput", "simplifyOutput", "transcript", "publish"]

  // check whether expected keys are all booleans (for now)
  for (key in expectedKeys) {
    assert auto.containsKey(key)
    assert auto[key] instanceof Boolean
  }

  return auto.subMap(expectedKeys)
}

def processProcessArgs(Map args) {
  // override defaults with args
  def processArgs = thisDefaultProcessArgs + args

  // check whether 'key' exists
  assert processArgs.containsKey("key")

  // if 'key' is a closure, apply it to the original key
  if (processArgs["key"] instanceof Closure) {
    processArgs["key"] = processArgs["key"](thisConfig.functionality.name)
  }
  assert processArgs["key"] instanceof CharSequence
  assert processArgs["key"] ==~ /^[a-zA-Z_][a-zA-Z0-9_]*$/

  // check whether directives exists and apply defaults
  assert processArgs.containsKey("directives")
  assert processArgs["directives"] instanceof Map
  processArgs["directives"] = processDirectives(thisDefaultProcessArgs.directives + processArgs["directives"])

  // check whether directives exists and apply defaults
  assert processArgs.containsKey("auto")
  assert processArgs["auto"] instanceof Map
  processArgs["auto"] = processAuto(thisDefaultProcessArgs.auto + processArgs["auto"])

  // auto define publish, if so desired
  if (processArgs.auto.publish == true && (processArgs.directives.publishDir ?: [:]).isEmpty()) {
    // can't assert at this level thanks to the no_publish profile
    // assert params.containsKey("publishDir") || params.containsKey("publish_dir") : 
    //   "Error in module '${processArgs['key']}': if auto.publish is true, params.publish_dir needs to be defined.\n" +
    //   "  Example: params.publish_dir = \"./output/\""
    def publishDir = 
      params.containsKey("publish_dir") ? params.publish_dir : 
      params.containsKey("publishDir") ? params.publishDir : 
      null
    
    if (publishDir != null) {
      processArgs.directives.publishDir = [[ 
        path: publishDir, 
        saveAs: "{ it.startsWith('.') ? null : it }", // don't publish hidden files, by default
        mode: "copy"
      ]]
    }
  }

  // auto define transcript, if so desired
  if (processArgs.auto.transcript == true) {
    // can't assert at this level thanks to the no_publish profile
    // assert params.containsKey("transcriptsDir") || params.containsKey("transcripts_dir") || params.containsKey("publishDir") || params.containsKey("publish_dir") : 
    //   "Error in module '${processArgs['key']}': if auto.transcript is true, either params.transcripts_dir or params.publish_dir needs to be defined.\n" +
    //   "  Example: params.transcripts_dir = \"./transcripts/\""
    def transcriptsDir = 
      params.containsKey("transcripts_dir") ? params.transcripts_dir : 
      params.containsKey("transcriptsDir") ? params.transcriptsDir : 
      params.containsKey("publish_dir") ? params.publish_dir + "/_transcripts" :
      params.containsKey("publishDir") ? params.publishDir + "/_transcripts" : 
      null
    if (transcriptsDir != null) {
      def timestamp = Nextflow.getSession().getWorkflowMetadata().start.format('yyyy-MM-dd_HH-mm-ss')
      def transcriptsPublishDir = [ 
        path: "$transcriptsDir/$timestamp/\${task.process.replaceAll(':', '-')}/\${id}/",
        saveAs: "{ it.startsWith('.') ? it.replaceAll('^.', '') : null }", 
        mode: "copy"
      ]
      def publishDirs = processArgs.directives.publishDir ?: []
      processArgs.directives.publishDir = publishDirs + transcriptsPublishDir
    }
  }

  // if this is a stubrun, remove certain directives?
  if (workflow.stubRun) {
    processArgs.directives.keySet().removeAll(["publishDir", "cpus", "memory", "label"])
  }

  for (nam in [ "map", "mapId", "mapData", "mapPassthrough", "filter" ]) {
    if (processArgs.containsKey(nam) && processArgs[nam]) {
      assert processArgs[nam] instanceof Closure : "Expected process argument '$nam' to be null or a Closure. Found: class ${processArgs[nam].getClass()}"
    }
  }

  // return output
  return processArgs
}

def processFactory(Map processArgs) {
  // autodetect process key
  def wfKey = processArgs["key"]
  def procKeyPrefix = "${wfKey}_process"
  def meta = ScriptMeta.current()
  def existing = meta.getProcessNames().findAll{it.startsWith(procKeyPrefix)}
  def numbers = existing.collect{it.replace(procKeyPrefix, "0").toInteger()}
  def newNumber = (numbers + [-1]).max() + 1

  def procKey = newNumber == 0 ? procKeyPrefix : "$procKeyPrefix$newNumber"

  if (newNumber > 0) {
    log.warn "Key for module '${wfKey}' is duplicated.\n",
      "If you run a component multiple times in the same workflow,\n" +
      "it's recommended you set a unique key for every call,\n" +
      "for example: ${wfKey}.run(key: \"foo\")."
  }

  // subset directives and convert to list of tuples
  def drctv = processArgs.directives

  // TODO: unit test the two commands below
  // convert publish array into tags
  def valueToStr = { val ->
    // ignore closures
    if (val instanceof CharSequence) {
      if (!val.matches('^[{].*[}]$')) {
        '"' + val + '"'
      } else {
        val
      }
    } else if (val instanceof List) {
      "[" + val.collect{valueToStr(it)}.join(", ") + "]"
    } else if (val instanceof Map) {
      "[" + val.collect{k, v -> k + ": " + valueToStr(v)}.join(", ") + "]"
    } else {
      val.inspect()
    }
  }

  // multiple entries allowed: label, publishdir
  def drctvStrs = drctv.collect { key, value ->
    if (key in ["label", "publishDir"]) {
      value.collect{ val ->
        if (val instanceof Map) {
          "\n$key " + val.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
        } else if (val == null) {
          ""
        } else {
          "\n$key " + valueToStr(val)
        }
      }.join()
    } else if (value instanceof Map) {
      "\n$key " + value.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
    } else {
      "\n$key " + valueToStr(value)
    }
  }.join()

  def inputPaths = thisConfig.functionality.allArguments
    .findAll { it.type == "file" && it.direction == "input" }
    .collect { ', path(viash_par_' + it.plainName + ')' }
    .join()

  def outputPaths = thisConfig.functionality.allArguments
    .findAll { it.type == "file" && it.direction == "output" }
    .collect { par ->
      // insert dummy into every output (see nextflow-io/nextflow#2678)
      if (!par.multiple) {
        ', path{[".exitcode", args.' + par.plainName + ']}'
      } else {
        ', path{[".exitcode"] + args.' + par.plainName + '}'
      }
    }
    .join()

  // TODO: move this functionality somewhere else?
  if (processArgs.auto.transcript) {
    outputPaths = outputPaths + ', path{[".exitcode", ".command*"]}'
  } else {
    outputPaths = outputPaths + ', path{[".exitcode"]}'
  }

  // construct inputFileExports
  def inputFileExports = thisConfig.functionality.allArguments
    .findAll { it.type == "file" && it.direction.toLowerCase() == "input" }
    .collect { par ->
      viash_par_contents = !par.required && !par.multiple ? "viash_par_${par.plainName}[0]" : "viash_par_${par.plainName}.join(\"${par.multiple_sep}\")"
      "\n\${viash_par_${par.plainName}.empty ? \"\" : \"export VIASH_PAR_${par.plainName.toUpperCase()}=\\\"\" + ${viash_par_contents} + \"\\\"\"}"
    }

  // NOTE: if using docker, use /tmp instead of tmpDir!
  def tmpDir = java.nio.file.Paths.get(
    System.getenv('NXF_TEMP') ?: 
    System.getenv('VIASH_TEMP') ?: 
    System.getenv('VIASH_TMPDIR') ?: 
    System.getenv('VIASH_TEMPDIR') ?: 
    System.getenv('VIASH_TMP') ?: 
    System.getenv('TEMP') ?: 
    System.getenv('TMPDIR') ?: 
    System.getenv('TEMPDIR') ?:
    System.getenv('TMP') ?: 
    '/tmp'
  ).toAbsolutePath()

  // construct stub
  def stub = thisConfig.functionality.allArguments
    .findAll { it.type == "file" && it.direction == "output" }
    .collect { par -> 
      "\${ args.containsKey(\"${par.plainName}\") ? \"touch \\\"\" + (args[\"${par.plainName}\"] instanceof String ? args[\"${par.plainName}\"].replace(\"_*\", \"_0\") : args[\"${par.plainName}\"].join('\" \"')) + \"\\\"\" : \"\" }"
    }
    .join("\n")

  // escape script
  def escapedScript = thisScript.replace('\\', '\\\\').replace('$', '\\$').replace('"""', '\\"\\"\\"')

  // publishdir assert
  def assertStr = processArgs.auto.publish || processArgs.auto.transcript ? 
    """\nassert task.publishDir.size() > 0: "if auto.publish is true, params.publish_dir needs to be defined.\\n  Example: --publish_dir './output/'" """ :
    ""

  // generate process string
  def procStr = 
  """nextflow.enable.dsl=2
  |
  |process $procKey {$drctvStrs
  |input:
  |  tuple val(id)$inputPaths, val(args), val(passthrough), path(resourcesDir)
  |output:
  |  tuple val("\$id"), val(passthrough)$outputPaths, optional: true
  |stub:
  |\"\"\"
  |$stub
  |\"\"\"
  |script:$assertStr
  |def escapeText = { s -> s.toString().replaceAll('([`"])', '\\\\\\\\\$1') }
  |def parInject = args
  |  .findAll{key, value -> value != null}
  |  .collect{key, value -> "export VIASH_PAR_\${key.toUpperCase()}=\\\"\${escapeText(value)}\\\""}
  |  .join("\\n")
  |\"\"\"
  |# meta exports
  |export VIASH_META_RESOURCES_DIR="\${resourcesDir.toRealPath().toAbsolutePath()}"
  |export VIASH_META_TEMP_DIR="${['docker', 'podman', 'charliecloud'].any{ it == workflow.containerEngine } ? '/tmp' : tmpDir}"
  |export VIASH_META_FUNCTIONALITY_NAME="${thisConfig.functionality.name}"
  |export VIASH_META_EXECUTABLE="\\\$VIASH_META_RESOURCES_DIR/\\\$VIASH_META_FUNCTIONALITY_NAME"
  |export VIASH_META_CONFIG="\\\$VIASH_META_RESOURCES_DIR/.config.vsh.yaml"
  |\${task.cpus ? "export VIASH_META_CPUS=\$task.cpus" : "" }
  |\${task.memory?.bytes != null ? "export VIASH_META_MEMORY_B=\$task.memory.bytes" : "" }
  |if [ ! -z \\\${VIASH_META_MEMORY_B+x} ]; then
  |  export VIASH_META_MEMORY_KB=\\\$(( (\\\$VIASH_META_MEMORY_B+1023) / 1024 ))
  |  export VIASH_META_MEMORY_MB=\\\$(( (\\\$VIASH_META_MEMORY_KB+1023) / 1024 ))
  |  export VIASH_META_MEMORY_GB=\\\$(( (\\\$VIASH_META_MEMORY_MB+1023) / 1024 ))
  |  export VIASH_META_MEMORY_TB=\\\$(( (\\\$VIASH_META_MEMORY_GB+1023) / 1024 ))
  |  export VIASH_META_MEMORY_PB=\\\$(( (\\\$VIASH_META_MEMORY_TB+1023) / 1024 ))
  |fi
  |
  |# meta synonyms
  |export VIASH_TEMP="\\\$VIASH_META_TEMP_DIR"
  |export TEMP_DIR="\\\$VIASH_META_TEMP_DIR"
  |
  |# argument exports${inputFileExports.join()}
  |\$parInject
  |
  |# process script
  |${escapedScript}
  |\"\"\"
  |}
  |""".stripMargin()

  // TODO: print on debug
  // if (processArgs.debug == true) {
  //   println("######################\n$procStr\n######################")
  // }

  // create runtime process
  def ownerParams = new ScriptBinding.ParamsMap()
  def binding = new ScriptBinding().setParams(ownerParams)
  def module = new IncludeDef.Module(name: procKey)
  def scriptParser = new ScriptParser(session)
    .setModule(true)
    .setBinding(binding)
  scriptParser.scriptPath = ScriptMeta.current().getScriptPath()
  def moduleScript = scriptParser.runScript(procStr)
    .getScript()

  // register module in meta
  meta.addModule(moduleScript, module.name, module.alias)

  // retrieve and return process from meta
  return meta.getProcess(procKey)
}

def debug(processArgs, debugKey) {
  if (processArgs.debug) {
    view { "process '${processArgs.key}' $debugKey tuple: $it"  }
  } else {
    map { it }
  }
}

def workflowFactory(Map args) {
  def processArgs = processProcessArgs(args)
  def key = processArgs["key"]
  def meta = ScriptMeta.current()

  def workflowKey = key

  // write process to temporary nf file and parse it in memory
  def processObj = processFactory(processArgs)
  
  workflow workflowInstance {
    take:
    input_

    main:
    mid1_ = input_
      | debug(processArgs, "input")
      | map { tuple ->
        tuple = tuple.clone()
        
        if (processArgs.map) {
          tuple = processArgs.map(tuple)
        }
        if (processArgs.mapId) {
          tuple[0] = processArgs.mapId(tuple[0])
        }
        if (processArgs.mapData) {
          tuple[1] = processArgs.mapData(tuple[1])
        }
        if (processArgs.mapPassthrough) {
          tuple = tuple.take(2) + processArgs.mapPassthrough(tuple.drop(2))
        }

        // check tuple
        assert tuple instanceof List : 
          "Error in module '${key}': element in channel should be a tuple [id, data, ...otherargs...]\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Expected class: List. Found: tuple.getClass() is ${tuple.getClass()}"
        assert tuple.size() >= 2 : 
          "Error in module '${key}': expected length of tuple in input channel to be two or greater.\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Found: tuple.size() == ${tuple.size()}"
        
        // check id field
        assert tuple[0] instanceof CharSequence : 
          "Error in module '${key}': first element of tuple in channel should be a String\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Found: ${tuple[0]}"
        
        // match file to input file
        if (processArgs.auto.simplifyInput && (tuple[1] instanceof Path || tuple[1] instanceof List)) {
          def inputFiles = thisConfig.functionality.allArguments
            .findAll { it.type == "file" && it.direction == "input" }
          
          assert inputFiles.size() == 1 : 
              "Error in module '${key}' id '${tuple[0]}'.\n" +
              "  Anonymous file inputs are only allowed when the process has exactly one file input.\n" +
              "  Expected: inputFiles.size() == 1. Found: inputFiles.size() is ${inputFiles.size()}"

          tuple[1] = [[ inputFiles[0].plainName, tuple[1] ]].collectEntries()
        }

        // check data field
        assert tuple[1] instanceof Map : 
          "Error in module '${key}' id '${tuple[0]}': second element of tuple in channel should be a Map\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"

        // rename keys of data field in tuple
        if (processArgs.renameKeys) {
          assert processArgs.renameKeys instanceof Map : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class: Map. Found: renameKeys.getClass() is ${processArgs.renameKeys.getClass()}"
          assert tuple[1] instanceof Map : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"

          // TODO: allow renameKeys to be a function?
          processArgs.renameKeys.each { newKey, oldKey ->
            assert newKey instanceof CharSequence : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class of newKey: String. Found: newKey.getClass() is ${newKey.getClass()}"
            assert oldKey instanceof CharSequence : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class of oldKey: String. Found: oldKey.getClass() is ${oldKey.getClass()}"
            assert tuple[1].containsKey(oldKey) : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Key '$oldKey' is missing in the data map. tuple[1].keySet() is '${tuple[1].keySet()}'"
            tuple[1].put(newKey, tuple[1][oldKey])
          }
          tuple[1].keySet().removeAll(processArgs.renameKeys.collect{ newKey, oldKey -> oldKey })
        }
        tuple
      }
    if (processArgs.filter) {
      mid2_ = mid1_
        | filter{processArgs.filter(it)}
    } else {
      mid2_ = mid1_
    }
    output_ = mid2_
      | debug(processArgs, "processed")
      | map { tuple ->
        def id = tuple[0]
        def data = tuple[1]
        def passthrough = tuple.drop(2)

        // fetch default params from functionality
        def defaultArgs = thisConfig.functionality.allArguments
          .findAll { it.containsKey("default") }
          .collectEntries { [ it.plainName, it.default ] }

        // fetch overrides in params
        def paramArgs = thisConfig.functionality.allArguments
          .findAll { par ->
            def argKey = key + "__" + par.plainName
            params.containsKey(argKey) && params[argKey] != "viash_no_value"
          }
          .collectEntries { [ it.plainName, params[key + "__" + it.plainName] ] }
        
        // fetch overrides in data
        def dataArgs = thisConfig.functionality.allArguments
          .findAll { data.containsKey(it.plainName) }
          .collectEntries { [ it.plainName, data[it.plainName] ] }
        
        // combine params
        def combinedArgs = defaultArgs + paramArgs + processArgs.args + dataArgs

        // remove arguments with explicit null values
        combinedArgs.removeAll{it.value == null}

        if (workflow.stubRun) {
          // add id if missing
          combinedArgs = [id: 'stub'] + combinedArgs
        } else {
          // check whether required arguments exist
          thisConfig.functionality.allArguments
            .forEach { par ->
              if (par.required) {
                assert combinedArgs.containsKey(par.plainName): "Argument ${par.plainName} is required but does not have a value"
              }
            }
        }

        // TODO: check whether parameters have the right type

        // process input files separately
        def inputPaths = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction == "input" }
          .collect { par ->
            def val = combinedArgs.containsKey(par.plainName) ? combinedArgs[par.plainName] : []
            def inputFiles = []
            if (val == null) {
              inputFiles = []
            } else if (val instanceof List) {
              inputFiles = val
            } else if (val instanceof Path) {
              inputFiles = [ val ]
            } else {
              inputFiles = []
            }
            if (!workflow.stubRun) {
              // throw error when an input file doesn't exist
              inputFiles.each{ file -> 
                assert file.exists() :
                  "Error in module '${key}' id '${id}' argument '${par.plainName}'.\n" +
                  "  Required input file does not exist.\n" +
                  "  Path: '$file'.\n" +
                  "  Expected input file to exist"
              }
            }
            inputFiles 
          } 

        // remove input files
        def argsExclInputFiles = thisConfig.functionality.allArguments
          .findAll { (it.type != "file" || it.direction != "input") && combinedArgs.containsKey(it.plainName) }
          .collectEntries { par ->
            def parName = par.plainName
            def val = combinedArgs[parName]
            if (par.multiple && val instanceof Collection) {
              val = val.join(par.multiple_sep)
            }
            if (par.direction == "output" && par.type == "file") {
              val = val.replaceAll('\\$id', id).replaceAll('\\$key', key)
            }
            [parName, val]
          }

        [ id ] + inputPaths + [ argsExclInputFiles, passthrough, resourcesDir ]
      }
      | processObj
      | map { output ->
        def outputFiles = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction == "output" }
          .indexed()
          .collectEntries{ index, par ->
            out = output[index + 2]
            // strip dummy '.exitcode' file from output (see nextflow-io/nextflow#2678)
            if (!out instanceof List || out.size() <= 1) {
              if (par.multiple) {
                out = []
              } else {
                assert !par.required :
                    "Error in module '${key}' id '${output[0]}' argument '${par.plainName}'.\n" +
                    "  Required output file is missing"
                out = null
              }
            } else if (out.size() == 2 && !par.multiple) {
              out = out[1]
            } else {
              out = out.drop(1)
            }
            [ par.plainName, out ]
          }
        
        // drop null outputs
        outputFiles.removeAll{it.value == null}

        if (processArgs.auto.simplifyOutput && outputFiles.size() == 1) {
          outputFiles = outputFiles.values()[0]
        }

        def out = [ output[0], outputFiles ]

        // passthrough additional items
        if (output[1]) {
          out.addAll(output[1])
        }

        out
      }
      | debug(processArgs, "output")

    emit:
    output_
  }

  def wf = workflowInstance.cloneWithName(workflowKey)

  // add factory function
  wf.metaClass.run = { runArgs ->
    workflowFactory(runArgs)
  }
  // add config to module for later introspection
  wf.metaClass.config = thisConfig

  return wf
}

// initialise default workflow
myWfInstance = workflowFactory([:])

// add workflow to environment
ScriptMeta.current().addDefinition(myWfInstance)

// anonymous workflow for running this module as a standalone
workflow {
  def mergedConfig = thisConfig

  // add id argument if it's not already in the config
  if (mergedConfig.functionality.arguments.every{it.plainName != "id"}) {
    def idArg = [
      'name': '--id',
      'required': false,
      'type': 'string',
      'description': 'A unique id for every entry.',
      'default': 'run',
      'multiple': false
    ]
    mergedConfig.functionality.arguments.add(0, idArg)
    mergedConfig = processConfig(mergedConfig)
  }

  helpMessage(mergedConfig)

  viashChannel(params, mergedConfig)
    | view { "input: $it" }
    | myWfInstance.run(
      auto: [ publish: true ]
    )
    | view { "output: $it" }
}