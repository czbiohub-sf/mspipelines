functionality:
  name: "maxquant"
  namespace: "maxquant"
  version: "0.3.1"
  authors:
  - name: "Robrecht Cannoodt"
    email: "rcannood@gmail.com"
    roles:
    - "maintainer"
    props:
      github: "rcannood"
      orcid: "0000-0003-3641-729X"
  inputs:
  - type: "file"
    name: "--input"
    alternatives: []
    description: "One or more Thermo Raw files."
    example:
    - "input.raw"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "file"
    name: "--reference"
    alternatives: []
    description: "A reference file in fasta format."
    example:
    - "reference.fasta"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: true
    multiple_sep: ":"
  outputs:
  - type: "file"
    name: "--output"
    alternatives: []
    description: "An output directory to store the 'mqpar.xml' and 'combined' outputs."
    example:
    - "output_dir"
    default: []
    must_exist: false
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ":"
  arguments:
  - type: "string"
    name: "--ref_taxonomy_id"
    alternatives: []
    description: "Taxonomy ID. Length must match --reference parameter. \nCommon taxonomy\
      \ IDs are Homo Sapiens: 9606, Mus Musculus: 10090.\n"
    example:
    - "9606"
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "boolean"
    name: "--match_between_runs"
    alternatives: []
    description: "Identifications are transferred to non-sequenced or non-identified\
      \ MS features in other LC-MS runs."
    example: []
    default:
    - false
    required: false
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--write_tables"
    alternatives: []
    description: "Which tables to write out."
    example: []
    default:
    - "msScans"
    - "msmsScans"
    - "ms3Scans"
    - "allPeptides"
    - "mzRange"
    - "mzTab"
    - "DIA fragments"
    - "DIA fragments quant"
    - "pasefMsmsScans"
    - "accumulatedMsmsScans"
    required: false
    choices:
    - "msScans"
    - "msmsScans"
    - "ms3Scans"
    - "allPeptides"
    - "mzRange"
    - "mzTab"
    - "DIA fragments"
    - "DIA fragments quant"
    - "pasefMsmsScans"
    - "accumulatedMsmsScans"
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "string"
    name: "--ms_instrument"
    alternatives: []
    description: "Type of intrument the data was generated on. Some internal parameters,\
      \ e.g. in peak detection are set to optimal values based on the machine type.\
      \ Currently Thermo Fisher Orbitrap and FT like instruments are supported, as\
      \ well as ToF instruments like Bruker Impact HD and AB Sciex TripleTOF 5600.\
      \ Usually there is no need for the user to adjust the sub-parameters."
    example: []
    default:
    - "Bruker TIMS"
    required: false
    choices:
    - "Orbitrap"
    - "Bruker Q-TOF"
    - "Sciex Q-TOF"
    - "Agilent Q-TOF"
    - "Bruker TIMS"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--lcms_run_type"
    alternatives: []
    description: "The type of LC-MS run. Select 'Standard' for label free and MS1\
      \ labeled samples. For conventional isobaric labeling samples, select 'Reporter\
      \ ion MS2'. In case the isobaric labeling reporters should be read from MS3\
      \ spectra, please select 'Reporter ion MS3'."
    example: []
    default:
    - "Standard"
    required: false
    choices:
    - "Standard"
    - "Reporter ion MS2"
    - "Reporter ion MS3"
    - "NeuCode"
    - "BoxCar"
    - "TIMS-DDA"
    - "MaxDIA"
    - "TIMS MaxDIA"
    - "BoxCar MaxDIA"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--lfq_mode"
    alternatives: []
    description: "Apply the algorithm for label free protein quantification. The use\
      \ of an experimental design so specify which LC-MS runs or groups of LC-MS runs\
      \ correspond to the different samples is obligatory here. The output of the\
      \ label free algorithm can be found in the proteinGroups table in the columns\
      \ starting with 'LFQ Intensity'."
    example: []
    default:
    - "LFQ"
    required: false
    choices:
    - "None"
    - "LFQ"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--num_cores"
    alternatives: []
    description: "Number of cores to use during the analysis."
    example:
    - 30
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "boolean_true"
    name: "--dryrun"
    alternatives: []
    description: "If true, will only generate the mqpar.xml file and not run MaxQuant."
    direction: "input"
  argument_groups: []
  resources:
  - type: "file"
    text: |
      // maxquant 0.3.1
      // 
      // This wrapper script is auto-generated by viash 0.5.15 and is thus a derivative
      // work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
      // Intuitive.
      // 
      // The component may contain files which fall under a different license. The
      // authors of this component should specify the license in the header of such
      // files, or include a separate license file detailing the licenses of all included
      // files.
      // 
      // Component authors:
      //  * Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid:
      // 0000-0003-3641-729X}
      
      nextflow.enable.dsl=2
      
      // Required imports
      import groovy.json.JsonSlurper
      
      // initialise slurper
      def jsonSlurper = new JsonSlurper()
      
      // DEFINE CUSTOM CODE
      
      // functionality metadata
      thisConfig = processConfig([
        functionality: jsonSlurper.parseText('''{
        "name" : "maxquant",
        "namespace" : "maxquant",
        "version" : "0.3.1",
        "authors" : [
          {
            "name" : "Robrecht Cannoodt",
            "email" : "rcannood@gmail.com",
            "roles" : [
              "maintainer"
            ],
            "props" : {
              "github" : "rcannood",
              "orcid" : "0000-0003-3641-729X"
            }
          }
        ],
        "inputs" : [
          {
            "type" : "file",
            "name" : "--input",
            "description" : "One or more Thermo Raw files.",
            "example" : [
              "input.raw"
            ],
            "must_exist" : false,
            "required" : true,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ":"
          },
          {
            "type" : "file",
            "name" : "--reference",
            "description" : "A reference file in fasta format.",
            "example" : [
              "reference.fasta"
            ],
            "must_exist" : false,
            "required" : true,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ":"
          }
        ],
        "outputs" : [
          {
            "type" : "file",
            "name" : "--output",
            "description" : "An output directory to store the 'mqpar.xml' and 'combined' outputs.",
            "example" : [
              "output_dir"
            ],
            "must_exist" : false,
            "required" : true,
            "direction" : "output",
            "multiple" : false,
            "multiple_sep" : ":"
          }
        ],
        "arguments" : [
          {
            "type" : "string",
            "name" : "--ref_taxonomy_id",
            "description" : "Taxonomy ID. Length must match --reference parameter. \nCommon taxonomy IDs are Homo Sapiens: 9606, Mus Musculus: 10090.\n",
            "example" : [
              "9606"
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ":"
          },
          {
            "type" : "boolean",
            "name" : "--match_between_runs",
            "description" : "Identifications are transferred to non-sequenced or non-identified MS features in other LC-MS runs.",
            "default" : [
              false
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":"
          },
          {
            "type" : "string",
            "name" : "--write_tables",
            "description" : "Which tables to write out.",
            "default" : [
              "msScans",
              "msmsScans",
              "ms3Scans",
              "allPeptides",
              "mzRange",
              "mzTab",
              "DIA fragments",
              "DIA fragments quant",
              "pasefMsmsScans",
              "accumulatedMsmsScans"
            ],
            "required" : false,
            "choices" : [
              "msScans",
              "msmsScans",
              "ms3Scans",
              "allPeptides",
              "mzRange",
              "mzTab",
              "DIA fragments",
              "DIA fragments quant",
              "pasefMsmsScans",
              "accumulatedMsmsScans"
            ],
            "direction" : "input",
            "multiple" : true,
            "multiple_sep" : ":"
          },
          {
            "type" : "string",
            "name" : "--ms_instrument",
            "description" : "Type of intrument the data was generated on. Some internal parameters, e.g. in peak detection are set to optimal values based on the machine type. Currently Thermo Fisher Orbitrap and FT like instruments are supported, as well as ToF instruments like Bruker Impact HD and AB Sciex TripleTOF 5600. Usually there is no need for the user to adjust the sub-parameters.",
            "default" : [
              "Bruker TIMS"
            ],
            "required" : false,
            "choices" : [
              "Orbitrap",
              "Bruker Q-TOF",
              "Sciex Q-TOF",
              "Agilent Q-TOF",
              "Bruker TIMS"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":"
          },
          {
            "type" : "string",
            "name" : "--lcms_run_type",
            "description" : "The type of LC-MS run. Select 'Standard' for label free and MS1 labeled samples. For conventional isobaric labeling samples, select 'Reporter ion MS2'. In case the isobaric labeling reporters should be read from MS3 spectra, please select 'Reporter ion MS3'.",
            "default" : [
              "Standard"
            ],
            "required" : false,
            "choices" : [
              "Standard",
              "Reporter ion MS2",
              "Reporter ion MS3",
              "NeuCode",
              "BoxCar",
              "TIMS-DDA",
              "MaxDIA",
              "TIMS MaxDIA",
              "BoxCar MaxDIA"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":"
          },
          {
            "type" : "string",
            "name" : "--lfq_mode",
            "description" : "Apply the algorithm for label free protein quantification. The use of an experimental design so specify which LC-MS runs or groups of LC-MS runs correspond to the different samples is obligatory here. The output of the label free algorithm can be found in the proteinGroups table in the columns starting with 'LFQ Intensity'.",
            "default" : [
              "LFQ"
            ],
            "required" : false,
            "choices" : [
              "None",
              "LFQ"
            ],
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":"
          },
          {
            "type" : "integer",
            "name" : "--num_cores",
            "description" : "Number of cores to use during the analysis.",
            "example" : [
              30
            ],
            "required" : false,
            "direction" : "input",
            "multiple" : false,
            "multiple_sep" : ":"
          },
          {
            "type" : "boolean_true",
            "name" : "--dryrun",
            "description" : "If true, will only generate the mqpar.xml file and not run MaxQuant.",
            "direction" : "input"
          }
        ],
        "resources" : [
          {
            "type" : "python_script",
            "path" : "script.py",
            "is_executable" : true,
            "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/maxquant/maxquant/config.vsh.yaml"
          },
          {
            "type" : "file",
            "path" : "settings",
            "parent" : "file:/home/runner/work/mspipelines/mspipelines/src/maxquant/maxquant/config.vsh.yaml"
          }
        ],
        "description" : "Perform a MaxQuant analysis with mostly default parameters.",
        "usage" : "maxquant --input file1.raw --input file2.raw --reference ref.fasta --output out/",
        "set_wd_to_resources_dir" : false,
        "enabled" : true
      }''')
      ])
      
      thisScript = '''set -e
      tempscript=".viash_script.sh"
      cat > "$tempscript" << VIASHMAIN
      
      import os
      import re
      import subprocess
      import tempfile
      import shutil
      import pandas as pd
      
      ## VIASH START
      # The following code has been auto-generated by Viash.
      par = {
        'input': $( if [ ! -z ${VIASH_PAR_INPUT+x} ]; then echo "'${VIASH_PAR_INPUT//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'reference': $( if [ ! -z ${VIASH_PAR_REFERENCE+x} ]; then echo "'${VIASH_PAR_REFERENCE//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'output': $( if [ ! -z ${VIASH_PAR_OUTPUT+x} ]; then echo "'${VIASH_PAR_OUTPUT//\\'/\\\\\\'}'"; else echo None; fi ),
        'ref_taxonomy_id': $( if [ ! -z ${VIASH_PAR_REF_TAXONOMY_ID+x} ]; then echo "'${VIASH_PAR_REF_TAXONOMY_ID//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'match_between_runs': $( if [ ! -z ${VIASH_PAR_MATCH_BETWEEN_RUNS+x} ]; then echo "'${VIASH_PAR_MATCH_BETWEEN_RUNS//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi ),
        'write_tables': $( if [ ! -z ${VIASH_PAR_WRITE_TABLES+x} ]; then echo "'${VIASH_PAR_WRITE_TABLES//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'ms_instrument': $( if [ ! -z ${VIASH_PAR_MS_INSTRUMENT+x} ]; then echo "'${VIASH_PAR_MS_INSTRUMENT//\\'/\\\\\\'}'"; else echo None; fi ),
        'lcms_run_type': $( if [ ! -z ${VIASH_PAR_LCMS_RUN_TYPE+x} ]; then echo "'${VIASH_PAR_LCMS_RUN_TYPE//\\'/\\\\\\'}'"; else echo None; fi ),
        'lfq_mode': $( if [ ! -z ${VIASH_PAR_LFQ_MODE+x} ]; then echo "'${VIASH_PAR_LFQ_MODE//\\'/\\\\\\'}'"; else echo None; fi ),
        'num_cores': $( if [ ! -z ${VIASH_PAR_NUM_CORES+x} ]; then echo "int('${VIASH_PAR_NUM_CORES//\\'/\\\\\\'}')"; else echo None; fi ),
        'dryrun': $( if [ ! -z ${VIASH_PAR_DRYRUN+x} ]; then echo "'${VIASH_PAR_DRYRUN//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi )
      }
      meta = {
        'functionality_name': '$VIASH_META_FUNCTIONALITY_NAME',
        'resources_dir': '$VIASH_META_RESOURCES_DIR',
        'executable': '$VIASH_META_EXECUTABLE',
        'temp_dir': '$VIASH_TEMP'
      }
      
      resources_dir = '$VIASH_META_RESOURCES_DIR'
      
      ## VIASH END
      
      # if par_input is a directory, look for raw files
      if len(par["input"]) == 1 and os.path.isdir(par["input"][0]):
         par["input"] = [ os.path.join(dp, f) for dp, dn, filenames in os.walk(par["input"]) for f in filenames if re.match(r'.*\\\\.raw', f) ]
      
      # set taxonomy id to empty string if not specified
      if not par["ref_taxonomy_id"]:
         par["ref_taxonomy_id"] = [ "" for ref in par["reference"] ]
      
      # use absolute paths
      par["input"] = [ os.path.abspath(f) for f in par["input"] ]
      par["reference"] = [ os.path.abspath(f) for f in par["reference"] ]
      par["output"] = os.path.abspath(par["output"])
      
      # auto set experiment names
      experiment_names = [ re.sub(r"_\\\\d+\\$", "", os.path.basename(file)) for file in par["input"] ]
      
      # load default matching settings
      match_between_runs_settings = pd.read_table(
         meta["resources_dir"] + "/settings/match_between_runs.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # load default instrument settings
      ms_instrument_settings = pd.read_table(
         meta["resources_dir"] + "/settings/ms_instrument.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # load default group type settings
      group_type_settings = pd.read_table(
         meta["resources_dir"] + "/settings/group_type.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # check reference metadata
      
      assert len(par["reference"]) == len(par["ref_taxonomy_id"]), "--ref_taxonomy_id must have same length as --reference"
      
      # copy input files to tempdir
      with tempfile.TemporaryDirectory() as temp_dir:
         # prepare to copy input files to tempdir
         old_inputs = par["input"]
         new_inputs = [ os.path.join(temp_dir, os.path.basename(f)) for f in old_inputs ]
         par["input"] = new_inputs
      
         # create output dir if not exists
         if not os.path.exists(par["output"]):
            os.makedirs(par["output"])
      
         # Create params file
         param_file = os.path.join(par["output"], "mqpar.xml")
         endl = "\\\\n"
         param_content = f"""<?xml version="1.0" encoding="utf-8"?>
      <MaxQuantParams xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
         <fastaFiles>"""
      
         # TODO: make taxonomy optional
         for path, taxid in zip(par["reference"], par["ref_taxonomy_id"]):
            param_content += f"""
            <FastaFileInfo>
               <fastaFilePath>{path}</fastaFilePath>
               <identifierParseRule>>.*\\\\|(.*)\\\\|</identifierParseRule>
               <descriptionParseRule>>(.*)</descriptionParseRule>
               <taxonomyParseRule></taxonomyParseRule>
               <variationParseRule></variationParseRule>
               <modificationParseRule></modificationParseRule>
               <taxonomyId>{taxid}</taxonomyId>
            </FastaFileInfo>"""
      
         param_content += f"""
         </fastaFiles>
         <fastaFilesProteogenomics>
         </fastaFilesProteogenomics>
         <fastaFilesFirstSearch>
         </fastaFilesFirstSearch>
         <fixedSearchFolder></fixedSearchFolder>
         <andromedaCacheSize>350000</andromedaCacheSize>
         <advancedRatios>True</advancedRatios>
         <pvalThres>0.005</pvalThres>
         <rtShift>False</rtShift>
         <separateLfq>False</separateLfq>
         <lfqStabilizeLargeRatios>True</lfqStabilizeLargeRatios>
         <lfqRequireMsms>True</lfqRequireMsms>
         <lfqBayesQuant>False</lfqBayesQuant>
         <decoyMode>revert</decoyMode>
         <boxCarMode>all</boxCarMode>
         <includeContaminants>True</includeContaminants>
         <maxPeptideMass>4600</maxPeptideMass>
         <epsilonMutationScore>True</epsilonMutationScore>
         <mutatedPeptidesSeparately>True</mutatedPeptidesSeparately>
         <proteogenomicPeptidesSeparately>True</proteogenomicPeptidesSeparately>
         <minDeltaScoreUnmodifiedPeptides>0</minDeltaScoreUnmodifiedPeptides>
         <minDeltaScoreModifiedPeptides>6</minDeltaScoreModifiedPeptides>
         <minScoreUnmodifiedPeptides>0</minScoreUnmodifiedPeptides>
         <minScoreModifiedPeptides>40</minScoreModifiedPeptides>
         <secondPeptide>True</secondPeptide>
         <matchBetweenRuns>{par["match_between_runs"]}</matchBetweenRuns>
         <matchUnidentifiedFeatures>False</matchUnidentifiedFeatures>
         <matchBetweenRunsFdr>False</matchBetweenRunsFdr>
         <dependentPeptides>False</dependentPeptides>
         <dependentPeptideFdr>0</dependentPeptideFdr>
         <dependentPeptideMassBin>0</dependentPeptideMassBin>
         <dependentPeptidesBetweenRuns>False</dependentPeptidesBetweenRuns>
         <dependentPeptidesWithinExperiment>False</dependentPeptidesWithinExperiment>
         <dependentPeptidesWithinParameterGroup>False</dependentPeptidesWithinParameterGroup>
         <dependentPeptidesRestrictFractions>False</dependentPeptidesRestrictFractions>
         <dependentPeptidesFractionDifference>0</dependentPeptidesFractionDifference>
         <ibaq>False</ibaq>
         <top3>False</top3>
         <independentEnzymes>False</independentEnzymes>
         <useDeltaScore>False</useDeltaScore>
         <splitProteinGroupsByTaxonomy>False</splitProteinGroupsByTaxonomy>
         <taxonomyLevel>Species</taxonomyLevel>
         <avalon>False</avalon>
         <nModColumns>3</nModColumns>
         <ibaqLogFit>False</ibaqLogFit>
         <ibaqChargeNormalization>False</ibaqChargeNormalization>
         <razorProteinFdr>True</razorProteinFdr>
         <deNovoSequencing>False</deNovoSequencing>
         <deNovoVarMods>False</deNovoVarMods>
         <deNovoCompleteSequence>False</deNovoCompleteSequence>
         <deNovoCalibratedMasses>False</deNovoCalibratedMasses>
         <deNovoMaxIterations>0</deNovoMaxIterations>
         <deNovoProteaseReward>0</deNovoProteaseReward>
         <deNovoProteaseRewardTof>0</deNovoProteaseRewardTof>
         <deNovoAgPenalty>0</deNovoAgPenalty>
         <deNovoGgPenalty>0</deNovoGgPenalty>
         <deNovoUseComplementScore>True</deNovoUseComplementScore>
         <deNovoUseProteaseScore>True</deNovoUseProteaseScore>
         <deNovoUseWaterLossScore>True</deNovoUseWaterLossScore>
         <deNovoUseAmmoniaLossScore>True</deNovoUseAmmoniaLossScore>
         <deNovoUseA2Score>True</deNovoUseA2Score>
         <massDifferenceSearch>False</massDifferenceSearch>
         <isotopeCalc>False</isotopeCalc>
         <writePeptidesForSpectrumFile></writePeptidesForSpectrumFile>
         <intensityPredictionsFile>
         </intensityPredictionsFile>
         <minPepLen>7</minPepLen>
         <psmFdrCrosslink>0.01</psmFdrCrosslink>
         <peptideFdr>0.01</peptideFdr>
         <proteinFdr>0.01</proteinFdr>
         <siteFdr>0.01</siteFdr>
         <minPeptideLengthForUnspecificSearch>8</minPeptideLengthForUnspecificSearch>
         <maxPeptideLengthForUnspecificSearch>25</maxPeptideLengthForUnspecificSearch>
         <useNormRatiosForOccupancy>True</useNormRatiosForOccupancy>
         <minPeptides>1</minPeptides>
         <minRazorPeptides>1</minRazorPeptides>
         <minUniquePeptides>0</minUniquePeptides>
         <useCounterparts>False</useCounterparts>
         <advancedSiteIntensities>True</advancedSiteIntensities>
         <customProteinQuantification>False</customProteinQuantification>
         <customProteinQuantificationFile></customProteinQuantificationFile>
         <minRatioCount>2</minRatioCount>
         <restrictProteinQuantification>True</restrictProteinQuantification>
         <restrictMods>
            <string>Oxidation (M)</string>
            <string>Acetyl (Protein N-term)</string>
         </restrictMods>
         <matchingTimeWindow>{match_between_runs_settings.at[par["match_between_runs"], "matchingTimeWindow"]}</matchingTimeWindow>
         <matchingIonMobilityWindow>{match_between_runs_settings.at[par["match_between_runs"], "matchingIonMobilityWindow"]}</matchingIonMobilityWindow>
         <alignmentTimeWindow>{match_between_runs_settings.at[par["match_between_runs"], "alignmentTimeWindow"]}</alignmentTimeWindow>
         <alignmentIonMobilityWindow>{match_between_runs_settings.at[par["match_between_runs"], "alignmentIonMobilityWindow"]}</alignmentIonMobilityWindow>
         <numberOfCandidatesMsms>15</numberOfCandidatesMsms>
         <compositionPrediction>0</compositionPrediction>
         <quantMode>1</quantMode>
         <massDifferenceMods>
         </massDifferenceMods>
         <mainSearchMaxCombinations>200</mainSearchMaxCombinations>
         <writeMsScansTable>{"msScans" in par["write_tables"]}</writeMsScansTable>
         <writeMsmsScansTable>{"msmsScans" in par["write_tables"]}</writeMsmsScansTable>
         <writePasefMsmsScansTable>{"pasefMsmsScans" in par["write_tables"]}</writePasefMsmsScansTable>
         <writeAccumulatedMsmsScansTable>{"accumulatedMsmsScans" in par["write_tables"]}</writeAccumulatedMsmsScansTable>
         <writeMs3ScansTable>{"ms3Scans" in par["write_tables"]}</writeMs3ScansTable>
         <writeAllPeptidesTable>{"allPeptides" in par["write_tables"]}</writeAllPeptidesTable>
         <writeMzRangeTable>{"mzRange" in par["write_tables"]}</writeMzRangeTable>
         <writeDiaFragmentTable>{"DIA fragments" in par["write_tables"]}</writeDiaFragmentTable>
         <writeDiaFragmentQuantTable>{"DIA fragments quant" in par["write_tables"]}</writeDiaFragmentQuantTable>
         <writeMzTab>{"mzTab" in par["write_tables"]}</writeMzTab>
         <disableMd5>False</disableMd5>
         <cacheBinInds>True</cacheBinInds>
         <etdIncludeB>False</etdIncludeB>
         <ms2PrecursorShift>0</ms2PrecursorShift>
         <complementaryIonPpm>20</complementaryIonPpm>
         <variationParseRule></variationParseRule>
         <variationMode>none</variationMode>
         <useSeriesReporters>False</useSeriesReporters>
         <name>session1</name>
         <maxQuantVersion>2.0.3.0</maxQuantVersion>
         <pluginFolder></pluginFolder>
         <numThreads>{par["num_cores"]}</numThreads>
         <emailAddress></emailAddress>
         <smtpHost></smtpHost>
         <emailFromAddress></emailFromAddress>
         <fixedCombinedFolder>{par["output"]}/</fixedCombinedFolder>
         <fullMinMz>-1.79769313486232E+308</fullMinMz>
         <fullMaxMz>1.79769313486232E+308</fullMaxMz>
         <sendEmail>False</sendEmail>
         <ionCountIntensities>False</ionCountIntensities>
         <verboseColumnHeaders>False</verboseColumnHeaders>
         <calcPeakProperties>True</calcPeakProperties>
         <showCentroidMassDifferences>False</showCentroidMassDifferences>
         <showIsotopeMassDifferences>False</showIsotopeMassDifferences>
         <useDotNetCore>True</useDotNetCore>
         <profilePerformance>False</profilePerformance>
         <filePaths>{''.join([ f"{endl}      <string>{file}</string>" for file in par["input"] ])}
         </filePaths>
         <experiments>{''.join([ f"{endl}      <string>{exp}</string>" for exp in experiment_names ])}
         </experiments>
         <fractions>{''.join([ f"{endl}      <short>32767</short>" for file in par["input"] ])}
         </fractions>
         <ptms>{''.join([ f"{endl}      <boolean>False</boolean>" for file in par["input"] ])}
         </ptms>
         <paramGroupIndices>{''.join([ f"{endl}      <int>0</int>" for file in par["input"] ])}
         </paramGroupIndices>
         <referenceChannel>{''.join([ f"{endl}      <string></string>" for file in par["input"] ])}
         </referenceChannel>
         <intensPred>False</intensPred>
         <intensPredModelReTrain>False</intensPredModelReTrain>
         <lfqTopNPeptides>0</lfqTopNPeptides>
         <diaJoinPrecChargesForLfq>False</diaJoinPrecChargesForLfq>
         <diaFragChargesForQuant>1</diaFragChargesForQuant>
         <timsRearrangeSpectra>False</timsRearrangeSpectra>
         <gridSpacing>0.5</gridSpacing>
         <proteinGroupingFile></proteinGroupingFile>
         <parameterGroups>
            <parameterGroup>
               <msInstrument>{ms_instrument_settings.at[par["ms_instrument"], "msInstrument"]}</msInstrument>
               <maxCharge>{ms_instrument_settings.at[par["ms_instrument"], "maxCharge"]}</maxCharge>
               <minPeakLen>{ms_instrument_settings.at[par["ms_instrument"], "minPeakLen"]}</minPeakLen>
               <diaMinPeakLen>{ms_instrument_settings.at[par["ms_instrument"], "diaMinPeakLen"]}</diaMinPeakLen>
               <useMs1Centroids>{ms_instrument_settings.at[par["ms_instrument"], "useMs1Centroids"]}</useMs1Centroids>
               <useMs2Centroids>{ms_instrument_settings.at[par["ms_instrument"], "useMs2Centroids"]}</useMs2Centroids>
               <cutPeaks>True</cutPeaks>
               <gapScans>1</gapScans>
               <minTime>NaN</minTime>
               <maxTime>NaN</maxTime>
               <matchType>MatchFromAndTo</matchType>
               <intensityDetermination>{ms_instrument_settings.at[par["ms_instrument"], "intensityDetermination"]}</intensityDetermination>
               <centroidMatchTol>{ms_instrument_settings.at[par["ms_instrument"], "centroidMatchTol"]}</centroidMatchTol>
               <centroidMatchTolInPpm>True</centroidMatchTolInPpm>
               <centroidHalfWidth>35</centroidHalfWidth>
               <centroidHalfWidthInPpm>True</centroidHalfWidthInPpm>
               <valleyFactor>{ms_instrument_settings.at[par["ms_instrument"], "valleyFactor"]}</valleyFactor>
               <isotopeValleyFactor>1.2</isotopeValleyFactor>
               <advancedPeakSplitting>{ms_instrument_settings.at[par["ms_instrument"], "advancedPeakSplitting"]}</advancedPeakSplitting>
               <intensityThresholdMs1>{ms_instrument_settings.at[par["ms_instrument"], "intensityThresholdMs1"]}</intensityThresholdMs1>
               <intensityThresholdMs2>{ms_instrument_settings.at[par["ms_instrument"], "intensityThresholdMs2"]}</intensityThresholdMs2>
               <labelMods>
                  <string></string>
               </labelMods>
               <lcmsRunType>{par["lcms_run_type"]}</lcmsRunType>
               <reQuantify>False</reQuantify>
               <lfqMode>{"1" if par["lfq_mode"] == "LFQ" else "0"}</lfqMode>
               <lfqNormClusterSize>80</lfqNormClusterSize>
               <lfqMinEdgesPerNode>3</lfqMinEdgesPerNode>
               <lfqAvEdgesPerNode>6</lfqAvEdgesPerNode>
               <lfqMaxFeatures>100000</lfqMaxFeatures>
               <neucodeMaxPpm>{group_type_settings.at[par["lcms_run_type"], "neucodeMaxPpm"]}</neucodeMaxPpm>
               <neucodeResolution>{group_type_settings.at[par["lcms_run_type"], "neucodeResolution"]}</neucodeResolution>
               <neucodeResolutionInMda>{group_type_settings.at[par["lcms_run_type"], "neucodeResolutionInMda"]}</neucodeResolutionInMda>
               <neucodeInSilicoLowRes>{group_type_settings.at[par["lcms_run_type"], "neucodeInSilicoLowRes"]}</neucodeInSilicoLowRes>
               <fastLfq>True</fastLfq>
               <lfqRestrictFeatures>False</lfqRestrictFeatures>
               <lfqMinRatioCount>2</lfqMinRatioCount>
               <maxLabeledAa>{group_type_settings.at[par["lcms_run_type"], "maxLabeledAa"]}</maxLabeledAa>
               <maxNmods>5</maxNmods>
               <maxMissedCleavages>2</maxMissedCleavages>
               <multiplicity>1</multiplicity>
               <enzymeMode>0</enzymeMode>
               <complementaryReporterType>0</complementaryReporterType>
               <reporterNormalization>0</reporterNormalization>
               <neucodeIntensityMode>0</neucodeIntensityMode>
               <fixedModifications>
                  <string>Carbamidomethyl (C)</string>
               </fixedModifications>
               <enzymes>
                  <string>Trypsin/P</string>
               </enzymes>
               <enzymesFirstSearch>
               </enzymesFirstSearch>
               <enzymeModeFirstSearch>0</enzymeModeFirstSearch>
               <useEnzymeFirstSearch>False</useEnzymeFirstSearch>
               <useVariableModificationsFirstSearch>False</useVariableModificationsFirstSearch>
               <variableModifications>
                  <string>Oxidation (M)</string>
                  <string>Acetyl (Protein N-term)</string>
               </variableModifications>
               <useMultiModification>False</useMultiModification>
               <multiModifications>
               </multiModifications>
               <isobaricLabels>
               </isobaricLabels>
               <neucodeLabels>
               </neucodeLabels>
               <variableModificationsFirstSearch>
               </variableModificationsFirstSearch>
               <hasAdditionalVariableModifications>False</hasAdditionalVariableModifications>
               <additionalVariableModifications>
               </additionalVariableModifications>
               <additionalVariableModificationProteins>
               </additionalVariableModificationProteins>
               <doMassFiltering>True</doMassFiltering>
               <firstSearchTol>20</firstSearchTol>
               <mainSearchTol>{ms_instrument_settings.at[par["ms_instrument"], "mainSearchTol"]}</mainSearchTol>
               <searchTolInPpm>True</searchTolInPpm>
               <isotopeMatchTol>{ms_instrument_settings.at[par["ms_instrument"], "isotopeMatchTol"]}</isotopeMatchTol>
               <isotopeMatchTolInPpm>{ms_instrument_settings.at[par["ms_instrument"], "isotopeMatchTolInPpm"]}</isotopeMatchTolInPpm>
               <isotopeTimeCorrelation>0.6</isotopeTimeCorrelation>
               <theorIsotopeCorrelation>0.6</theorIsotopeCorrelation>
               <checkMassDeficit>{ms_instrument_settings.at[par["ms_instrument"], "checkMassDeficit"]}</checkMassDeficit>
               <recalibrationInPpm>True</recalibrationInPpm>
               <intensityDependentCalibration>{ms_instrument_settings.at[par["ms_instrument"], "intensityDependentCalibration"]}</intensityDependentCalibration>
               <minScoreForCalibration>{ms_instrument_settings.at[par["ms_instrument"], "minScoreForCalibration"]}</minScoreForCalibration>
               <matchLibraryFile>False</matchLibraryFile>
               <libraryFile></libraryFile>
               <matchLibraryMassTolPpm>0</matchLibraryMassTolPpm>
               <matchLibraryTimeTolMin>0</matchLibraryTimeTolMin>
               <matchLabelTimeTolMin>0</matchLabelTimeTolMin>
               <reporterMassTolerance>NaN</reporterMassTolerance>
               <reporterPif>{group_type_settings.at[par["lcms_run_type"], "reporterPif"]}</reporterPif>
               <filterPif>False</filterPif>
               <reporterFraction>{group_type_settings.at[par["lcms_run_type"], "reporterFraction"]}</reporterFraction>
               <reporterBasePeakRatio>{group_type_settings.at[par["lcms_run_type"], "reporterBasePeakRatio"]}</reporterBasePeakRatio>
               <timsHalfWidth>{group_type_settings.at[par["lcms_run_type"], "timsHalfWidth"]}</timsHalfWidth>
               <timsStep>{group_type_settings.at[par["lcms_run_type"], "timsStep"]}</timsStep>
               <timsResolution>{group_type_settings.at[par["lcms_run_type"], "timsResolution"]}</timsResolution>
               <timsMinMsmsIntensity>{group_type_settings.at[par["lcms_run_type"], "timsMinMsmsIntensity"]}</timsMinMsmsIntensity>
               <timsRemovePrecursor>True</timsRemovePrecursor>
               <timsIsobaricLabels>False</timsIsobaricLabels>
               <timsCollapseMsms>True</timsCollapseMsms>
               <crossLinkingType>0</crossLinkingType>
               <crossLinker></crossLinker>
               <minMatchXl>3</minMatchXl>
               <minPairedPepLenXl>6</minPairedPepLenXl>
               <minScore_Dipeptide>40</minScore_Dipeptide>
               <minScore_Monopeptide>0</minScore_Monopeptide>
               <minScore_PartialCross>10</minScore_PartialCross>
               <crosslinkOnlyIntraProtein>False</crosslinkOnlyIntraProtein>
               <crosslinkIntensityBasedPrecursor>True</crosslinkIntensityBasedPrecursor>
               <isHybridPrecDetermination>False</isHybridPrecDetermination>
               <topXcross>3</topXcross>
               <doesSeparateInterIntraProteinCross>False</doesSeparateInterIntraProteinCross>
               <crosslinkMaxMonoUnsaturated>0</crosslinkMaxMonoUnsaturated>
               <crosslinkMaxMonoSaturated>0</crosslinkMaxMonoSaturated>
               <crosslinkMaxDiUnsaturated>0</crosslinkMaxDiUnsaturated>
               <crosslinkMaxDiSaturated>0</crosslinkMaxDiSaturated>
               <crosslinkModifications>
               </crosslinkModifications>
               <crosslinkFastaFiles>
               </crosslinkFastaFiles>
               <crosslinkSites>
               </crosslinkSites>
               <crosslinkNetworkFiles>
               </crosslinkNetworkFiles>
               <crosslinkMode></crosslinkMode>
               <peakRefinement>False</peakRefinement>
               <isobaricSumOverWindow>True</isobaricSumOverWindow>
               <isobaricWeightExponent>0.75</isobaricWeightExponent>
               <collapseMsmsOnIsotopePatterns>False</collapseMsmsOnIsotopePatterns>
               <diaLibraryType>0</diaLibraryType>
               <diaLibraryPaths>
               </diaLibraryPaths>
               <diaPeptidePaths>
               </diaPeptidePaths>
               <diaEvidencePaths>
               </diaEvidencePaths>
               <diaMsmsPaths>
               </diaMsmsPaths>
               <diaInitialPrecMassTolPpm>20</diaInitialPrecMassTolPpm>
               <diaInitialFragMassTolPpm>20</diaInitialFragMassTolPpm>
               <diaCorrThresholdFeatureClustering>0.85</diaCorrThresholdFeatureClustering>
               <diaPrecTolPpmFeatureClustering>2</diaPrecTolPpmFeatureClustering>
               <diaFragTolPpmFeatureClustering>2</diaFragTolPpmFeatureClustering>
               <diaScoreN>7</diaScoreN>
               <diaMinScore>1.99</diaMinScore>
               <diaXgBoostBaseScore>0.4</diaXgBoostBaseScore>
               <diaXgBoostSubSample>0.9</diaXgBoostSubSample>
               <centroidPosition>0</centroidPosition>
               <diaQuantMethod>7</diaQuantMethod>
               <diaFeatureQuantMethod>2</diaFeatureQuantMethod>
               <lfqNormType>1</lfqNormType>
               <diaTopNForQuant>{ms_instrument_settings.at[par["ms_instrument"], "diaTopNForQuant"]}</diaTopNForQuant>
               <diaMinMsmsIntensityForQuant>0</diaMinMsmsIntensityForQuant>
               <diaTopMsmsIntensityQuantileForQuant>0.85</diaTopMsmsIntensityQuantileForQuant>
               <diaPrecursorFilterType>0</diaPrecursorFilterType>
               <diaMinFragmentOverlapScore>1</diaMinFragmentOverlapScore>
               <diaMinPrecursorScore>0.5</diaMinPrecursorScore>
               <diaMinProfileCorrelation>0</diaMinProfileCorrelation>
               <diaXgBoostMinChildWeight>9</diaXgBoostMinChildWeight>
               <diaXgBoostMaximumTreeDepth>12</diaXgBoostMaximumTreeDepth>
               <diaXgBoostEstimators>580</diaXgBoostEstimators>
               <diaXgBoostGamma>0.9</diaXgBoostGamma>
               <diaXgBoostMaxDeltaStep>3</diaXgBoostMaxDeltaStep>
               <diaGlobalMl>True</diaGlobalMl>
               <diaAdaptiveMassAccuracy>False</diaAdaptiveMassAccuracy>
               <diaMassWindowFactor>3.3</diaMassWindowFactor>
               <diaRtPrediction>False</diaRtPrediction>
               <diaRtPredictionSecondRound>False</diaRtPredictionSecondRound>
               <diaNoMl>False</diaNoMl>
               <diaPermuteRt>False</diaPermuteRt>
               <diaPermuteCcs>False</diaPermuteCcs>
               <diaBackgroundSubtraction>{ms_instrument_settings.at[par["ms_instrument"], "diaBackgroundSubtraction"]}</diaBackgroundSubtraction>
               <diaBackgroundSubtractionQuantile>{ms_instrument_settings.at[par["ms_instrument"], "diaBackgroundSubtractionQuantile"]}</diaBackgroundSubtractionQuantile>
               <diaBackgroundSubtractionFactor>4</diaBackgroundSubtractionFactor>
               <diaLfqWeightedMedian>{ms_instrument_settings.at[par["ms_instrument"], "diaLfqWeightedMedian"]}</diaLfqWeightedMedian>
               <diaTransferQvalue>0.3</diaTransferQvalue>
               <diaOnlyIsosForRecal>True</diaOnlyIsosForRecal>
               <diaMinPeaksForRecal>5</diaMinPeaksForRecal>
               <diaUseFragIntensForMl>False</diaUseFragIntensForMl>
               <diaUseFragMassesForMl>False</diaUseFragMassesForMl>
               <diaMaxTrainInstances>1000000</diaMaxTrainInstances>
            </parameterGroup>
         </parameterGroups>
         <msmsParamsArray>
            <msmsParams>
               <Name>FTMS</Name>
               <MatchTolerance>20</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>7</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>True</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>12</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>ITMS</Name>
               <MatchTolerance>0.5</MatchTolerance>
               <MatchToleranceInPpm>False</MatchToleranceInPpm>
               <DeisotopeTolerance>0.15</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>False</DeisotopeToleranceInPpm>
               <DeNovoTolerance>0.5</DeNovoTolerance>
               <DeNovoToleranceInPpm>False</DeNovoToleranceInPpm>
               <Deisotope>False</Deisotope>
               <Topx>8</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>TOF</Name>
               <MatchTolerance>40</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>0.01</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>False</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>10</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>Unknown</Name>
               <MatchTolerance>20</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>7</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>True</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>12</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
         </msmsParamsArray>
         <fragmentationParamsArray>
            <fragmentationParams>
               <Name>CID</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>HCD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>PQD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETHCD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETCID</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>UVPD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>Unknown</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
         </fragmentationParamsArray>
      </MaxQuantParams>
      """
      
         with open(param_file, "w") as f:
            f.write(param_content)
      
         if not par["dryrun"]:
            # copy input files
            for old, new in zip(old_inputs, new_inputs):
               if (os.path.isdir(old)):
                  shutil.copytree(old, new)
               else:
                  shutil.copyfile(old, new)
               
            
            # run maxquant
            p = subprocess.Popen(
               ["dotnet", "/maxquant/bin/MaxQuantCmd.exe", os.path.basename(param_file)], 
               # ["maxquant", os.path.basename(param_file)], 
               cwd=os.path.dirname(param_file)
            )
            p.wait()
      
            if p.returncode != 0:
               raise Exception(f"MaxQuant finished with exit code {p.returncode}") 
      
      VIASHMAIN
      python "$tempscript"
      '''
      
      thisDefaultProcessArgs = [
        // key to be used to trace the process and determine output names
        key: thisConfig.functionality.name,
        // fixed arguments to be passed to script
        args: [:],
        // default directives
        directives: jsonSlurper.parseText('''{
        "container" : {
          "registry" : "ghcr.io",
          "image" : "czbiohub/mspipelines/maxquant_maxquant",
          "tag" : "0.3.1"
        },
        "label" : [
          "highmem",
          "highcpu"
        ]
      }'''),
        // auto settings
        auto: jsonSlurper.parseText('''{
        "simplifyInput" : true,
        "simplifyOutput" : true,
        "transcript" : false,
        "publish" : false
      }'''),
        // apply a map over the incoming tuple
        // example: { tup -> [ tup[0], [input: tup[1].output], tup[2] ] }
        map: null,
        // apply a map over the ID element of a tuple (i.e. the first element)
        // example: { id -> id + "_foo" }
        mapId: null,
        // apply a map over the data element of a tuple (i.e. the second element)
        // example: { data -> [ input: data.output ] }
        mapData: null,
        // apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements)
        // example: { pt -> pt.drop(1) }
        mapPassthrough: null,
        // rename keys in the data field of the tuple (i.e. the second element)
        // example: [ "new_key": "old_key" ]
        renameKeys: null,
        // whether or not to print debug messages
        debug: false
      ]
      
      // END CUSTOM CODE
      
      /////////////////////////////////////
      // Viash Workflow helper functions //
      /////////////////////////////////////
      
      import java.util.regex.Pattern
      import java.io.BufferedReader
      import java.io.FileReader
      import java.nio.file.Paths
      import groovy.json.JsonSlurper
      import groovy.text.SimpleTemplateEngine
      import org.yaml.snakeyaml.Yaml
      
      // param helpers //
      def paramExists(name) {
        return params.containsKey(name) && params[name] != ""
      }
      
      def assertParamExists(name, description) {
        if (!paramExists(name)) {
          exit 1, "ERROR: Please provide a --${name} parameter ${description}"
        }
      }
      
      // helper functions for reading params from file //
      def getChild(parent, child) {
        if (child.contains("://") || Paths.get(child).isAbsolute()) {
          child
        } else {
          parent.replaceAll('/[^/]*$', "/") + child
        }
      }
      
      def readCsv(file) {
        def output = []
        def inputFile = file !instanceof File ? new File(file) : file
      
        // todo: allow escaped quotes in string
        // todo: allow single quotes?
        def splitRegex = Pattern.compile(''',(?=(?:[^"]*"[^"]*")*[^"]*$)''')
        def removeQuote = Pattern.compile('''"(.*)"''')
      
        def br = new BufferedReader(new FileReader(inputFile))
      
        def row = -1
        def header = null
        while (br.ready() && header == null) {
          def line = br.readLine()
          row++
          if (!line.startsWith("#")) {
            header = splitRegex.split(line, -1).collect{field ->
              m = removeQuote.matcher(field)
              m.find() ? m.replaceFirst('$1') : field
            }
          }
        }
        assert header != null: "CSV file should contain a header"
      
        while (br.ready()) {
          def line = br.readLine()
          row++
          if (!line.startsWith("#")) {
            def predata = splitRegex.split(line, -1)
            def data = predata.collect{field ->
              if (field == "") {
                return null
              }
              m = removeQuote.matcher(field)
              if (m.find()) {
                return m.replaceFirst('$1')
              } else {
                return field
              }
            }
            assert header.size() == data.size(): "Row $row should contain the same number as fields as the header"
            
            def dataMap = [header, data].transpose().collectEntries().findAll{it.value != null}
            output.add(dataMap)
          }
        }
      
        output
      }
      
      def readJsonBlob(str) {
        def jsonSlurper = new JsonSlurper()
        jsonSlurper.parseText(str)
      }
      
      def readJson(file) {
        def inputFile = file !instanceof File ? new File(file) : file
        def jsonSlurper = new JsonSlurper()
        jsonSlurper.parse(inputFile)
      }
      
      def readYamlBlob(str) {
        def yamlSlurper = new Yaml()
        yamlSlurper.load(str)
      }
      
      def readYaml(file) {
        def inputFile = file !instanceof File ? new File(file) : file
        def yamlSlurper = new Yaml()
        yamlSlurper.load(inputFile)
      }
      
      // helper functions for reading a viash config in groovy //
      
      // based on how Functionality.scala is implemented
      def processArgument(arg) {
        arg.multiple = arg.multiple ?: false
        arg.required = arg.required ?: false
        arg.direction = arg.direction ?: "input"
        arg.multiple_sep = arg.multiple_sep ?: ":"
        arg.plainName = arg.name.replaceAll("^-*", "")
      
        if (arg.type == "file" && arg.direction == "output") {
          def mult = arg.multiple ? "_*" : ""
          def extSearch = ""
          if (arg.default != null) {
            extSearch = arg.default
          } else if (arg.example != null) {
            extSearch = arg.example
          }
          if (extSearch instanceof List) {
            extSearch = extSearch[0]
          }
          def ext = extSearch.find("\\.[^\\.]+\$") ?: ""
          arg.default = "\$id.\$key.${arg.plainName}${mult}${ext}"
        }
      
        if (!arg.multiple) {
          if (arg.default != null && arg.default instanceof List) {
            arg.default = arg.default[0]
          }
          if (arg.example != null && arg.example instanceof List) {
            arg.example = arg.example[0]
          }
        }
      
        if (arg.type == "boolean_true") {
          arg.default = false
        }
        if (arg.type == "boolean_false") {
          arg.default = true
        }
      
        arg
      }
      
      // based on how Functionality.scala is implemented
      def processArgumentGroup(argumentGroups, name, arguments) {
        def argNamesInGroups = argumentGroups.collect{it.arguments}.flatten().toSet()
      
        // Check if 'arguments' is in 'argumentGroups'. 
        def argumentsNotInGroup = arguments.collect{it.plainName}.findAll{argName -> !argNamesInGroups.contains(argName)}
      
        // Check whether an argument group of 'name' exists.
        def existing = argumentGroups.find{gr -> name == gr.name}
      
        if (argumentsNotInGroup.isEmpty()) {
          return existing == null ? [] : [existing]
        } else if (existing != null) {
          def newEx = existing.clone()
          newEx.arguments.addAll(argumentsNotInGroup)
          return [newEx]
        } else {
          def newEx = [name: name, arguments: argumentsNotInGroup]
          return [newEx]
        }
      }
      
      // based on how Functionality.scala is implemented
      def processConfig(config) {
        // TODO: assert .functionality etc.
      
        // set defaults for inputs
        config.functionality.inputs = 
          (config.functionality.inputs ?: []).collect{arg ->
            arg.type = arg.type ?: "file"
            arg.direction = "input"
            processArgument(arg)
          }
        // set defaults for outputs
        config.functionality.outputs = 
          (config.functionality.outputs ?: []).collect{arg ->
            arg.type = arg.type ?: "file"
            arg.direction = "output"
            processArgument(arg)
          }
        // set defaults for arguments
        config.functionality.arguments = 
          (config.functionality.arguments ?: []).collect{arg ->
            processArgument(arg)
          }
        // create combined arguments list
        config.functionality.allArguments = 
          config.functionality.inputs +
          config.functionality.outputs +
          config.functionality.arguments
        
        // remove leading dashes for argument names in argument groups
        def argGroups = 
          (config.functionality.argument_groups ?: []).collect{grp ->
            grp.arguments = (grp.arguments ?: []).collect{arg_name -> arg_name.replaceAll("^-*", "")}
            grp
          }
        
        // add missing argument groups (based on Functionality::allArgumentGroups())
        def inputGroup = processArgumentGroup(argGroups, "Inputs", config.functionality.inputs)
        def outputGroup = processArgumentGroup(argGroups, "Outputs", config.functionality.outputs)
        def defaultGroup = processArgumentGroup(argGroups, "Arguments", config.functionality.arguments)
        def groupsFiltered = argGroups.findAll(gr -> !(["Inputs", "Outputs", "Arguments"].contains(gr.name)))
        config.functionality.argument_groups = inputGroup + outputGroup + groupsFiltered + defaultGroup
      
        config
      }
      
      def readConfig(file) {
        def config = readYaml(file)
        processConfig(config)
      }
      
      // recursively merge two maps
      def mergeMap(Map lhs, Map rhs) {
        return rhs.inject(lhs.clone()) { map, entry ->
          if (map[entry.key] instanceof Map && entry.value instanceof Map) {
            map[entry.key] = mergeMap(map[entry.key], entry.value)
          } else if (map[entry.key] instanceof Collection && entry.value instanceof Collection) {
            map[entry.key] += entry.value
          } else {
            map[entry.key] = entry.value
          }
          return map
        }
      }
      
      def addGlobalParams(config) {
        def localConfig = [
          "functionality" : [
            "arguments": [
              [
                'name': '--publish_dir',
                'required': true,
                'type': 'string',
                'description': 'Path to an output directory.',
                'example': 'output/',
                'multiple': false
              ],
              [
                'name': '--param_list',
                'required': false,
                'type': 'string',
                'description': '''Allows inputting multiple parameter sets to initialise a Nextflow channel. Possible formats are csv, json, yaml, or simply a yaml_blob.
                |A csv should have column names which correspond to the different arguments of this pipeline.
                |A json or a yaml file should be a list of maps, each of which has keys corresponding to the arguments of the pipeline.
                |A yaml blob can also be passed directly as a parameter.
                |Inside the Nextflow pipeline code, params.params_list can also be used to directly a list of parameter sets.
                |When passing a csv, json or yaml, relative path names are relativized to the location of the parameter file.'''.stripMargin(),
                'example': 'my_params.yaml',
                'multiple': false
              ],
              [
                'name': '--param_list_format',
                'required': false,
                'type': 'string',
                'description': 'Manually specify the param_list_format. Must be one of \'csv\', \'json\', \'yaml\', \'yaml_blob\', \'asis\' or \'none\'.',
                'example': 'yaml',
                'choices': ['csv', 'json', 'yaml', 'yaml_blob', 'asis', 'none'],
                'multiple': false
              ],
            ],
            "argument_groups": [
              [
                "name": "Nextflow input-output arguments",
                "arguments" : [ "publish_dir", "param_list", "param_list_format" ]
              ]
            ]
          ]
        ]
      
        return processConfig(mergeMap(config, localConfig))
      }
      
      // helper functions for generating help // 
      
      // based on io.viash.helpers.Format.wordWrap
      def formatWordWrap(str, maxLength) {
        def words = str.split("\\s").toList()
      
        def word = null
        def line = ""
        def lines = []
        while(!words.isEmpty()) {
          word = words.pop()
          if (line.length() + word.length() + 1 <= maxLength) {
            line = line + " " + word
          } else {
            lines.add(line)
            line = word
          }
          if (words.isEmpty()) {
            lines.add(line)
          }
        }
        return lines
      }
      
      // based on Format.paragraphWrap
      def paragraphWrap(str, maxLength) {
        def outLines = []
        str.split("\n").each{par ->
          def words = par.split("\\s").toList()
      
          def word = null
          def line = words.pop()
          while(!words.isEmpty()) {
            word = words.pop()
            if (line.length() + word.length() + 1 <= maxLength) {
              line = line + " " + word
            } else {
              outLines.add(line)
              line = word
            }
          }
          if (words.isEmpty()) {
            outLines.add(line)
          }
        }
        return outLines
      }
      
      def generateArgumentHelp(param) {
        // alternatives are not supported
        // def names = param.alternatives ::: List(param.name)
      
        def unnamedProps = [
          ["required parameter", param.required],
          ["multiple values allowed", param.multiple],
          ["output", param.direction.toLowerCase() == "output"],
          ["file must exist", param.type == "file" && param.must_exist]
        ].findAll{it[1]}.collect{it[0]}
        
        def dflt = null
        if (param.default != null) {
          if (param.default instanceof List) {
            dflt = param.default.join(param.multiple_sep ?: ", ")
          } else {
            dflt = param.default.toString()
          }
        }
        def example = null
        if (param.example != null) {
          if (param.example instanceof List) {
            example = param.example.join(param.multiple_sep ?: ", ")
          } else {
            example = param.example.toString()
          }
        }
        def min = param.min?.toString()
        def max = param.max?.toString()
      
        def escapeChoice = { choice ->
          def s1 = choice.replaceAll("\\n", "\\\\n")
          def s2 = s1.replaceAll("\"", """\\\"""")
          s2.contains(",") || s2 != choice ? "\"" + s2 + "\"" : s2
        }
        def choices = param.choices == null ? 
          null : 
          "[ " + param.choices.collect{escapeChoice(it.toString())}.join(", ") + " ]"
      
        def namedPropsStr = [
          ["type", ([param.type] + unnamedProps).join(", ")],
          ["default", dflt],
          ["example", example],
          ["choices", choices],
          ["min", min],
          ["max", max]
        ]
          .findAll{it[1]}
          .collect{"\n        " + it[0] + ": " + it[1].replaceAll("\n", "\\n")}
          .join("")
        
        def descStr = param.description == null ?
          "" :
          paragraphWrap("\n" + param.description.trim(), 80 - 8).join("\n        ")
        
        "\n    --" + param.plainName +
          namedPropsStr +
          descStr
      }
      
      def generateHelp(config) {
        def fun = config.functionality
      
        // PART 1: NAME AND VERSION
        def nameStr = fun.name + 
          (fun.version == null ? "" : " " + fun.version)
      
        // PART 2: DESCRIPTION
        def descrStr = fun.description == null ? 
          "" :
          "\n\n" + paragraphWrap(fun.description.trim(), 80).join("\n")
      
        // PART 3: Usage
        def usageStr = fun.usage == null ? 
          "" :
          "\n\nUsage:\n" + fun.usage.trim()
      
        // PART 4: Options
        def argGroupStrs = fun.argument_groups.collect{argGroup ->
          def name = argGroup.name
          def descriptionStr = argGroup.description == null ?
            "" :
            "\n    " + paragraphWrap(argGroup.description.trim(), 80-4).join("\n    ") + "\n"
          def arguments = argGroup.arguments.collect{argName -> 
            fun.allArguments.find{it.plainName == argName}
          }.findAll{it != null}
          def argumentStrs = arguments.collect{param -> generateArgumentHelp(param)}
          
          "\n\n$name:" +
            descriptionStr +
            argumentStrs.join("\n")
        }
      
        // FINAL: combine
        def out = nameStr + 
          descrStr +
          usageStr + 
          argGroupStrs.join("")
      
        return out
      }
      
      def helpMessage(config) {
        if (paramExists("help")) {
          def mergedConfig = addGlobalParams(config)
          def helpStr = generateHelp(mergedConfig)
          println(helpStr)
          exit 0
        }
      }
      
      def guessMultiParamFormat(params) {
        if (!params.containsKey("param_list") || params.param_list == null) {
          "none"
        } else if (params.containsKey("multiParamsFormat")) {
          params.multiParamsFormat
        } else {
          def param_list = params.param_list
      
          if (param_list !instanceof String) {
            "asis"
          } else if (param_list.endsWith(".csv")) {
            "csv"
          } else if (param_list.endsWith(".json") || param_list.endsWith(".jsn")) {
            "json"
          } else if (param_list.endsWith(".yaml") || param_list.endsWith(".yml")) {
            "yaml"
          } else {
            "yaml_blob"
          }
        }
      }
      
      def paramsToList(params, config) {
        // fetch default params from functionality
        def defaultArgs = config.functionality.allArguments
          .findAll { it.containsKey("default") }
          .collectEntries { [ it.plainName, it.default ] }
      
        // fetch overrides in params
        def paramArgs = config.functionality.allArguments
          .findAll { params.containsKey(it.plainName) }
          .collectEntries { [ it.plainName, params[it.plainName] ] }
        
        // check multi input params
        // objects should be closures and not functions, thanks to FunctionDef
        def multiParamFormat = guessMultiParamFormat(params)
      
        def multiOptionFunctions = [ 
          "csv": {[it, readCsv(it)]},
          "json": {[it, readJson(it)]},
          "yaml": {[it, readYaml(it)]},
          "yaml_blob": {[null, readYamlBlob(it)]},
          "asis": {[null, it]},
          "none": {[null, [[:]]]}
        ]
        assert multiOptionFunctions.containsKey(multiParamFormat): 
          "Format of provided --param_list not recognised.\n" +
          "You can use '--param_list_format' to manually specify the format.\n" +
          "Found: '$multiParamFormat'. Expected: one of 'csv', 'json', 'yaml', 'yaml_blob', 'asis' or 'none'"
      
        // fetch multi param inputs
        def multiOptionFun = multiOptionFunctions.get(multiParamFormat)
        // todo: add try catch
        def multiOptionOut = multiOptionFun(params.containsKey("param_list") ? params.param_list : "")
        def paramList = multiOptionOut[1]
        def multiFile = multiOptionOut[0]
      
        // data checks
        assert paramList instanceof List: "--param_list should contain a list of maps"
        for (value in paramList) {
          assert value instanceof Map: "--param_list should contain a list of maps"
        }
        
        // combine parameters
        def processedParams = paramList.collect{ multiParam ->
          // combine params
          def combinedArgs = defaultArgs + paramArgs + multiParam
      
          // check whether required arguments exist
          config.functionality.allArguments
            .findAll { it.required }
            .forEach { par ->
              assert combinedArgs.containsKey(par.plainName): "Argument ${par.plainName} is required but does not have a value"
            }
          
          // process arguments
          def inputs = config.functionality.allArguments
            .findAll{ par -> combinedArgs.containsKey(par.plainName) }
            .collectEntries { par ->
              // split on 'multiple_sep'
              if (par.multiple) {
                parData = combinedArgs[par.plainName]
                if (parData instanceof List) {
                  parData = parData.collect{it instanceof String ? it.split(par.multiple_sep) : it }
                } else if (parData instanceof String) {
                  parData = parData.split(par.multiple_sep)
                } else if (parData == null) {
                  parData = []
                } else {
                  parData = [ parData ]
                }
              } else {
                parData = [ combinedArgs[par.plainName] ]
              }
      
              // flatten
              parData = parData.flatten()
      
              // cast types
              if (par.type == "file" && ((par.direction ?: "input") == "input")) {
                parData = parData.collect{path ->
                  if (path !instanceof String) {
                    path
                  } else if (multiFile) {
                    file(getChild(multiFile, path))
                  } else {
                    file(path)
                  }
                }.flatten()
              } else if (par.type == "integer") {
                parData = parData.collect{it as Integer}
              } else if (par.type == "double") {
                parData = parData.collect{it as Double}
              } else if (par.type == "boolean" || par.type == "boolean_true" || par.type == "boolean_false") {
                parData = parData.collect{it as Boolean}
              }
              // simplify list to value if need be
              if (!par.multiple) {
                assert parData.size() == 1 : 
                  "Error: argument ${par.plainName} has too many values.\n" +
                  "  Expected amount: 1. Found: ${parData.length}"
                parData = parData[0]
              }
      
              // return pair
              [ par.plainName, parData ]
            }
            // remove parameters which were explicitly set to null
            .findAll{ par -> par != null }
          }
          
        
        // check processed params
        processedParams.forEach { args ->
          assert args.containsKey("id"): "Each argument set should have an 'id'. Argument set: $args"
        }
        def ppIds = processedParams.collect{it.id}
        assert ppIds.size() == ppIds.unique().size() : "All argument sets should have unique ids. Detected ids: $ppIds"
      
        processedParams
      }
      
      def paramsToChannel(params, config) {
        Channel.fromList(paramsToList(params, config))
      }
      
      def viashChannel(params, config) {
        paramsToChannel(params, config)
          | map{tup -> [tup.id, tup]}
      }
      
      ////////////////////////////
      // VDSL3 helper functions //
      ////////////////////////////
      
      import nextflow.Nextflow
      import nextflow.script.IncludeDef
      import nextflow.script.ScriptBinding
      import nextflow.script.ScriptMeta
      import nextflow.script.ScriptParser
      
      // retrieve resourcesDir here to make sure the correct path is found
      resourcesDir = ScriptMeta.current().getScriptPath().getParent()
      
      def assertMapKeys(map, expectedKeys, requiredKeys, mapName) {
        assert map instanceof Map : "Expected argument '$mapName' to be a Map. Found: class ${map.getClass()}"
        map.forEach { key, val -> 
          assert key in expectedKeys : "Unexpected key '$key' in ${mapName ? mapName + " " : ""}map"
        }
        requiredKeys.forEach { requiredKey -> 
          assert map.containsKey(requiredKey) : "Missing required key '$key' in ${mapName ? mapName + " " : ""}map"
        }
      }
      
      // TODO: unit test processDirectives
      def processDirectives(Map drctv) {
        // remove null values
        drctv = drctv.findAll{k, v -> v != null}
      
        /* DIRECTIVE accelerator
          accepted examples:
          - [ limit: 4, type: "nvidia-tesla-k80" ]
        */
        if (drctv.containsKey("accelerator")) {
          assertMapKeys(drctv["accelerator"], ["type", "limit", "request", "runtime"], [], "accelerator")
        }
      
        /* DIRECTIVE afterScript
          accepted examples:
          - "source /cluster/bin/cleanup"
        */
        if (drctv.containsKey("afterScript")) {
          assert drctv["afterScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE beforeScript
          accepted examples:
          - "source /cluster/bin/setup"
        */
        if (drctv.containsKey("beforeScript")) {
          assert drctv["beforeScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE cache
          accepted examples:
          - true
          - false
          - "deep"
          - "lenient"
        */
        if (drctv.containsKey("cache")) {
          assert drctv["cache"] instanceof CharSequence || drctv["cache"] instanceof Boolean
          if (drctv["cache"] instanceof CharSequence) {
            assert drctv["cache"] in ["deep", "lenient"] : "Unexpected value for cache"
          }
        }
      
        /* DIRECTIVE conda
          accepted examples:
          - "bwa=0.7.15"
          - "bwa=0.7.15 fastqc=0.11.5"
          - ["bwa=0.7.15", "fastqc=0.11.5"]
        */
        if (drctv.containsKey("conda")) {
          if (drctv["conda"] instanceof List) {
            drctv["conda"] = drctv["conda"].join(" ")
          }
          assert drctv["conda"] instanceof CharSequence
        }
      
        /* DIRECTIVE container
          accepted examples:
          - "foo/bar:tag"
          - [ registry: "reg", image: "im", tag: "ta" ]
            is transformed to "reg/im:ta"
          - [ image: "im" ] 
            is transformed to "im:latest"
        */
        if (drctv.containsKey("container")) {
          assert drctv["container"] instanceof Map || drctv["container"] instanceof CharSequence
          if (drctv["container"] instanceof Map) {
            def m = drctv["container"]
            assertMapKeys(m, [ "registry", "image", "tag" ], ["image"], "container")
            def part1 = 
              System.getenv('OVERRIDE_CONTAINER_REGISTRY') ? System.getenv('OVERRIDE_CONTAINER_REGISTRY') + "/" : 
              params.containsKey("override_container_registry") ? params["override_container_registry"] + "/" : // todo: remove?
              m.registry ? m.registry + "/" : 
              ""
            def part2 = m.image
            def part3 = m.tag ? ":" + m.tag : ":latest"
            drctv["container"] = part1 + part2 + part3
          }
        }
      
        /* DIRECTIVE containerOptions
          accepted examples:
          - "--foo bar"
          - ["--foo bar", "-f b"]
        */
        if (drctv.containsKey("containerOptions")) {
          if (drctv["containerOptions"] instanceof List) {
            drctv["containerOptions"] = drctv["containerOptions"].join(" ")
          }
          assert drctv["containerOptions"] instanceof CharSequence
        }
      
        /* DIRECTIVE cpus
          accepted examples:
          - 1
          - 10
        */
        if (drctv.containsKey("cpus")) {
          assert drctv["cpus"] instanceof Integer
        }
      
        /* DIRECTIVE disk
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("disk")) {
          assert drctv["disk"] instanceof CharSequence
          // assert drctv["disk"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE echo
          accepted examples:
          - true
          - false
        */
        if (drctv.containsKey("echo")) {
          assert drctv["echo"] instanceof Boolean
        }
      
        /* DIRECTIVE errorStrategy
          accepted examples:
          - "terminate"
          - "finish"
        */
        if (drctv.containsKey("errorStrategy")) {
          assert drctv["errorStrategy"] instanceof CharSequence
          assert drctv["errorStrategy"] in ["terminate", "finish", "ignore", "retry"] : "Unexpected value for errorStrategy"
        }
      
        /* DIRECTIVE executor
          accepted examples:
          - "local"
          - "sge"
        */
        if (drctv.containsKey("executor")) {
          assert drctv["executor"] instanceof CharSequence
          assert drctv["executor"] in ["local", "sge", "uge", "lsf", "slurm", "pbs", "pbspro", "moab", "condor", "nqsii", "ignite", "k8s", "awsbatch", "google-pipelines"] : "Unexpected value for executor"
        }
      
        /* DIRECTIVE machineType
          accepted examples:
          - "n1-highmem-8"
        */
        if (drctv.containsKey("machineType")) {
          assert drctv["machineType"] instanceof CharSequence
        }
      
        /* DIRECTIVE maxErrors
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxErrors")) {
          assert drctv["maxErrors"] instanceof Integer
        }
      
        /* DIRECTIVE maxForks
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxForks")) {
          assert drctv["maxForks"] instanceof Integer
        }
      
        /* DIRECTIVE maxRetries
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxRetries")) {
          assert drctv["maxRetries"] instanceof Integer
        }
      
        /* DIRECTIVE memory
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("memory")) {
          assert drctv["memory"] instanceof CharSequence
          // assert drctv["memory"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE module
          accepted examples:
          - "ncbi-blast/2.2.27"
          - "ncbi-blast/2.2.27:t_coffee/10.0"
          - ["ncbi-blast/2.2.27", "t_coffee/10.0"]
        */
        if (drctv.containsKey("module")) {
          if (drctv["module"] instanceof List) {
            drctv["module"] = drctv["module"].join(":")
          }
          assert drctv["module"] instanceof CharSequence
        }
      
        /* DIRECTIVE penv
          accepted examples:
          - "smp"
        */
        if (drctv.containsKey("penv")) {
          assert drctv["penv"] instanceof CharSequence
        }
      
        /* DIRECTIVE pod
          accepted examples:
          - [ label: "key", value: "val" ]
          - [ annotation: "key", value: "val" ]
          - [ env: "key", value: "val" ]
          - [ [label: "l", value: "v"], [env: "e", value: "v"]]
        */
        if (drctv.containsKey("pod")) {
          if (drctv["pod"] instanceof Map) {
            drctv["pod"] = [ drctv["pod"] ]
          }
          assert drctv["pod"] instanceof List
          drctv["pod"].forEach { pod ->
            assert pod instanceof Map
            // TODO: should more checks be added?
            // See https://www.nextflow.io/docs/latest/process.html?highlight=directives#pod
            // e.g. does it contain 'label' and 'value', or 'annotation' and 'value', or ...?
          }
        }
      
        /* DIRECTIVE publishDir
          accepted examples:
          - []
          - [ [ path: "foo", enabled: true ], [ path: "bar", enabled: false ] ]
          - "/path/to/dir" 
            is transformed to [[ path: "/path/to/dir" ]]
          - [ path: "/path/to/dir", mode: "cache" ]
            is transformed to [[ path: "/path/to/dir", mode: "cache" ]]
        */
        // TODO: should we also look at params["publishDir"]?
        if (drctv.containsKey("publishDir")) {
          def pblsh = drctv["publishDir"]
          
          // check different options
          assert pblsh instanceof List || pblsh instanceof Map || pblsh instanceof CharSequence
          
          // turn into list if not already so
          // for some reason, 'if (!pblsh instanceof List) pblsh = [ pblsh ]' doesn't work.
          pblsh = pblsh instanceof List ? pblsh : [ pblsh ]
      
          // check elements of publishDir
          pblsh = pblsh.collect{ elem ->
            // turn into map if not already so
            elem = elem instanceof CharSequence ? [ path: elem ] : elem
      
            // check types and keys
            assert elem instanceof Map : "Expected publish argument '$elem' to be a String or a Map. Found: class ${elem.getClass()}"
            assertMapKeys(elem, [ "path", "mode", "overwrite", "pattern", "saveAs", "enabled" ], ["path"], "publishDir")
      
            // check elements in map
            assert elem.containsKey("path")
            assert elem["path"] instanceof CharSequence
            if (elem.containsKey("mode")) {
              assert elem["mode"] instanceof CharSequence
              assert elem["mode"] in [ "symlink", "rellink", "link", "copy", "copyNoFollow", "move" ]
            }
            if (elem.containsKey("overwrite")) {
              assert elem["overwrite"] instanceof Boolean
            }
            if (elem.containsKey("pattern")) {
              assert elem["pattern"] instanceof CharSequence
            }
            if (elem.containsKey("saveAs")) {
              assert elem["saveAs"] instanceof CharSequence //: "saveAs as a Closure is currently not supported. Surround your closure with single quotes to get the desired effect. Example: '\{ foo \}'"
            }
            if (elem.containsKey("enabled")) {
              assert elem["enabled"] instanceof Boolean
            }
      
            // return final result
            elem
          }
          // store final directive
          drctv["publishDir"] = pblsh
        }
      
        /* DIRECTIVE queue
          accepted examples:
          - "long"
          - "short,long"
          - ["short", "long"]
        */
        if (drctv.containsKey("queue")) {
          if (drctv["queue"] instanceof List) {
            drctv["queue"] = drctv["queue"].join(",")
          }
          assert drctv["queue"] instanceof CharSequence
        }
      
        /* DIRECTIVE label
          accepted examples:
          - "big_mem"
          - "big_cpu"
          - ["big_mem", "big_cpu"]
        */
        if (drctv.containsKey("label")) {
          if (drctv["label"] instanceof CharSequence) {
            drctv["label"] = [ drctv["label"] ]
          }
          assert drctv["label"] instanceof List
          drctv["label"].forEach { label ->
            assert label instanceof CharSequence
            // assert label.matches("[a-zA-Z0-9]([a-zA-Z0-9_]*[a-zA-Z0-9])?")
            // ^ does not allow closures
          }
        }
      
        /* DIRECTIVE scratch
          accepted examples:
          - true
          - "/path/to/scratch"
          - '$MY_PATH_TO_SCRATCH'
          - "ram-disk"
        */
        if (drctv.containsKey("scratch")) {
          assert drctv["scratch"] == true || drctv["scratch"] instanceof CharSequence
        }
      
        /* DIRECTIVE storeDir
          accepted examples:
          - "/path/to/storeDir"
        */
        if (drctv.containsKey("storeDir")) {
          assert drctv["storeDir"] instanceof CharSequence
        }
      
        /* DIRECTIVE stageInMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageInMode")) {
          assert drctv["stageInMode"] instanceof CharSequence
          assert drctv["stageInMode"] in ["copy", "link", "symlink", "rellink"]
        }
      
        /* DIRECTIVE stageOutMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageOutMode")) {
          assert drctv["stageOutMode"] instanceof CharSequence
          assert drctv["stageOutMode"] in ["copy", "move", "rsync"]
        }
      
        /* DIRECTIVE tag
          accepted examples:
          - "foo"
          - '$id'
        */
        if (drctv.containsKey("tag")) {
          assert drctv["tag"] instanceof CharSequence
        }
      
        /* DIRECTIVE time
          accepted examples:
          - "1h"
          - "2days"
          - "1day 6hours 3minutes 30seconds"
        */
        if (drctv.containsKey("time")) {
          assert drctv["time"] instanceof CharSequence
          // todo: validation regex?
        }
      
        return drctv
      }
      
      // TODO: unit test processAuto
      def processAuto(Map auto) {
        // remove null values
        auto = auto.findAll{k, v -> v != null}
      
        expectedKeys = ["simplifyInput", "simplifyOutput", "transcript", "publish"]
      
        // check whether expected keys are all booleans (for now)
        for (key in expectedKeys) {
          assert auto.containsKey(key)
          assert auto[key] instanceof Boolean
        }
      
        return auto.subMap(expectedKeys)
      }
      
      def processProcessArgs(Map args) {
        // override defaults with args
        def processArgs = thisDefaultProcessArgs + args
      
        // check whether 'key' exists
        assert processArgs.containsKey("key")
      
        // if 'key' is a closure, apply it to the original key
        if (processArgs["key"] instanceof Closure) {
          processArgs["key"] = processArgs["key"](thisConfig.functionality.name)
        }
        assert processArgs["key"] instanceof CharSequence
        assert processArgs["key"] ==~ /^[a-zA-Z_][a-zA-Z0-9_]*$/
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("directives")
        assert processArgs["directives"] instanceof Map
        processArgs["directives"] = processDirectives(thisDefaultProcessArgs.directives + processArgs["directives"])
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("auto")
        assert processArgs["auto"] instanceof Map
        processArgs["auto"] = processAuto(thisDefaultProcessArgs.auto + processArgs["auto"])
      
        // auto define publish, if so desired
        if (processArgs.auto.publish == true && (processArgs.directives.publishDir ?: [:]).isEmpty()) {
          assert params.containsKey("publishDir") || params.containsKey("publish_dir") : 
            "Error in module '${processArgs['key']}': if auto.publish is true, params.publish_dir needs to be defined.\n" +
            "  Example: params.publish_dir = \"./output/\""
          def publishDir = params.containsKey("publish_dir") ? params.publish_dir : params.publishDir
          
          // TODO: more asserts on publishDir?
          processArgs.directives.publishDir = [[ 
            path: publishDir, 
            saveAs: "{ it.startsWith('.') ? null : it }", // don't publish hidden files, by default
            mode: "copy"
          ]]
        }
      
        // auto define transcript, if so desired
        if (processArgs.auto.transcript == true) {
          assert params.containsKey("transcriptsDir") || params.containsKey("transcripts_dir") || params.containsKey("publishDir") || params.containsKey("publish_dir") : 
            "Error in module '${processArgs['key']}': if auto.transcript is true, either params.transcripts_dir or params.publish_dir needs to be defined.\n" +
            "  Example: params.transcripts_dir = \"./transcripts/\""
          def transcriptsDir = 
            params.containsKey("transcripts_dir") ? params.transcripts_dir : 
            params.containsKey("transcriptsDir") ? params.transcriptsDir : 
            params.containsKey("publish_dir") ? params.publish_dir + "/_transcripts" :
            params.publishDir + "/_transcripts"
          def timestamp = Nextflow.getSession().getWorkflowMetadata().start.format('yyyy-MM-dd_HH-mm-ss')
          def transcriptsPublishDir = [ 
            path: "$transcriptsDir/$timestamp/\${task.process.replaceAll(':', '-')}/\${id}/", 
            saveAs: "{ it.startsWith('.') ? it.replaceAll('^.', '') : null }", 
            mode: "copy"
          ]
          def publishDirs = processArgs.directives.publishDir ?: []
          processArgs.directives.publishDir = publishDirs + transcriptsPublishDir
        }
      
        for (nam in [ "map", "mapId", "mapData", "mapPassthrough" ]) {
          if (processArgs.containsKey(nam) && processArgs[nam]) {
            assert processArgs[nam] instanceof Closure : "Expected process argument '$nam' to be null or a Closure. Found: class ${processArgs[nam].getClass()}"
          }
        }
      
        // return output
        return processArgs
      }
      
      def processFactory(Map processArgs) {
        def tripQuo = "\"\"\""
      
        // autodetect process key
        def wfKey = processArgs["key"]
        def procKeyPrefix = "${wfKey}_process"
        def meta = ScriptMeta.current()
        def existing = meta.getProcessNames().findAll{it.startsWith(procKeyPrefix)}
        def numbers = existing.collect{it.replace(procKeyPrefix, "0").toInteger()}
        def newNumber = (numbers + [-1]).max() + 1
      
        def procKey = newNumber == 0 ? procKeyPrefix : "$procKeyPrefix$newNumber"
      
        if (newNumber > 0) {
          log.warn "Key for module '${wfKey}' is duplicated.\n",
            "If you run a component multiple times in the same workflow,\n" +
            "it's recommended you set a unique key for every call,\n" +
            "for example: ${wfKey}.run(key: \"foo\")."
        }
      
        // subset directives and convert to list of tuples
        def drctv = processArgs.directives
      
        // TODO: unit test the two commands below
        // convert publish array into tags
        def valueToStr = { val ->
          // ignore closures
          if (val instanceof CharSequence) {
            if (!val.matches('^[{].*[}]$')) {
              '"' + val + '"'
            } else {
              val
            }
          } else if (val instanceof List) {
            "[" + val.collect{valueToStr(it)}.join(", ") + "]"
          } else if (val instanceof Map) {
            "[" + val.collect{k, v -> k + ": " + valueToStr(v)}.join(", ") + "]"
          } else {
            val.inspect()
          }
        }
        // multiple entries allowed: label, publishdir
        def drctvStrs = drctv.collect { key, value ->
          if (key in ["label", "publishDir"]) {
            value.collect{ val ->
              if (val instanceof Map) {
                "\n$key " + val.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
              } else {
                "\n$key " + valueToStr(val)
              }
            }.join()
          } else if (value instanceof Map) {
            "\n$key " + value.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
          } else {
            "\n$key " + valueToStr(value)
          }
        }.join()
      
        def inputPaths = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction == "input" }
          .collect { ', path(viash_par_' + it.plainName + ')' }
          .join()
      
        def outputPaths = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par ->
            // insert dummy into every output (see nextflow-io/nextflow#2678)
            if (!par.multiple) {
              ', path{[".exitcode", args.' + par.plainName + ']}'
            } else {
              ', path{[".exitcode"] + args.' + par.plainName + '}'
            }
          }
          .join()
      
        // TODO: move this functionality somewhere else?
        if (processArgs.auto.transcript) {
          outputPaths = outputPaths + ', path{[".exitcode", ".command*"]}'
        } else {
          outputPaths = outputPaths + ', path{[".exitcode"]}'
        }
      
        // construct inputFileExports
        def inputFileExports = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction.toLowerCase() == "input" }
          .collect { par ->
            viash_par_contents = !par.required && !par.multiple ? "viash_par_${par.plainName}[0]" : "viash_par_${par.plainName}.join(\"${par.multiple_sep}\")"
            "\n\${viash_par_${par.plainName}.empty ? \"\" : \"export VIASH_PAR_${par.plainName.toUpperCase()}=\\\"\" + ${viash_par_contents} + \"\\\"\"}"
          }
      
        // NOTE: if using docker, use /tmp instead of tmpDir!
        def tmpDir = java.nio.file.Paths.get(
          System.getenv('NXF_TEMP') ?: 
          System.getenv('VIASH_TEMP') ?: 
          System.getenv('VIASH_TMPDIR') ?: 
          System.getenv('VIASH_TEMPDIR') ?: 
          System.getenv('VIASH_TMP') ?: 
          System.getenv('TEMP') ?: 
          System.getenv('TMPDIR') ?: 
          System.getenv('TEMPDIR') ?:
          System.getenv('TMP') ?: 
          '/tmp'
        ).toAbsolutePath()
      
        // construct stub
        def stub = thisConfig.functionality.allArguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par -> 
            'touch "${viash_par_' + par.plainName + '.join(\'" "\')}"'
          }
          .join("\n")
      
        // escape script
        def escapedScript = thisScript.replace('\\', '\\\\').replace('$', '\\$').replace('"""', '\\"\\"\\"')
      
        // generate process string
        def procStr = 
        """nextflow.enable.dsl=2
        |
        |process $procKey {$drctvStrs
        |input:
        |  tuple val(id)$inputPaths, val(args), val(passthrough), path(resourcesDir)
        |output:
        |  tuple val("\$id"), val(passthrough)$outputPaths, optional: true
        |stub:
        |$tripQuo
        |$stub
        |$tripQuo
        |script:
        |def escapeText = { s -> s.toString().replaceAll('([`"])', '\\\\\\\\\$1') }
        |def parInject = args
        |  .findAll{key, value -> value != null}
        |  .collect{key, value -> "export VIASH_PAR_\${key.toUpperCase()}=\\\"\${escapeText(value)}\\\""}
        |  .join("\\n")
        |$tripQuo
        |# meta exports
        |export VIASH_META_RESOURCES_DIR="\${resourcesDir.toRealPath().toAbsolutePath()}"
        |export VIASH_META_TEMP_DIR="${['docker', 'podman', 'charliecloud'].any{ it == workflow.containerEngine } ? '/tmp' : tmpDir}"
        |export VIASH_META_FUNCTIONALITY_NAME="${thisConfig.functionality.name}"
        |export VIASH_META_EXECUTABLE="\\\$VIASH_META_RESOURCES_DIR/\\\$VIASH_META_FUNCTIONALITY_NAME"
        |
        |# meta synonyms
        |export VIASH_RESOURCES_DIR="\\\$VIASH_META_RESOURCES_DIR"
        |export VIASH_TEMP="\\\$VIASH_META_TEMP_DIR"
        |export TEMP_DIR="\\\$VIASH_META_TEMP_DIR"
        |
        |# argument exports${inputFileExports.join()}
        |\$parInject
        |
        |# process script
        |${escapedScript}
        |$tripQuo
        |}
        |""".stripMargin()
      
        // TODO: print on debug
        // if (processArgs.debug == true) {
        //   println("######################\n$procStr\n######################")
        // }
      
        // create runtime process
        def ownerParams = new ScriptBinding.ParamsMap()
        def binding = new ScriptBinding().setParams(ownerParams)
        def module = new IncludeDef.Module(name: procKey)
        def moduleScript = new ScriptParser(session)
          .setModule(true)
          .setBinding(binding)
          .runScript(procStr)
          .getScript()
      
        // register module in meta
        meta.addModule(moduleScript, module.name, module.alias)
      
        // retrieve and return process from meta
        return meta.getProcess(procKey)
      }
      
      def debug(processArgs, debugKey) {
        if (processArgs.debug) {
          view { "process '${processArgs.key}' $debugKey tuple: $it"  }
        } else {
          map { it }
        }
      }
      
      def workflowFactory(Map args) {
        def processArgs = processProcessArgs(args)
        def key = processArgs["key"]
        def meta = ScriptMeta.current()
      
        def workflowKey = key
      
        // write process to temporary nf file and parse it in memory
        def processObj = processFactory(processArgs)
        
        workflow workflowInstance {
          take:
          input_
      
          main:
          output_ = input_
            | debug(processArgs, "input")
            | map { tuple ->
              tuple = tuple.clone()
              
              if (processArgs.map) {
                tuple = processArgs.map(tuple)
              }
              if (processArgs.mapId) {
                tuple[0] = processArgs.mapId(tuple[0])
              }
              if (processArgs.mapData) {
                tuple[1] = processArgs.mapData(tuple[1])
              }
              if (processArgs.mapPassthrough) {
                tuple = tuple.take(2) + processArgs.mapPassthrough(tuple.drop(2))
              }
      
              // check tuple
              assert tuple instanceof List : 
                "Error in module '${key}': element in channel should be a tuple [id, data, ...otherargs...]\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: List. Found: tuple.getClass() is ${tuple.getClass()}"
              assert tuple.size() >= 2 : 
                "Error in module '${key}': expected length of tuple in input channel to be two or greater.\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: tuple.size() == ${tuple.size()}"
              
              // check id field
              assert tuple[0] instanceof CharSequence : 
                "Error in module '${key}': first element of tuple in channel should be a String\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: ${tuple[0]}"
              
              // match file to input file
              if (processArgs.auto.simplifyInput && (tuple[1] instanceof Path || tuple[1] instanceof List)) {
                def inputFiles = thisConfig.functionality.allArguments
                  .findAll { it.type == "file" && it.direction == "input" }
                
                assert inputFiles.size() == 1 : 
                    "Error in module '${key}' id '${tuple[0]}'.\n" +
                    "  Anonymous file inputs are only allowed when the process has exactly one file input.\n" +
                    "  Expected: inputFiles.size() == 1. Found: inputFiles.size() is ${inputFiles.size()}"
      
                tuple[1] = [[ inputFiles[0].plainName, tuple[1] ]].collectEntries()
              }
      
              // check data field
              assert tuple[1] instanceof Map : 
                "Error in module '${key}' id '${tuple[0]}': second element of tuple in channel should be a Map\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
              // rename keys of data field in tuple
              if (processArgs.renameKeys) {
                assert processArgs.renameKeys instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class: Map. Found: renameKeys.getClass() is ${processArgs.renameKeys.getClass()}"
                assert tuple[1] instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
                // TODO: allow renameKeys to be a function?
                processArgs.renameKeys.each { newKey, oldKey ->
                  assert newKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of newKey: String. Found: newKey.getClass() is ${newKey.getClass()}"
                  assert oldKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of oldKey: String. Found: oldKey.getClass() is ${oldKey.getClass()}"
                  assert tuple[1].containsKey(oldKey) : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Key '$oldKey' is missing in the data map. tuple[1].keySet() is '${tuple[1].keySet()}'"
                  tuple[1].put(newKey, tuple[1][oldKey])
                }
                tuple[1].keySet().removeAll(processArgs.renameKeys.collect{ newKey, oldKey -> oldKey })
              }
              tuple
            }
            | debug(processArgs, "processed")
            | map { tuple ->
              def id = tuple[0]
              def data = tuple[1]
              def passthrough = tuple.drop(2)
      
              // fetch default params from functionality
              def defaultArgs = thisConfig.functionality.allArguments
                .findAll { it.containsKey("default") }
                .collectEntries { [ it.plainName, it.default ] }
      
              // fetch overrides in params
              def paramArgs = thisConfig.functionality.allArguments
                .findAll { par ->
                  def argKey = key + "__" + par.plainName
                  params.containsKey(argKey) && params[argKey] != "viash_no_value"
                }
                .collectEntries { [ it.plainName, params[key + "__" + it.plainName] ] }
              
              // fetch overrides in data
              def dataArgs = thisConfig.functionality.allArguments
                .findAll { data.containsKey(it.plainName) }
                .collectEntries { [ it.plainName, data[it.plainName] ] }
              
              // combine params
              def combinedArgs = defaultArgs + paramArgs + processArgs.args + dataArgs
      
              // remove arguments with explicit null values
              combinedArgs.removeAll{it.value == null}
      
              // check whether required arguments exist
              thisConfig.functionality.allArguments
                .forEach { par ->
                  if (par.required) {
                    assert combinedArgs.containsKey(par.plainName): "Argument ${par.plainName} is required but does not have a value"
                  }
                }
      
              // TODO: check whether parameters have the right type
      
              // process input files separately
              def inputPaths = thisConfig.functionality.allArguments
                .findAll { it.type == "file" && it.direction == "input" }
                .collect { par ->
                  def val = combinedArgs.containsKey(par.plainName) ? combinedArgs[par.plainName] : []
                  def inputFiles = []
                  if (val == null) {
                    inputFiles = []
                  } else if (val instanceof List) {
                    inputFiles = val
                  } else if (val instanceof Path) {
                    inputFiles = [ val ]
                  } else {
                    inputFiles = []
                  }
                  // throw error when an input file doesn't exist
                  inputFiles.each{ file -> 
                    assert file.exists() :
                      "Error in module '${key}' id '${id}' argument '${par.plainName}'.\n" +
                      "  Required input file does not exist.\n" +
                      "  Path: '$file'.\n" +
                      "  Expected input file to exist"
                  }
                  inputFiles 
                } 
      
              // remove input files
              def argsExclInputFiles = thisConfig.functionality.allArguments
                .findAll { it.type != "file" || it.direction != "input" }
                .collectEntries { par ->
                  def parName = par.plainName
                  def val = combinedArgs[parName]
                  if (par.multiple && val instanceof Collection) {
                    val = val.join(par.multiple_sep)
                  }
                  if (par.direction == "output" && par.type == "file") {
                    val = val.replaceAll('\\$id', id).replaceAll('\\$key', key)
                  }
                  [parName, val]
                }
      
              [ id ] + inputPaths + [ argsExclInputFiles, passthrough, resourcesDir ]
            }
            | processObj
            | map { output ->
              def outputFiles = thisConfig.functionality.allArguments
                .findAll { it.type == "file" && it.direction == "output" }
                .indexed()
                .collectEntries{ index, par ->
                  out = output[index + 2]
                  // strip dummy '.exitcode' file from output (see nextflow-io/nextflow#2678)
                  if (!out instanceof List || out.size() <= 1) {
                    if (par.multiple) {
                      out = []
                    } else {
                      assert !par.required :
                          "Error in module '${key}' id '${output[0]}' argument '${par.plainName}'.\n" +
                          "  Required output file is missing"
                      out = null
                    }
                  } else if (out.size() == 2 && !par.multiple) {
                    out = out[1]
                  } else {
                    out = out.drop(1)
                  }
                  [ par.plainName, out ]
                }
              
              // drop null outputs
              outputFiles.removeAll{it.value == null}
      
              if (processArgs.auto.simplifyOutput && outputFiles.size() == 1) {
                outputFiles = outputFiles.values()[0]
              }
      
              def out = [ output[0], outputFiles ]
      
              // passthrough additional items
              if (output[1]) {
                out.addAll(output[1])
              }
      
              out
            }
            | debug(processArgs, "output")
      
          emit:
          output_
        }
      
        def wf = workflowInstance.cloneWithName(workflowKey)
      
        // add factory function
        wf.metaClass.run = { runArgs ->
          workflowFactory(runArgs)
        }
      
        return wf
      }
      
      // initialise default workflow
      myWfInstance = workflowFactory([:])
      
      // add workflow to environment
      ScriptMeta.current().addDefinition(myWfInstance)
      
      // anonymous workflow for running this module as a standalone
      workflow {
        def mergedConfig = thisConfig
      
        // add id argument if it's not already in the config
        if (mergedConfig.functionality.arguments.every{it.plainName != "id"}) {
          def idArg = [
            'name': '--id',
            'required': false,
            'type': 'string',
            'description': 'A unique id for every entry.',
            'default': 'run',
            'multiple': false
          ]
          mergedConfig.functionality.arguments.add(0, idArg)
          mergedConfig = processConfig(mergedConfig)
        }
      
        helpMessage(mergedConfig)
      
        viashChannel(params, mergedConfig)
          | view { "input: $it" }
          | myWfInstance.run(
            auto: [ publish: true ]
          )
          | view { "output: $it" }
      }

    dest: "main.nf"
  - type: "file"
    text: |
      manifest {
        name = 'maxquant'
        mainScript = 'main.nf'
        nextflowVersion = '!>=20.12.1-edge'
        version = '0.3.1'
        description = 'Perform a MaxQuant analysis with mostly default parameters.'
        author = 'Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid: 0000-0003-3641-729X}'
      }
      
      process.container = 'nextflow/bash:latest'
      
      // detect tempdir
      tempDir = java.nio.file.Paths.get(
        System.getenv('NXF_TEMP') ?:
          System.getenv('VIASH_TEMP') ?: 
          System.getenv('TEMPDIR') ?: 
          System.getenv('TMPDIR') ?: 
          '/tmp'
      ).toAbsolutePath()
      
      profiles {
        mount_temp {
          docker.temp            = tempDir
          podman.temp            = tempDir
          charliecloud.temp      = tempDir
        }
        docker {
          docker.enabled         = true
          // docker.userEmulation   = true
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        singularity {
          singularity.enabled    = true
          singularity.autoMounts = true
          docker.enabled         = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        podman {
          podman.enabled         = true
          docker.enabled         = false
          singularity.enabled    = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        shifter {
          shifter.enabled        = true
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          charliecloud.enabled   = false
        }
        charliecloud {
          charliecloud.enabled   = true
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
        }
      }

    dest: "nextflow.config"
  - type: "file"
    path: "settings"
  description: "Perform a MaxQuant analysis with mostly default parameters."
  usage: "maxquant --input file1.raw --input file2.raw --reference ref.fasta --output\
    \ out/"
  test_resources: []
  info: {}
  dummy_arguments: []
  set_wd_to_resources_dir: false
  enabled: true
platform:
  type: "nextflow"
  id: "nextflow"
  variant: "vdsl3"
  directives:
    accelerator: {}
    conda: []
    containerOptions: []
    label:
    - "highmem"
    - "highcpu"
    module: []
    pod: []
    publishDir: []
    queue: []
  auto:
    simplifyInput: true
    simplifyOutput: true
    transcript: false
    publish: false
  debug: false
  container: "docker"
platforms: []
info:
  config: "src/maxquant/maxquant/config.vsh.yaml"
  platform: "nextflow"
  output: "target/nextflow/maxquant/maxquant"
  viash_version: "0.5.15"
  git_commit: "b0bacbf6eebc79710dba2bdd698ea41cf1565ec9"
  git_remote: "https://github.com/czbiohub/mspipelines"
