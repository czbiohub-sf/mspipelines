functionality:
  name: "maxquant"
  namespace: "maxquant"
  version: "main_build"
  authors:
  - name: "Robrecht Cannoodt"
    email: "rcannood@gmail.com"
    roles:
    - "maintainer"
    props:
      github: "rcannood"
      orcid: "0000-0003-3641-729X"
  inputs:
  - type: "file"
    name: "--input"
    alternatives: []
    description: "One or more Thermo Raw files."
    example:
    - "input.raw"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "file"
    name: "--reference"
    alternatives: []
    description: "A reference file in fasta format."
    example:
    - "reference.fasta"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: true
    multiple_sep: ":"
  outputs:
  - type: "file"
    name: "--output"
    alternatives: []
    description: "An output directory to store the 'mqpar.xml' and 'combined' outputs."
    example:
    - "output_dir"
    default: []
    must_exist: false
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ":"
  arguments:
  - type: "string"
    name: "--ref_taxonomy_id"
    alternatives: []
    description: "Taxonomy ID. Length must match --reference parameter. \nCommon taxonomy\
      \ IDs are Homo Sapiens: 9606, Mus Musculus: 10090.\n"
    example:
    - "9606"
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "boolean"
    name: "--match_between_runs"
    alternatives: []
    description: "Identifications are transferred to non-sequenced or non-identified\
      \ MS features in other LC-MS runs."
    example: []
    default:
    - false
    required: false
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--write_tables"
    alternatives: []
    description: "Which tables to write out."
    example: []
    default:
    - "msScans"
    - "msmsScans"
    - "ms3Scans"
    - "allPeptides"
    - "mzRange"
    - "mzTab"
    - "DIA fragments"
    - "DIA fragments quant"
    - "pasefMsmsScans"
    - "accumulatedMsmsScans"
    required: false
    choices:
    - "msScans"
    - "msmsScans"
    - "ms3Scans"
    - "allPeptides"
    - "mzRange"
    - "mzTab"
    - "DIA fragments"
    - "DIA fragments quant"
    - "pasefMsmsScans"
    - "accumulatedMsmsScans"
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "string"
    name: "--ms_instrument"
    alternatives: []
    description: "Type of intrument the data was generated on. Some internal parameters,\
      \ e.g. in peak detection are set to optimal values based on the machine type.\
      \ Currently Thermo Fisher Orbitrap and FT like instruments are supported, as\
      \ well as ToF instruments like Bruker Impact HD and AB Sciex TripleTOF 5600.\
      \ Usually there is no need for the user to adjust the sub-parameters."
    example: []
    default:
    - "Bruker TIMS"
    required: false
    choices:
    - "Orbitrap"
    - "Bruker Q-TOF"
    - "Sciex Q-TOF"
    - "Agilent Q-TOF"
    - "Bruker TIMS"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--lcms_run_type"
    alternatives: []
    description: "The type of LC-MS run. Select 'Standard' for label free and MS1\
      \ labeled samples. For conventional isobaric labeling samples, select 'Reporter\
      \ ion MS2'. In case the isobaric labeling reporters should be read from MS3\
      \ spectra, please select 'Reporter ion MS3'."
    example: []
    default:
    - "Standard"
    required: true
    choices:
    - "Standard"
    - "Reporter ion MS2"
    - "Reporter ion MS3"
    - "NeuCode"
    - "BoxCar"
    - "TIMS-DDA"
    - "MaxDIA"
    - "TIMS MaxDIA"
    - "BoxCar MaxDIA"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--lfq_mode"
    alternatives: []
    description: "Apply the algorithm for label free protein quantification. The use\
      \ of an experimental design so specify which LC-MS runs or groups of LC-MS runs\
      \ correspond to the different samples is obligatory here. The output of the\
      \ label free algorithm can be found in the proteinGroups table in the columns\
      \ starting with 'LFQ Intensity'."
    example: []
    default:
    - "LFQ"
    required: false
    choices:
    - "None"
    - "LFQ"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--num_cores"
    alternatives: []
    description: ""
    example:
    - 30
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "boolean_true"
    name: "--dryrun"
    alternatives: []
    description: "If true, will only generate the mqpar.xml file and not run MaxQuant."
    direction: "input"
  resources:
  - type: "file"
    text: |
      // maxquant main_build
      // 
      // This wrapper script is auto-generated by viash 0.5.13 and is thus a derivative
      // work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
      // Intuitive.
      // 
      // The component may contain files which fall under a different license. The
      // authors of this component should specify the license in the header of such
      // files, or include a separate license file detailing the licenses of all included
      // files.
      // 
      // Component authors:
      //  * Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid:
      // 0000-0003-3641-729X}
      
      nextflow.enable.dsl=2
      
      // Required imports
      import groovy.json.JsonSlurper
      
      // initialise slurper
      def jsonSlurper = new JsonSlurper()
      
      // DEFINE CUSTOM CODE
      
      // functionality metadata
      thisFunctionality = [
        'name': 'maxquant',
        'arguments': [
          [
            'name': 'input',
            'required': true,
            'type': 'file',
            'direction': 'input',
            'description': 'One or more Thermo Raw files.',
            'example': ['input.raw'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'reference',
            'required': true,
            'type': 'file',
            'direction': 'input',
            'description': 'A reference file in fasta format.',
            'example': ['reference.fasta'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'output',
            'required': true,
            'type': 'file',
            'direction': 'output',
            'description': 'An output directory to store the \'mqpar.xml\' and \'combined\' outputs.',
            'default': '$id.$key.output',
            'example': 'output_dir',
            'multiple': false
          ],
          [
            'name': 'ref_taxonomy_id',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Taxonomy ID. Length must match --reference parameter. \nCommon taxonomy IDs are Homo Sapiens: 9606, Mus Musculus: 10090.\n',
            'example': ['9606'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'match_between_runs',
            'required': false,
            'type': 'boolean',
            'direction': 'input',
            'description': 'Identifications are transferred to non-sequenced or non-identified MS features in other LC-MS runs.',
            'default': false,
            'multiple': false
          ],
          [
            'name': 'write_tables',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Which tables to write out.',
            'default': ['msScans\', \'msmsScans\', \'ms3Scans\', \'allPeptides\', \'mzRange\', \'mzTab\', \'DIA fragments\', \'DIA fragments quant\', \'pasefMsmsScans\', \'accumulatedMsmsScans'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'ms_instrument',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Type of intrument the data was generated on. Some internal parameters, e.g. in peak detection are set to optimal values based on the machine type. Currently Thermo Fisher Orbitrap and FT like instruments are supported, as well as ToF instruments like Bruker Impact HD and AB Sciex TripleTOF 5600. Usually there is no need for the user to adjust the sub-parameters.',
            'default': 'Bruker TIMS',
            'multiple': false
          ],
          [
            'name': 'lcms_run_type',
            'required': true,
            'type': 'string',
            'direction': 'input',
            'description': 'The type of LC-MS run. Select \'Standard\' for label free and MS1 labeled samples. For conventional isobaric labeling samples, select \'Reporter ion MS2\'. In case the isobaric labeling reporters should be read from MS3 spectra, please select \'Reporter ion MS3\'.',
            'default': 'Standard',
            'multiple': false
          ],
          [
            'name': 'lfq_mode',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Apply the algorithm for label free protein quantification. The use of an experimental design so specify which LC-MS runs or groups of LC-MS runs correspond to the different samples is obligatory here. The output of the label free algorithm can be found in the proteinGroups table in the columns starting with \'LFQ Intensity\'.',
            'default': 'LFQ',
            'multiple': false
          ],
          [
            'name': 'num_cores',
            'required': false,
            'type': 'integer',
            'direction': 'input',
            'description': '',
            'example': 30,
            'multiple': false
          ],
          [
            'name': 'dryrun',
            'required': false,
            'type': 'boolean_true',
            'direction': 'input',
            'description': 'If true, will only generate the mqpar.xml file and not run MaxQuant.',
            'default': false,
            'multiple': false
          ]
        ]
      ]
      
      thisHelpMessage = '''maxquant main_build
      
      Perform a MaxQuant analysis with mostly default parameters.
      
      Usage:
      maxquant --input file1.raw --input file2.raw --reference ref.fasta --output out/
      
      Options:
          --input
              type: file, required parameter, multiple values allowed
              example: input.raw
              One or more Thermo Raw files.
      
          --reference
              type: file, required parameter, multiple values allowed
              example: reference.fasta
              A reference file in fasta format.
      
          --output
              type: file, required parameter, output
              example: output_dir
              An output directory to store the 'mqpar.xml' and 'combined' outputs.
      
          --ref_taxonomy_id
              type: string, multiple values allowed
              example: 9606
              Taxonomy ID. Length must match --reference parameter.
              Common taxonomy IDs are Homo Sapiens: 9606, Mus Musculus: 10090.
      
          --match_between_runs
              type: boolean
              default: false
              Identifications are transferred to non-sequenced or non-identified MS
      features in other LC-MS runs.
      
          --write_tables
              type: string, multiple values allowed
              default: msScans:msmsScans:ms3Scans:allPeptides:mzRange:mzTab:DIA
      fragments:DIA fragments quant:pasefMsmsScans:accumulatedMsmsScans
              choices: [ msScans, msmsScans, ms3Scans, allPeptides, mzRange, mzTab,
      DIA fragments, DIA fragments quant, pasefMsmsScans, accumulatedMsmsScans ]
              Which tables to write out.
      
          --ms_instrument
              type: string
              default: Bruker TIMS
              choices: [ Orbitrap, Bruker Q-TOF, Sciex Q-TOF, Agilent Q-TOF, Bruker
      TIMS ]
              Type of intrument the data was generated on. Some internal parameters,
      e.g. in peak detection are set to optimal values based on the machine type.
      Currently Thermo Fisher Orbitrap and FT like instruments are supported, as well
      as ToF instruments like Bruker Impact HD and AB Sciex TripleTOF 5600. Usually
      there is no need for the user to adjust the sub-parameters.
      
          --lcms_run_type
              type: string, required parameter
              default: Standard
              choices: [ Standard, Reporter ion MS2, Reporter ion MS3, NeuCode,
      BoxCar, TIMS-DDA, MaxDIA, TIMS MaxDIA, BoxCar MaxDIA ]
              The type of LC-MS run. Select 'Standard' for label free and MS1 labeled
      samples. For conventional isobaric labeling samples, select 'Reporter ion MS2'.
      In case the isobaric labeling reporters should be read from MS3 spectra, please
      select 'Reporter ion MS3'.
      
          --lfq_mode
              type: string
              default: LFQ
              choices: [ None, LFQ ]
              Apply the algorithm for label free protein quantification. The use of an
      experimental design so specify which LC-MS runs or groups of LC-MS runs
      correspond to the different samples is obligatory here. The output of the label
      free algorithm can be found in the proteinGroups table in the columns starting
      with 'LFQ Intensity'.
      
          --num_cores
              type: integer
              example: 30
      
          --dryrun
              type: boolean_true
              If true, will only generate the mqpar.xml file and not run MaxQuant.'''
      
      thisScript = '''set -e
      tempscript=".viash_script.sh"
      cat > "$tempscript" << VIASHMAIN
      import os
      import re
      import subprocess
      import tempfile
      import shutil
      import pandas as pd
      
      ## VIASH START
      # The following code has been auto-generated by Viash.
      par = {
        'input': $( if [ ! -z ${VIASH_PAR_INPUT+x} ]; then echo "'${VIASH_PAR_INPUT//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'reference': $( if [ ! -z ${VIASH_PAR_REFERENCE+x} ]; then echo "'${VIASH_PAR_REFERENCE//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'output': $( if [ ! -z ${VIASH_PAR_OUTPUT+x} ]; then echo "'${VIASH_PAR_OUTPUT//\\'/\\\\\\'}'"; else echo None; fi ),
        'ref_taxonomy_id': $( if [ ! -z ${VIASH_PAR_REF_TAXONOMY_ID+x} ]; then echo "'${VIASH_PAR_REF_TAXONOMY_ID//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'match_between_runs': $( if [ ! -z ${VIASH_PAR_MATCH_BETWEEN_RUNS+x} ]; then echo "'${VIASH_PAR_MATCH_BETWEEN_RUNS//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi ),
        'write_tables': $( if [ ! -z ${VIASH_PAR_WRITE_TABLES+x} ]; then echo "'${VIASH_PAR_WRITE_TABLES//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'ms_instrument': $( if [ ! -z ${VIASH_PAR_MS_INSTRUMENT+x} ]; then echo "'${VIASH_PAR_MS_INSTRUMENT//\\'/\\\\\\'}'"; else echo None; fi ),
        'lcms_run_type': $( if [ ! -z ${VIASH_PAR_LCMS_RUN_TYPE+x} ]; then echo "'${VIASH_PAR_LCMS_RUN_TYPE//\\'/\\\\\\'}'"; else echo None; fi ),
        'lfq_mode': $( if [ ! -z ${VIASH_PAR_LFQ_MODE+x} ]; then echo "'${VIASH_PAR_LFQ_MODE//\\'/\\\\\\'}'"; else echo None; fi ),
        'num_cores': $( if [ ! -z ${VIASH_PAR_NUM_CORES+x} ]; then echo "int('${VIASH_PAR_NUM_CORES//\\'/\\\\\\'}')"; else echo None; fi ),
        'dryrun': $( if [ ! -z ${VIASH_PAR_DRYRUN+x} ]; then echo "'${VIASH_PAR_DRYRUN//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi )
      }
      meta = {
        'functionality_name': '$VIASH_META_FUNCTIONALITY_NAME',
        'resources_dir': '$VIASH_META_RESOURCES_DIR',
        'temp_dir': '$VIASH_TEMP'
      }
      
      resources_dir = '$VIASH_META_RESOURCES_DIR'
      
      ## VIASH END
      
      # if par_input is a directory, look for raw files
      if len(par["input"]) == 1 and os.path.isdir(par["input"][0]):
         par["input"] = [ os.path.join(dp, f) for dp, dn, filenames in os.walk(par["input"]) for f in filenames if re.match(r'.*\\\\.raw', f) ]
      
      # set taxonomy id to empty string if not specified
      if not par["ref_taxonomy_id"]:
         par["ref_taxonomy_id"] = [ "" for ref in par["reference"] ]
      
      # use absolute paths
      par["input"] = [ os.path.abspath(f) for f in par["input"] ]
      par["reference"] = [ os.path.abspath(f) for f in par["reference"] ]
      par["output"] = os.path.abspath(par["output"])
      
      # auto set experiment names
      experiment_names = [ re.sub(r"_\\\\d+\\$", "", os.path.basename(file)) for file in par["input"] ]
      
      # load default matching settings
      match_between_runs_settings = pd.read_table(
         meta["resources_dir"] + "/settings/match_between_runs.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # load default instrument settings
      ms_instrument_settings = pd.read_table(
         meta["resources_dir"] + "/settings/ms_instrument.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # load default group type settings
      group_type_settings = pd.read_table(
         meta["resources_dir"] + "/settings/group_type.tsv",
         sep="\\\\t",
         index_col="id",
         dtype=str,
         keep_default_na=False,
         na_values=['_']
      )
      
      # check reference metadata
      
      assert len(par["reference"]) == len(par["ref_taxonomy_id"]), "--ref_taxonomy_id must have same length as --reference"
      
      # copy input files to tempdir
      with tempfile.TemporaryDirectory() as temp_dir:
         # prepare to copy input files to tempdir
         old_inputs = par["input"]
         new_inputs = [ os.path.join(temp_dir, os.path.basename(f)) for f in old_inputs ]
         par["input"] = new_inputs
      
         # create output dir if not exists
         if not os.path.exists(par["output"]):
            os.makedirs(par["output"])
      
         # Create params file
         param_file = os.path.join(par["output"], "mqpar.xml")
         endl = "\\\\n"
         param_content = f"""<?xml version="1.0" encoding="utf-8"?>
      <MaxQuantParams xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
         <fastaFiles>"""
      
         # TODO: make taxonomy optional
         for path, taxid in zip(par["reference"], par["ref_taxonomy_id"]):
            param_content += f"""
            <FastaFileInfo>
               <fastaFilePath>{path}</fastaFilePath>
               <identifierParseRule>>.*\\\\|(.*)\\\\|</identifierParseRule>
               <descriptionParseRule>>(.*)</descriptionParseRule>
               <taxonomyParseRule></taxonomyParseRule>
               <variationParseRule></variationParseRule>
               <modificationParseRule></modificationParseRule>
               <taxonomyId>{taxid}</taxonomyId>
            </FastaFileInfo>"""
      
         param_content += f"""
         </fastaFiles>
         <fastaFilesProteogenomics>
         </fastaFilesProteogenomics>
         <fastaFilesFirstSearch>
         </fastaFilesFirstSearch>
         <fixedSearchFolder></fixedSearchFolder>
         <andromedaCacheSize>350000</andromedaCacheSize>
         <advancedRatios>True</advancedRatios>
         <pvalThres>0.005</pvalThres>
         <rtShift>False</rtShift>
         <separateLfq>False</separateLfq>
         <lfqStabilizeLargeRatios>True</lfqStabilizeLargeRatios>
         <lfqRequireMsms>True</lfqRequireMsms>
         <lfqBayesQuant>False</lfqBayesQuant>
         <decoyMode>revert</decoyMode>
         <boxCarMode>all</boxCarMode>
         <includeContaminants>True</includeContaminants>
         <maxPeptideMass>4600</maxPeptideMass>
         <epsilonMutationScore>True</epsilonMutationScore>
         <mutatedPeptidesSeparately>True</mutatedPeptidesSeparately>
         <proteogenomicPeptidesSeparately>True</proteogenomicPeptidesSeparately>
         <minDeltaScoreUnmodifiedPeptides>0</minDeltaScoreUnmodifiedPeptides>
         <minDeltaScoreModifiedPeptides>6</minDeltaScoreModifiedPeptides>
         <minScoreUnmodifiedPeptides>0</minScoreUnmodifiedPeptides>
         <minScoreModifiedPeptides>40</minScoreModifiedPeptides>
         <secondPeptide>True</secondPeptide>
         <matchBetweenRuns>{par["match_between_runs"]}</matchBetweenRuns>
         <matchUnidentifiedFeatures>False</matchUnidentifiedFeatures>
         <matchBetweenRunsFdr>False</matchBetweenRunsFdr>
         <dependentPeptides>False</dependentPeptides>
         <dependentPeptideFdr>0</dependentPeptideFdr>
         <dependentPeptideMassBin>0</dependentPeptideMassBin>
         <dependentPeptidesBetweenRuns>False</dependentPeptidesBetweenRuns>
         <dependentPeptidesWithinExperiment>False</dependentPeptidesWithinExperiment>
         <dependentPeptidesWithinParameterGroup>False</dependentPeptidesWithinParameterGroup>
         <dependentPeptidesRestrictFractions>False</dependentPeptidesRestrictFractions>
         <dependentPeptidesFractionDifference>0</dependentPeptidesFractionDifference>
         <ibaq>False</ibaq>
         <top3>False</top3>
         <independentEnzymes>False</independentEnzymes>
         <useDeltaScore>False</useDeltaScore>
         <splitProteinGroupsByTaxonomy>False</splitProteinGroupsByTaxonomy>
         <taxonomyLevel>Species</taxonomyLevel>
         <avalon>False</avalon>
         <nModColumns>3</nModColumns>
         <ibaqLogFit>False</ibaqLogFit>
         <ibaqChargeNormalization>False</ibaqChargeNormalization>
         <razorProteinFdr>True</razorProteinFdr>
         <deNovoSequencing>False</deNovoSequencing>
         <deNovoVarMods>False</deNovoVarMods>
         <deNovoCompleteSequence>False</deNovoCompleteSequence>
         <deNovoCalibratedMasses>False</deNovoCalibratedMasses>
         <deNovoMaxIterations>0</deNovoMaxIterations>
         <deNovoProteaseReward>0</deNovoProteaseReward>
         <deNovoProteaseRewardTof>0</deNovoProteaseRewardTof>
         <deNovoAgPenalty>0</deNovoAgPenalty>
         <deNovoGgPenalty>0</deNovoGgPenalty>
         <deNovoUseComplementScore>True</deNovoUseComplementScore>
         <deNovoUseProteaseScore>True</deNovoUseProteaseScore>
         <deNovoUseWaterLossScore>True</deNovoUseWaterLossScore>
         <deNovoUseAmmoniaLossScore>True</deNovoUseAmmoniaLossScore>
         <deNovoUseA2Score>True</deNovoUseA2Score>
         <massDifferenceSearch>False</massDifferenceSearch>
         <isotopeCalc>False</isotopeCalc>
         <writePeptidesForSpectrumFile></writePeptidesForSpectrumFile>
         <intensityPredictionsFile>
         </intensityPredictionsFile>
         <minPepLen>7</minPepLen>
         <psmFdrCrosslink>0.01</psmFdrCrosslink>
         <peptideFdr>0.01</peptideFdr>
         <proteinFdr>0.01</proteinFdr>
         <siteFdr>0.01</siteFdr>
         <minPeptideLengthForUnspecificSearch>8</minPeptideLengthForUnspecificSearch>
         <maxPeptideLengthForUnspecificSearch>25</maxPeptideLengthForUnspecificSearch>
         <useNormRatiosForOccupancy>True</useNormRatiosForOccupancy>
         <minPeptides>1</minPeptides>
         <minRazorPeptides>1</minRazorPeptides>
         <minUniquePeptides>0</minUniquePeptides>
         <useCounterparts>False</useCounterparts>
         <advancedSiteIntensities>True</advancedSiteIntensities>
         <customProteinQuantification>False</customProteinQuantification>
         <customProteinQuantificationFile></customProteinQuantificationFile>
         <minRatioCount>2</minRatioCount>
         <restrictProteinQuantification>True</restrictProteinQuantification>
         <restrictMods>
            <string>Oxidation (M)</string>
            <string>Acetyl (Protein N-term)</string>
         </restrictMods>
         <matchingTimeWindow>{match_between_runs_settings.at[par["match_between_runs"], "matchingTimeWindow"]}</matchingTimeWindow>
         <matchingIonMobilityWindow>{match_between_runs_settings.at[par["match_between_runs"], "matchingIonMobilityWindow"]}</matchingIonMobilityWindow>
         <alignmentTimeWindow>{match_between_runs_settings.at[par["match_between_runs"], "alignmentTimeWindow"]}</alignmentTimeWindow>
         <alignmentIonMobilityWindow>{match_between_runs_settings.at[par["match_between_runs"], "alignmentIonMobilityWindow"]}</alignmentIonMobilityWindow>
         <numberOfCandidatesMsms>15</numberOfCandidatesMsms>
         <compositionPrediction>0</compositionPrediction>
         <quantMode>1</quantMode>
         <massDifferenceMods>
         </massDifferenceMods>
         <mainSearchMaxCombinations>200</mainSearchMaxCombinations>
         <writeMsScansTable>{"msScans" in par["write_tables"]}</writeMsScansTable>
         <writeMsmsScansTable>{"msmsScans" in par["write_tables"]}</writeMsmsScansTable>
         <writePasefMsmsScansTable>{"pasefMsmsScans" in par["write_tables"]}</writePasefMsmsScansTable>
         <writeAccumulatedMsmsScansTable>{"accumulatedMsmsScans" in par["write_tables"]}</writeAccumulatedMsmsScansTable>
         <writeMs3ScansTable>{"ms3Scans" in par["write_tables"]}</writeMs3ScansTable>
         <writeAllPeptidesTable>{"allPeptides" in par["write_tables"]}</writeAllPeptidesTable>
         <writeMzRangeTable>{"mzRange" in par["write_tables"]}</writeMzRangeTable>
         <writeDiaFragmentTable>{"DIA fragments" in par["write_tables"]}</writeDiaFragmentTable>
         <writeDiaFragmentQuantTable>{"DIA fragments quant" in par["write_tables"]}</writeDiaFragmentQuantTable>
         <writeMzTab>{"mzTab" in par["write_tables"]}</writeMzTab>
         <disableMd5>False</disableMd5>
         <cacheBinInds>True</cacheBinInds>
         <etdIncludeB>False</etdIncludeB>
         <ms2PrecursorShift>0</ms2PrecursorShift>
         <complementaryIonPpm>20</complementaryIonPpm>
         <variationParseRule></variationParseRule>
         <variationMode>none</variationMode>
         <useSeriesReporters>False</useSeriesReporters>
         <name>session1</name>
         <maxQuantVersion>2.0.3.0</maxQuantVersion>
         <pluginFolder></pluginFolder>
         <numThreads>30</numThreads>
         <emailAddress></emailAddress>
         <smtpHost></smtpHost>
         <emailFromAddress></emailFromAddress>
         <fixedCombinedFolder>{par["output"]}/</fixedCombinedFolder>
         <fullMinMz>-1.79769313486232E+308</fullMinMz>
         <fullMaxMz>1.79769313486232E+308</fullMaxMz>
         <sendEmail>False</sendEmail>
         <ionCountIntensities>False</ionCountIntensities>
         <verboseColumnHeaders>False</verboseColumnHeaders>
         <calcPeakProperties>True</calcPeakProperties>
         <showCentroidMassDifferences>False</showCentroidMassDifferences>
         <showIsotopeMassDifferences>False</showIsotopeMassDifferences>
         <useDotNetCore>True</useDotNetCore>
         <profilePerformance>False</profilePerformance>
         <filePaths>{''.join([ f"{endl}      <string>{file}</string>" for file in par["input"] ])}
         </filePaths>
         <experiments>{''.join([ f"{endl}      <string>{exp}</string>" for exp in experiment_names ])}
         </experiments>
         <fractions>{''.join([ f"{endl}      <short>32767</short>" for file in par["input"] ])}
         </fractions>
         <ptms>{''.join([ f"{endl}      <boolean>False</boolean>" for file in par["input"] ])}
         </ptms>
         <paramGroupIndices>{''.join([ f"{endl}      <int>0</int>" for file in par["input"] ])}
         </paramGroupIndices>
         <referenceChannel>{''.join([ f"{endl}      <string></string>" for file in par["input"] ])}
         </referenceChannel>
         <intensPred>False</intensPred>
         <intensPredModelReTrain>False</intensPredModelReTrain>
         <lfqTopNPeptides>0</lfqTopNPeptides>
         <diaJoinPrecChargesForLfq>False</diaJoinPrecChargesForLfq>
         <diaFragChargesForQuant>1</diaFragChargesForQuant>
         <timsRearrangeSpectra>False</timsRearrangeSpectra>
         <gridSpacing>0.5</gridSpacing>
         <proteinGroupingFile></proteinGroupingFile>
         <parameterGroups>
            <parameterGroup>
               <msInstrument>{ms_instrument_settings.at[par["ms_instrument"], "msInstrument"]}</msInstrument>
               <maxCharge>{ms_instrument_settings.at[par["ms_instrument"], "maxCharge"]}</maxCharge>
               <minPeakLen>{ms_instrument_settings.at[par["ms_instrument"], "minPeakLen"]}</minPeakLen>
               <diaMinPeakLen>{ms_instrument_settings.at[par["ms_instrument"], "diaMinPeakLen"]}</diaMinPeakLen>
               <useMs1Centroids>{ms_instrument_settings.at[par["ms_instrument"], "useMs1Centroids"]}</useMs1Centroids>
               <useMs2Centroids>{ms_instrument_settings.at[par["ms_instrument"], "useMs2Centroids"]}</useMs2Centroids>
               <cutPeaks>True</cutPeaks>
               <gapScans>1</gapScans>
               <minTime>NaN</minTime>
               <maxTime>NaN</maxTime>
               <matchType>MatchFromAndTo</matchType>
               <intensityDetermination>{ms_instrument_settings.at[par["ms_instrument"], "intensityDetermination"]}</intensityDetermination>
               <centroidMatchTol>{ms_instrument_settings.at[par["ms_instrument"], "centroidMatchTol"]}</centroidMatchTol>
               <centroidMatchTolInPpm>True</centroidMatchTolInPpm>
               <centroidHalfWidth>35</centroidHalfWidth>
               <centroidHalfWidthInPpm>True</centroidHalfWidthInPpm>
               <valleyFactor>{ms_instrument_settings.at[par["ms_instrument"], "valleyFactor"]}</valleyFactor>
               <isotopeValleyFactor>1.2</isotopeValleyFactor>
               <advancedPeakSplitting>{ms_instrument_settings.at[par["ms_instrument"], "advancedPeakSplitting"]}</advancedPeakSplitting>
               <intensityThresholdMs1>{ms_instrument_settings.at[par["ms_instrument"], "intensityThresholdMs1"]}</intensityThresholdMs1>
               <intensityThresholdMs2>{ms_instrument_settings.at[par["ms_instrument"], "intensityThresholdMs2"]}</intensityThresholdMs2>
               <labelMods>
                  <string></string>
               </labelMods>
               <lcmsRunType>{par["lcms_run_type"]}</lcmsRunType>
               <reQuantify>False</reQuantify>
               <lfqMode>{"1" if par["lfq_mode"] == "LFQ" else "0"}</lfqMode>
               <lfqNormClusterSize>80</lfqNormClusterSize>
               <lfqMinEdgesPerNode>3</lfqMinEdgesPerNode>
               <lfqAvEdgesPerNode>6</lfqAvEdgesPerNode>
               <lfqMaxFeatures>100000</lfqMaxFeatures>
               <neucodeMaxPpm>{group_type_settings.at[par["lcms_run_type"], "neucodeMaxPpm"]}</neucodeMaxPpm>
               <neucodeResolution>{group_type_settings.at[par["lcms_run_type"], "neucodeResolution"]}</neucodeResolution>
               <neucodeResolutionInMda>{group_type_settings.at[par["lcms_run_type"], "neucodeResolutionInMda"]}</neucodeResolutionInMda>
               <neucodeInSilicoLowRes>{group_type_settings.at[par["lcms_run_type"], "neucodeInSilicoLowRes"]}</neucodeInSilicoLowRes>
               <fastLfq>True</fastLfq>
               <lfqRestrictFeatures>False</lfqRestrictFeatures>
               <lfqMinRatioCount>2</lfqMinRatioCount>
               <maxLabeledAa>{group_type_settings.at[par["lcms_run_type"], "maxLabeledAa"]}</maxLabeledAa>
               <maxNmods>5</maxNmods>
               <maxMissedCleavages>2</maxMissedCleavages>
               <multiplicity>1</multiplicity>
               <enzymeMode>0</enzymeMode>
               <complementaryReporterType>0</complementaryReporterType>
               <reporterNormalization>0</reporterNormalization>
               <neucodeIntensityMode>0</neucodeIntensityMode>
               <fixedModifications>
                  <string>Carbamidomethyl (C)</string>
               </fixedModifications>
               <enzymes>
                  <string>Trypsin/P</string>
               </enzymes>
               <enzymesFirstSearch>
               </enzymesFirstSearch>
               <enzymeModeFirstSearch>0</enzymeModeFirstSearch>
               <useEnzymeFirstSearch>False</useEnzymeFirstSearch>
               <useVariableModificationsFirstSearch>False</useVariableModificationsFirstSearch>
               <variableModifications>
                  <string>Oxidation (M)</string>
                  <string>Acetyl (Protein N-term)</string>
               </variableModifications>
               <useMultiModification>False</useMultiModification>
               <multiModifications>
               </multiModifications>
               <isobaricLabels>
               </isobaricLabels>
               <neucodeLabels>
               </neucodeLabels>
               <variableModificationsFirstSearch>
               </variableModificationsFirstSearch>
               <hasAdditionalVariableModifications>False</hasAdditionalVariableModifications>
               <additionalVariableModifications>
               </additionalVariableModifications>
               <additionalVariableModificationProteins>
               </additionalVariableModificationProteins>
               <doMassFiltering>True</doMassFiltering>
               <firstSearchTol>20</firstSearchTol>
               <mainSearchTol>{ms_instrument_settings.at[par["ms_instrument"], "mainSearchTol"]}</mainSearchTol>
               <searchTolInPpm>True</searchTolInPpm>
               <isotopeMatchTol>{ms_instrument_settings.at[par["ms_instrument"], "isotopeMatchTol"]}</isotopeMatchTol>
               <isotopeMatchTolInPpm>{ms_instrument_settings.at[par["ms_instrument"], "isotopeMatchTolInPpm"]}</isotopeMatchTolInPpm>
               <isotopeTimeCorrelation>0.6</isotopeTimeCorrelation>
               <theorIsotopeCorrelation>0.6</theorIsotopeCorrelation>
               <checkMassDeficit>{ms_instrument_settings.at[par["ms_instrument"], "checkMassDeficit"]}</checkMassDeficit>
               <recalibrationInPpm>True</recalibrationInPpm>
               <intensityDependentCalibration>{ms_instrument_settings.at[par["ms_instrument"], "intensityDependentCalibration"]}</intensityDependentCalibration>
               <minScoreForCalibration>{ms_instrument_settings.at[par["ms_instrument"], "minScoreForCalibration"]}</minScoreForCalibration>
               <matchLibraryFile>False</matchLibraryFile>
               <libraryFile></libraryFile>
               <matchLibraryMassTolPpm>0</matchLibraryMassTolPpm>
               <matchLibraryTimeTolMin>0</matchLibraryTimeTolMin>
               <matchLabelTimeTolMin>0</matchLabelTimeTolMin>
               <reporterMassTolerance>NaN</reporterMassTolerance>
               <reporterPif>{group_type_settings.at[par["lcms_run_type"], "reporterPif"]}</reporterPif>
               <filterPif>False</filterPif>
               <reporterFraction>{group_type_settings.at[par["lcms_run_type"], "reporterFraction"]}</reporterFraction>
               <reporterBasePeakRatio>{group_type_settings.at[par["lcms_run_type"], "reporterBasePeakRatio"]}</reporterBasePeakRatio>
               <timsHalfWidth>{group_type_settings.at[par["lcms_run_type"], "timsHalfWidth"]}</timsHalfWidth>
               <timsStep>{group_type_settings.at[par["lcms_run_type"], "timsStep"]}</timsStep>
               <timsResolution>{group_type_settings.at[par["lcms_run_type"], "timsResolution"]}</timsResolution>
               <timsMinMsmsIntensity>{group_type_settings.at[par["lcms_run_type"], "timsMinMsmsIntensity"]}</timsMinMsmsIntensity>
               <timsRemovePrecursor>True</timsRemovePrecursor>
               <timsIsobaricLabels>False</timsIsobaricLabels>
               <timsCollapseMsms>True</timsCollapseMsms>
               <crossLinkingType>0</crossLinkingType>
               <crossLinker></crossLinker>
               <minMatchXl>3</minMatchXl>
               <minPairedPepLenXl>6</minPairedPepLenXl>
               <minScore_Dipeptide>40</minScore_Dipeptide>
               <minScore_Monopeptide>0</minScore_Monopeptide>
               <minScore_PartialCross>10</minScore_PartialCross>
               <crosslinkOnlyIntraProtein>False</crosslinkOnlyIntraProtein>
               <crosslinkIntensityBasedPrecursor>True</crosslinkIntensityBasedPrecursor>
               <isHybridPrecDetermination>False</isHybridPrecDetermination>
               <topXcross>3</topXcross>
               <doesSeparateInterIntraProteinCross>False</doesSeparateInterIntraProteinCross>
               <crosslinkMaxMonoUnsaturated>0</crosslinkMaxMonoUnsaturated>
               <crosslinkMaxMonoSaturated>0</crosslinkMaxMonoSaturated>
               <crosslinkMaxDiUnsaturated>0</crosslinkMaxDiUnsaturated>
               <crosslinkMaxDiSaturated>0</crosslinkMaxDiSaturated>
               <crosslinkModifications>
               </crosslinkModifications>
               <crosslinkFastaFiles>
               </crosslinkFastaFiles>
               <crosslinkSites>
               </crosslinkSites>
               <crosslinkNetworkFiles>
               </crosslinkNetworkFiles>
               <crosslinkMode></crosslinkMode>
               <peakRefinement>False</peakRefinement>
               <isobaricSumOverWindow>True</isobaricSumOverWindow>
               <isobaricWeightExponent>0.75</isobaricWeightExponent>
               <collapseMsmsOnIsotopePatterns>False</collapseMsmsOnIsotopePatterns>
               <diaLibraryType>0</diaLibraryType>
               <diaLibraryPaths>
               </diaLibraryPaths>
               <diaPeptidePaths>
               </diaPeptidePaths>
               <diaEvidencePaths>
               </diaEvidencePaths>
               <diaMsmsPaths>
               </diaMsmsPaths>
               <diaInitialPrecMassTolPpm>20</diaInitialPrecMassTolPpm>
               <diaInitialFragMassTolPpm>20</diaInitialFragMassTolPpm>
               <diaCorrThresholdFeatureClustering>0.85</diaCorrThresholdFeatureClustering>
               <diaPrecTolPpmFeatureClustering>2</diaPrecTolPpmFeatureClustering>
               <diaFragTolPpmFeatureClustering>2</diaFragTolPpmFeatureClustering>
               <diaScoreN>7</diaScoreN>
               <diaMinScore>1.99</diaMinScore>
               <diaXgBoostBaseScore>0.4</diaXgBoostBaseScore>
               <diaXgBoostSubSample>0.9</diaXgBoostSubSample>
               <centroidPosition>0</centroidPosition>
               <diaQuantMethod>7</diaQuantMethod>
               <diaFeatureQuantMethod>2</diaFeatureQuantMethod>
               <lfqNormType>1</lfqNormType>
               <diaTopNForQuant>{ms_instrument_settings.at[par["ms_instrument"], "diaTopNForQuant"]}</diaTopNForQuant>
               <diaMinMsmsIntensityForQuant>0</diaMinMsmsIntensityForQuant>
               <diaTopMsmsIntensityQuantileForQuant>0.85</diaTopMsmsIntensityQuantileForQuant>
               <diaPrecursorFilterType>0</diaPrecursorFilterType>
               <diaMinFragmentOverlapScore>1</diaMinFragmentOverlapScore>
               <diaMinPrecursorScore>0.5</diaMinPrecursorScore>
               <diaMinProfileCorrelation>0</diaMinProfileCorrelation>
               <diaXgBoostMinChildWeight>9</diaXgBoostMinChildWeight>
               <diaXgBoostMaximumTreeDepth>12</diaXgBoostMaximumTreeDepth>
               <diaXgBoostEstimators>580</diaXgBoostEstimators>
               <diaXgBoostGamma>0.9</diaXgBoostGamma>
               <diaXgBoostMaxDeltaStep>3</diaXgBoostMaxDeltaStep>
               <diaGlobalMl>True</diaGlobalMl>
               <diaAdaptiveMassAccuracy>False</diaAdaptiveMassAccuracy>
               <diaMassWindowFactor>3.3</diaMassWindowFactor>
               <diaRtPrediction>False</diaRtPrediction>
               <diaRtPredictionSecondRound>False</diaRtPredictionSecondRound>
               <diaNoMl>False</diaNoMl>
               <diaPermuteRt>False</diaPermuteRt>
               <diaPermuteCcs>False</diaPermuteCcs>
               <diaBackgroundSubtraction>{ms_instrument_settings.at[par["ms_instrument"], "diaBackgroundSubtraction"]}</diaBackgroundSubtraction>
               <diaBackgroundSubtractionQuantile>{ms_instrument_settings.at[par["ms_instrument"], "diaBackgroundSubtractionQuantile"]}</diaBackgroundSubtractionQuantile>
               <diaBackgroundSubtractionFactor>4</diaBackgroundSubtractionFactor>
               <diaLfqWeightedMedian>{ms_instrument_settings.at[par["ms_instrument"], "diaLfqWeightedMedian"]}</diaLfqWeightedMedian>
               <diaTransferQvalue>0.3</diaTransferQvalue>
               <diaOnlyIsosForRecal>True</diaOnlyIsosForRecal>
               <diaMinPeaksForRecal>5</diaMinPeaksForRecal>
               <diaUseFragIntensForMl>False</diaUseFragIntensForMl>
               <diaUseFragMassesForMl>False</diaUseFragMassesForMl>
               <diaMaxTrainInstances>1000000</diaMaxTrainInstances>
            </parameterGroup>
         </parameterGroups>
         <msmsParamsArray>
            <msmsParams>
               <Name>FTMS</Name>
               <MatchTolerance>20</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>7</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>True</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>12</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>ITMS</Name>
               <MatchTolerance>0.5</MatchTolerance>
               <MatchToleranceInPpm>False</MatchToleranceInPpm>
               <DeisotopeTolerance>0.15</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>False</DeisotopeToleranceInPpm>
               <DeNovoTolerance>0.5</DeNovoTolerance>
               <DeNovoToleranceInPpm>False</DeNovoToleranceInPpm>
               <Deisotope>False</Deisotope>
               <Topx>8</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>TOF</Name>
               <MatchTolerance>40</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>0.01</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>False</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>10</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
            <msmsParams>
               <Name>Unknown</Name>
               <MatchTolerance>20</MatchTolerance>
               <MatchToleranceInPpm>True</MatchToleranceInPpm>
               <DeisotopeTolerance>7</DeisotopeTolerance>
               <DeisotopeToleranceInPpm>True</DeisotopeToleranceInPpm>
               <DeNovoTolerance>25</DeNovoTolerance>
               <DeNovoToleranceInPpm>True</DeNovoToleranceInPpm>
               <Deisotope>True</Deisotope>
               <Topx>12</Topx>
               <TopxInterval>100</TopxInterval>
               <HigherCharges>True</HigherCharges>
               <IncludeWater>True</IncludeWater>
               <IncludeAmmonia>True</IncludeAmmonia>
               <DependentLosses>True</DependentLosses>
               <Recalibration>False</Recalibration>
            </msmsParams>
         </msmsParamsArray>
         <fragmentationParamsArray>
            <fragmentationParams>
               <Name>CID</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>HCD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>PQD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETHCD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>ETCID</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>UVPD</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
            <fragmentationParams>
               <Name>Unknown</Name>
               <Connected>False</Connected>
               <ConnectedScore0>1</ConnectedScore0>
               <ConnectedScore1>1</ConnectedScore1>
               <ConnectedScore2>1</ConnectedScore2>
               <InternalFragments>False</InternalFragments>
               <InternalFragmentWeight>1</InternalFragmentWeight>
               <InternalFragmentAas>KRH</InternalFragmentAas>
            </fragmentationParams>
         </fragmentationParamsArray>
      </MaxQuantParams>
      """
      
         with open(param_file, "w") as f:
            f.write(param_content)
      
         if not par["dryrun"]:
            # copy input files
            for old, new in zip(old_inputs, new_inputs):
               if (os.path.isdir(old)):
                  shutil.copytree(old, new)
               else:
                  shutil.copyfile(old, new)
               
            
            # run maxquant
            p = subprocess.Popen(
               ["dotnet", "/maxquant/bin/MaxQuantCmd.exe", os.path.basename(param_file)], 
               # ["maxquant", os.path.basename(param_file)], 
               cwd=os.path.dirname(param_file)
            )
            p.wait()
      
            if p.returncode != 0:
               raise Exception(f"MaxQuant finished with exit code {p.returncode}") 
      VIASHMAIN
      python "$tempscript"
      '''
      
      thisDefaultProcessArgs = [
        // key to be used to trace the process and determine output names
        key: thisFunctionality.name,
        // fixed arguments to be passed to script
        args: [:],
        // default directives
        directives: jsonSlurper.parseText("""{
        "container" : {
          "registry" : "ghcr.io",
          "image" : "czbiohub/mspipelines/maxquant_maxquant",
          "tag" : "main_build"
        },
        "label" : [
          "highmem",
          "highcpu"
        ]
      }"""),
        // auto settings
        auto: jsonSlurper.parseText("""{
        "simplifyInput" : true,
        "simplifyOutput" : true,
        "transcript" : false,
        "publish" : false
      }"""),
        // apply a map over the incoming tuple
        // example: { tup -> [ tup[0], [input: tup[1].output], tup[2] ] }
        map: null,
        // apply a map over the ID element of a tuple (i.e. the first element)
        // example: { id -> id + "_foo" }
        mapId: null,
        // apply a map over the data element of a tuple (i.e. the second element)
        // example: { data -> [ input: data.output ] }
        mapData: null,
        // apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements)
        // example: { pt -> pt.drop(1) }
        mapPassthrough: null,
        // rename keys in the data field of the tuple (i.e. the second element)
        // example: [ "new_key": "old_key" ]
        renameKeys: null,
        // whether or not to print debug messages
        debug: false
      ]
      
      // END CUSTOM CODE
      
      import nextflow.Nextflow
      import nextflow.script.IncludeDef
      import nextflow.script.ScriptBinding
      import nextflow.script.ScriptMeta
      import nextflow.script.ScriptParser
      
      // retrieve resourcesDir here to make sure the correct path is found
      resourcesDir = ScriptMeta.current().getScriptPath().getParent()
      
      def assertMapKeys(map, expectedKeys, requiredKeys, mapName) {
        assert map instanceof Map : "Expected argument '$mapName' to be a Map. Found: class ${map.getClass()}"
        map.forEach { key, val -> 
          assert key in expectedKeys : "Unexpected key '$key' in ${mapName ? mapName + " " : ""}map"
        }
        requiredKeys.forEach { requiredKey -> 
          assert map.containsKey(requiredKey) : "Missing required key '$key' in ${mapName ? mapName + " " : ""}map"
        }
      }
      
      // TODO: unit test processDirectives
      def processDirectives(Map drctv) {
        // remove null values
        drctv = drctv.findAll{k, v -> v != null}
      
        /* DIRECTIVE accelerator
          accepted examples:
          - [ limit: 4, type: "nvidia-tesla-k80" ]
        */
        if (drctv.containsKey("accelerator")) {
          assertMapKeys(drctv["accelerator"], ["type", "limit", "request", "runtime"], [], "accelerator")
        }
      
        /* DIRECTIVE afterScript
          accepted examples:
          - "source /cluster/bin/cleanup"
        */
        if (drctv.containsKey("afterScript")) {
          assert drctv["afterScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE beforeScript
          accepted examples:
          - "source /cluster/bin/setup"
        */
        if (drctv.containsKey("beforeScript")) {
          assert drctv["beforeScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE cache
          accepted examples:
          - true
          - false
          - "deep"
          - "lenient"
        */
        if (drctv.containsKey("cache")) {
          assert drctv["cache"] instanceof CharSequence || drctv["cache"] instanceof Boolean
          if (drctv["cache"] instanceof CharSequence) {
            assert drctv["cache"] in ["deep", "lenient"] : "Unexpected value for cache"
          }
        }
      
        /* DIRECTIVE conda
          accepted examples:
          - "bwa=0.7.15"
          - "bwa=0.7.15 fastqc=0.11.5"
          - ["bwa=0.7.15", "fastqc=0.11.5"]
        */
        if (drctv.containsKey("conda")) {
          if (drctv["conda"] instanceof List) {
            drctv["conda"] = drctv["conda"].join(" ")
          }
          assert drctv["conda"] instanceof CharSequence
        }
      
        /* DIRECTIVE container
          accepted examples:
          - "foo/bar:tag"
          - [ registry: "reg", image: "im", tag: "ta" ]
            is transformed to "reg/im:ta"
          - [ image: "im" ] 
            is transformed to "im:latest"
        */
        if (drctv.containsKey("container")) {
          assert drctv["container"] instanceof Map || drctv["container"] instanceof CharSequence
          if (drctv["container"] instanceof Map) {
            def m = drctv["container"]
            assertMapKeys(m, [ "registry", "image", "tag" ], ["image"], "container")
            def part1 = 
              params.containsKey("override_container_registry") ? params["override_container_registry"] + "/" : 
              m.registry ? m.registry + "/" : 
              ""
            def part2 = m.image
            def part3 = m.tag ? ":" + m.tag : ":latest"
            drctv["container"] = part1 + part2 + part3
          }
        }
      
        /* DIRECTIVE containerOptions
          accepted examples:
          - "--foo bar"
          - ["--foo bar", "-f b"]
        */
        if (drctv.containsKey("containerOptions")) {
          if (drctv["containerOptions"] instanceof List) {
            drctv["containerOptions"] = drctv["containerOptions"].join(" ")
          }
          assert drctv["containerOptions"] instanceof CharSequence
        }
      
        /* DIRECTIVE cpus
          accepted examples:
          - 1
          - 10
        */
        if (drctv.containsKey("cpus")) {
          assert drctv["cpus"] instanceof Integer
        }
      
        /* DIRECTIVE disk
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("disk")) {
          assert drctv["disk"] instanceof CharSequence
          // assert drctv["disk"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE echo
          accepted examples:
          - true
          - false
        */
        if (drctv.containsKey("echo")) {
          assert drctv["echo"] instanceof Boolean
        }
      
        /* DIRECTIVE errorStrategy
          accepted examples:
          - "terminate"
          - "finish"
        */
        if (drctv.containsKey("errorStrategy")) {
          assert drctv["errorStrategy"] instanceof CharSequence
          assert drctv["errorStrategy"] in ["terminate", "finish", "ignore", "retry"] : "Unexpected value for errorStrategy"
        }
      
        /* DIRECTIVE executor
          accepted examples:
          - "local"
          - "sge"
        */
        if (drctv.containsKey("executor")) {
          assert drctv["executor"] instanceof CharSequence
          assert drctv["executor"] in ["local", "sge", "uge", "lsf", "slurm", "pbs", "pbspro", "moab", "condor", "nqsii", "ignite", "k8s", "awsbatch", "google-pipelines"] : "Unexpected value for executor"
        }
      
        /* DIRECTIVE machineType
          accepted examples:
          - "n1-highmem-8"
        */
        if (drctv.containsKey("machineType")) {
          assert drctv["machineType"] instanceof CharSequence
        }
      
        /* DIRECTIVE maxErrors
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxErrors")) {
          assert drctv["maxErrors"] instanceof Integer
        }
      
        /* DIRECTIVE maxForks
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxForks")) {
          assert drctv["maxForks"] instanceof Integer
        }
      
        /* DIRECTIVE maxRetries
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxRetries")) {
          assert drctv["maxRetries"] instanceof Integer
        }
      
        /* DIRECTIVE memory
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("memory")) {
          assert drctv["memory"] instanceof CharSequence
          // assert drctv["memory"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE module
          accepted examples:
          - "ncbi-blast/2.2.27"
          - "ncbi-blast/2.2.27:t_coffee/10.0"
          - ["ncbi-blast/2.2.27", "t_coffee/10.0"]
        */
        if (drctv.containsKey("module")) {
          if (drctv["module"] instanceof List) {
            drctv["module"] = drctv["module"].join(":")
          }
          assert drctv["module"] instanceof CharSequence
        }
      
        /* DIRECTIVE penv
          accepted examples:
          - "smp"
        */
        if (drctv.containsKey("penv")) {
          assert drctv["penv"] instanceof CharSequence
        }
      
        /* DIRECTIVE pod
          accepted examples:
          - [ label: "key", value: "val" ]
          - [ annotation: "key", value: "val" ]
          - [ env: "key", value: "val" ]
          - [ [label: "l", value: "v"], [env: "e", value: "v"]]
        */
        if (drctv.containsKey("pod")) {
          if (drctv["pod"] instanceof Map) {
            drctv["pod"] = [ drctv["pod"] ]
          }
          assert drctv["pod"] instanceof List
          drctv["pod"].forEach { pod ->
            assert pod instanceof Map
            // TODO: should more checks be added?
            // See https://www.nextflow.io/docs/latest/process.html?highlight=directives#pod
            // e.g. does it contain 'label' and 'value', or 'annotation' and 'value', or ...?
          }
        }
      
        /* DIRECTIVE publishDir
          accepted examples:
          - []
          - [ [ path: "foo", enabled: true ], [ path: "bar", enabled: false ] ]
          - "/path/to/dir" 
            is transformed to [[ path: "/path/to/dir" ]]
          - [ path: "/path/to/dir", mode: "cache" ]
            is transformed to [[ path: "/path/to/dir", mode: "cache" ]]
        */
        // TODO: should we also look at params["publishDir"]?
        if (drctv.containsKey("publishDir")) {
          def pblsh = drctv["publishDir"]
          
          // check different options
          assert pblsh instanceof List || pblsh instanceof Map || pblsh instanceof CharSequence
          
          // turn into list if not already so
          // for some reason, 'if (!pblsh instanceof List) pblsh = [ pblsh ]' doesn't work.
          pblsh = pblsh instanceof List ? pblsh : [ pblsh ]
      
          // check elements of publishDir
          pblsh = pblsh.collect{ elem ->
            // turn into map if not already so
            elem = elem instanceof CharSequence ? [ path: elem ] : elem
      
            // check types and keys
            assert elem instanceof Map : "Expected publish argument '$elem' to be a String or a Map. Found: class ${elem.getClass()}"
            assertMapKeys(elem, [ "path", "mode", "overwrite", "pattern", "saveAs", "enabled" ], ["path"], "publishDir")
      
            // check elements in map
            assert elem.containsKey("path")
            assert elem["path"] instanceof CharSequence
            if (elem.containsKey("mode")) {
              assert elem["mode"] instanceof CharSequence
              assert elem["mode"] in [ "symlink", "rellink", "link", "copy", "copyNoFollow", "move" ]
            }
            if (elem.containsKey("overwrite")) {
              assert elem["overwrite"] instanceof Boolean
            }
            if (elem.containsKey("pattern")) {
              assert elem["pattern"] instanceof CharSequence
            }
            if (elem.containsKey("saveAs")) {
              assert elem["saveAs"] instanceof CharSequence //: "saveAs as a Closure is currently not supported. Surround your closure with single quotes to get the desired effect. Example: '\{ foo \}'"
            }
            if (elem.containsKey("enabled")) {
              assert elem["enabled"] instanceof Boolean
            }
      
            // return final result
            elem
          }
          // store final directive
          drctv["publishDir"] = pblsh
        }
      
        /* DIRECTIVE queue
          accepted examples:
          - "long"
          - "short,long"
          - ["short", "long"]
        */
        if (drctv.containsKey("queue")) {
          if (drctv["queue"] instanceof List) {
            drctv["queue"] = drctv["queue"].join(",")
          }
          assert drctv["queue"] instanceof CharSequence
        }
      
        /* DIRECTIVE label
          accepted examples:
          - "big_mem"
          - "big_cpu"
          - ["big_mem", "big_cpu"]
        */
        if (drctv.containsKey("label")) {
          if (drctv["label"] instanceof CharSequence) {
            drctv["label"] = [ drctv["label"] ]
          }
          assert drctv["label"] instanceof List
          drctv["label"].forEach { label ->
            assert label instanceof CharSequence
            // assert label.matches("[a-zA-Z0-9]([a-zA-Z0-9_]*[a-zA-Z0-9])?")
            // ^ does not allow closures
          }
        }
      
        /* DIRECTIVE scratch
          accepted examples:
          - true
          - "/path/to/scratch"
          - '$MY_PATH_TO_SCRATCH'
          - "ram-disk"
        */
        if (drctv.containsKey("scratch")) {
          assert drctv["scratch"] == true || drctv["scratch"] instanceof CharSequence
        }
      
        /* DIRECTIVE storeDir
          accepted examples:
          - "/path/to/storeDir"
        */
        if (drctv.containsKey("storeDir")) {
          assert drctv["storeDir"] instanceof CharSequence
        }
      
        /* DIRECTIVE stageInMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageInMode")) {
          assert drctv["stageInMode"] instanceof CharSequence
          assert drctv["stageInMode"] in ["copy", "link", "symlink", "rellink"]
        }
      
        /* DIRECTIVE stageOutMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageOutMode")) {
          assert drctv["stageOutMode"] instanceof CharSequence
          assert drctv["stageOutMode"] in ["copy", "move", "rsync"]
        }
      
        /* DIRECTIVE tag
          accepted examples:
          - "foo"
          - '$id'
        */
        if (drctv.containsKey("tag")) {
          assert drctv["tag"] instanceof CharSequence
        }
      
        /* DIRECTIVE time
          accepted examples:
          - "1h"
          - "2days"
          - "1day 6hours 3minutes 30seconds"
        */
        if (drctv.containsKey("time")) {
          assert drctv["time"] instanceof CharSequence
          // todo: validation regex?
        }
      
        return drctv
      }
      
      // TODO: unit test processAuto
      def processAuto(Map auto) {
        // remove null values
        auto = auto.findAll{k, v -> v != null}
      
        expectedKeys = ["simplifyInput", "simplifyOutput", "transcript", "publish"]
      
        // check whether expected keys are all booleans (for now)
        for (key in expectedKeys) {
          assert auto.containsKey(key)
          assert auto[key] instanceof Boolean
        }
      
        return auto.subMap(expectedKeys)
      }
      
      def processProcessArgs(Map args) {
        // override defaults with args
        def processArgs = thisDefaultProcessArgs + args
      
        // check whether 'key' exists
        assert processArgs.containsKey("key")
      
        // if 'key' is a closure, apply it to the original key
        if (processArgs["key"] instanceof Closure) {
          processArgs["key"] = processArgs["key"](thisFunctionality.name)
        }
        assert processArgs["key"] instanceof CharSequence
        assert processArgs["key"] ==~ /^[a-zA-Z_][a-zA-Z0-9_]*$/
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("directives")
        assert processArgs["directives"] instanceof Map
        processArgs["directives"] = processDirectives(thisDefaultProcessArgs.directives + processArgs["directives"])
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("auto")
        assert processArgs["auto"] instanceof Map
        processArgs["auto"] = processAuto(thisDefaultProcessArgs.auto + processArgs["auto"])
      
        // auto define publish, if so desired
        if (processArgs.auto.publish == true && (processArgs.directives.publishDir ?: [:]).isEmpty()) {
          assert params.containsKey("publishDir") || params.containsKey("publish_dir") : 
            "Error in module '${processArgs['key']}': if auto.publish is true, params.publish_dir needs to be defined.\n" +
            "  Example: params.transcripts_dir = \"./output/\""
          def publishDir = params.containsKey("publish_dir") ? params.publish_dir : params.publishDir
          
          // TODO: more asserts on publishDir?
          processArgs.directives.publishDir = [[ 
            path: publishDir, 
            saveAs: "{ it.startsWith('.') ? null : it }", // don't publish hidden files, by default
            mode: "copy"
          ]]
        }
      
        // auto define transcript, if so desired
        if (processArgs.auto.transcript == true) {
          assert params.containsKey("transcriptsDir") || params.containsKey("transcripts_dir") || params.containsKey("publishDir") || params.containsKey("publish_dir") : 
            "Error in module '${processArgs['key']}': if auto.transcript is true, either params.transcripts_dir or params.publish_dir needs to be defined.\n" +
            "  Example: params.transcripts_dir = \"./transcripts/\""
          def transcriptsDir = 
            params.containsKey("transcripts_dir") ? params.transcripts_dir : 
            params.containsKey("transcriptsDir") ? params.transcriptsDir : 
            params.containsKey("publish_dir") ? params.publish_dir + "/_transcripts" :
            params.publishDir + "/_transcripts"
          def timestamp = Nextflow.getSession().getWorkflowMetadata().start.format('yyyy-MM-dd_HH-mm-ss')
          def transcriptsPublishDir = [ 
            path: "$transcriptsDir/$timestamp/\${task.process.replaceAll(':', '-')}/\${id}/", 
            saveAs: "{ it.startsWith('.') ? it.replaceAll('^.', '') : null }", 
            mode: "copy"
          ]
          def publishDirs = processArgs.directives.publishDir ?: []
          processArgs.directives.publishDir = publishDirs + transcriptsPublishDir
        }
      
        for (nam in [ "map", "mapId", "mapData", "mapPassthrough" ]) {
          if (processArgs.containsKey(nam) && processArgs[nam]) {
            assert processArgs[nam] instanceof Closure : "Expected process argument '$nam' to be null or a Closure. Found: class ${processArgs[nam].getClass()}"
          }
        }
      
        // return output
        return processArgs
      }
      
      def processFactory(Map processArgs) {
        def tripQuo = "\"\"\""
      
        // autodetect process key
        def wfKey = processArgs["key"]
        def procKeyPrefix = "${wfKey}_process"
        def meta = ScriptMeta.current()
        def existing = meta.getProcessNames().findAll{it.startsWith(procKeyPrefix)}
        def numbers = existing.collect{it.replace(procKeyPrefix, "0").toInteger()}
        def newNumber = (numbers + [-1]).max() + 1
      
        def procKey = newNumber == 0 ? procKeyPrefix : "$procKeyPrefix$newNumber"
      
        if (newNumber > 0) {
          log.warn "Key for module '${wfKey}' is duplicated.\n",
            "If you run a component multiple times in the same workflow,\n" +
            "it's recommended you set a unique key for every call,\n" +
            "for example: ${wfKey}.run(key: \"foo\")."
        }
      
        // subset directives and convert to list of tuples
        def drctv = processArgs.directives
      
        // TODO: unit test the two commands below
        // convert publish array into tags
        def valueToStr = { val ->
          // ignore closures
          if (val instanceof CharSequence) {
            if (!val.matches('^[{].*[}]$')) {
              '"' + val + '"'
            } else {
              val
            }
          } else if (val instanceof List) {
            "[" + val.collect{valueToStr(it)}.join(", ") + "]"
          } else if (val instanceof Map) {
            "[" + val.collect{k, v -> k + ": " + valueToStr(v)}.join(", ") + "]"
          } else {
            val.inspect()
          }
        }
        // multiple entries allowed: label, publishdir
        def drctvStrs = drctv.collect { key, value ->
          if (key in ["label", "publishDir"]) {
            value.collect{ val ->
              if (val instanceof Map) {
                "\n$key " + val.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
              } else {
                "\n$key " + valueToStr(val)
              }
            }.join()
          } else if (value instanceof Map) {
            "\n$key " + value.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
          } else {
            "\n$key " + valueToStr(value)
          }
        }.join()
      
        def inputPaths = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "input" }
          .collect { ', path(viash_par_' + it.name + ')' }
          .join()
      
        def outputPaths = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par ->
            // insert dummy into every output (see nextflow-io/nextflow#2678)
            if (!par.multiple) {
              ', path{[".exitcode", args.' + par.name + ']}'
            } else {
              ', path{[".exitcode"] + args.' + par.name + '}'
            }
          }
          .join()
      
        // TODO: move this functionality somewhere else?
        if (processArgs.auto.transcript) {
          outputPaths = outputPaths + ', path{[".exitcode", ".command*"]}'
        } else {
          outputPaths = outputPaths + ', path{[".exitcode"]}'
        }
      
        // construct inputFileExports
        def inputFileExports = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction.toLowerCase() == "input" }
          .collect { par ->
            viash_par_contents = !par.required && !par.multiple ? "viash_par_${par.name}[0]" : "viash_par_${par.name}.join(\":\")"
            "\n\${viash_par_${par.name}.empty ? \"\" : \"export VIASH_PAR_${par.name.toUpperCase()}=\\\"\" + ${viash_par_contents} + \"\\\"\"}"
          }
        
        def tmpDir = "/tmp" // check if component is docker based
      
        // construct stub
        def stub = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par -> 
            'touch "${viash_par_' + par.name + '.join(\'" "\')}"'
          }
          .join("\n")
      
        // escape script
        def escapedScript = thisScript.replace('\\', '\\\\').replace('$', '\\$').replace('"""', '\\"\\"\\"')
      
        // generate process string
        def procStr = 
        """nextflow.enable.dsl=2
        |
        |process $procKey {$drctvStrs
        |input:
        |  tuple val(id)$inputPaths, val(args), val(passthrough), path(resourcesDir)
        |output:
        |  tuple val("\$id"), val(passthrough)$outputPaths, optional: true
        |stub:
        |$tripQuo
        |$stub
        |$tripQuo
        |script:
        |def escapeText = { s -> s.toString().replaceAll('([`"])', '\\\\\\\\\$1') }
        |def parInject = args
        |  .findAll{key, value -> value != null}
        |  .collect{key, value -> "export VIASH_PAR_\${key.toUpperCase()}=\\\"\${escapeText(value)}\\\""}
        |  .join("\\n")
        |$tripQuo
        |# meta exports
        |export VIASH_META_RESOURCES_DIR="\${resourcesDir.toRealPath().toAbsolutePath()}"
        |export VIASH_META_TEMP_DIR="${tmpDir}"
        |export VIASH_META_FUNCTIONALITY_NAME="${thisFunctionality.name}"
        |
        |# meta synonyms
        |export VIASH_RESOURCES_DIR="\\\$VIASH_META_RESOURCES_DIR"
        |export VIASH_TEMP="\\\$VIASH_META_TEMP_DIR"
        |export TEMP_DIR="\\\$VIASH_META_TEMP_DIR"
        |
        |# argument exports${inputFileExports.join()}
        |\$parInject
        |
        |# process script
        |${escapedScript}
        |$tripQuo
        |}
        |""".stripMargin()
      
        // TODO: print on debug
        // if (processArgs.debug == true) {
        //   println("######################\n$procStr\n######################")
        // }
      
        // create runtime process
        def ownerParams = new ScriptBinding.ParamsMap()
        def binding = new ScriptBinding().setParams(ownerParams)
        def module = new IncludeDef.Module(name: procKey)
        def moduleScript = new ScriptParser(session)
          .setModule(true)
          .setBinding(binding)
          .runScript(procStr)
          .getScript()
      
        // register module in meta
        meta.addModule(moduleScript, module.name, module.alias)
      
        // retrieve and return process from meta
        return meta.getProcess(procKey)
      }
      
      def debug(processArgs, debugKey) {
        if (processArgs.debug) {
          view { "process '${processArgs.key}' $debugKey tuple: $it"  }
        } else {
          map { it }
        }
      }
      
      // wfKeyCounter = -1
      
      def workflowFactory(Map args) {
        def processArgs = processProcessArgs(args)
        def key = processArgs["key"]
        def meta = ScriptMeta.current()
      
        // def workflowKey = wfKeyCounter == -1 ? key : "$key$wfKeyCounter"
        // wfKeyCounter++
        def workflowKey = key
      
        // write process to temporary nf file and parse it in memory
        def processObj = processFactory(processArgs)
        
        workflow workflowInstance {
          take:
          input_
      
          main:
          output_ = input_
            | debug(processArgs, "input")
            | map { tuple ->
              if (processArgs.map) {
                tuple = processArgs.map(tuple)
              }
              if (processArgs.mapId) {
                tuple[0] = processArgs.mapId(tuple[0])
              }
              if (processArgs.mapData) {
                tuple[1] = processArgs.mapData(tuple[1])
              }
              if (processArgs.mapPassthrough) {
                tuple = tuple.take(2) + processArgs.mapPassthrough(tuple.drop(2))
              }
      
              // check tuple
              assert tuple instanceof List : 
                "Error in module '${key}': element in channel should be a tuple [id, data, ...otherargs...]\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: List. Found: tuple.getClass() is ${tuple.getClass()}"
              assert tuple.size() >= 2 : 
                "Error in module '${key}': expected length of tuple in input channel to be two or greater.\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: tuple.size() == ${tuple.size()}"
              
              // check id field
              assert tuple[0] instanceof CharSequence : 
                "Error in module '${key}': first element of tuple in channel should be a String\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: ${tuple[0]}"
              
              // match file to input file
              if (processArgs.auto.simplifyInput && (tuple[1] instanceof Path || tuple[1] instanceof List)) {
                def inputFiles = thisFunctionality.arguments
                  .findAll { it.type == "file" && it.direction == "input" }
                
                assert inputFiles.size() == 1 : 
                    "Error in module '${key}' id '${tuple[0]}'.\n" +
                    "  Anonymous file inputs are only allowed when the process has exactly one file input.\n" +
                    "  Expected: inputFiles.size() == 1. Found: inputFiles.size() is ${inputFiles.size()}"
      
                tuple[1] = [[ inputFiles[0].name, tuple[1] ]].collectEntries()
              }
      
              // check data field
              assert tuple[1] instanceof Map : 
                "Error in module '${key}' id '${tuple[0]}': second element of tuple in channel should be a Map\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
              // rename keys of data field in tuple
              if (processArgs.renameKeys) {
                assert processArgs.renameKeys instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class: Map. Found: renameKeys.getClass() is ${processArgs.renameKeys.getClass()}"
                assert tuple[1] instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
                // TODO: allow renameKeys to be a function?
                processArgs.renameKeys.each { newKey, oldKey ->
                  assert newKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of newKey: String. Found: newKey.getClass() is ${newKey.getClass()}"
                  assert oldKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of oldKey: String. Found: oldKey.getClass() is ${oldKey.getClass()}"
                  assert tuple[1].containsKey(oldKey) : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Key '$oldKey' is missing in the data map. tuple[1].keySet() is '${tuple[1].keySet()}'"
                  tuple[1].put(newKey, tuple[1][oldKey])
                }
                tuple[1].keySet().removeAll(processArgs.renameKeys.collect{ newKey, oldKey -> oldKey })
              }
              tuple
            }
            | debug(processArgs, "processed")
            | map { tuple ->
              def id = tuple[0]
              def data = tuple[1]
              def passthrough = tuple.drop(2)
      
              // fetch default params from functionality
              def defaultArgs = thisFunctionality.arguments
                .findAll { it.containsKey("default") }
                .collectEntries { [ it.name, it.default ] }
      
              // fetch overrides in params
              def paramArgs = thisFunctionality.arguments
                .findAll { par ->
                  def argKey = key + "__" + par.name
                  params.containsKey(argKey) && params[argKey] != "viash_no_value"
                }
                .collectEntries { [ it.name, params[key + "__" + it.name] ] }
              
              // fetch overrides in data
              def dataArgs = thisFunctionality.arguments
                .findAll { data.containsKey(it.name) }
                .collectEntries { [ it.name, data[it.name] ] }
              
              // combine params
              def combinedArgs = defaultArgs + paramArgs + processArgs.args + dataArgs
      
              // remove arguments with explicit null values
              combinedArgs.removeAll{it == null}
      
              // check whether required arguments exist
              thisFunctionality.arguments
                .forEach { par ->
                  if (par.required) {
                    assert combinedArgs.containsKey(par.name): "Argument ${par.name} is required but does not have a value"
                  }
                }
      
              // TODO: check whether parameters have the right type
      
              // process input files separately
              def inputPaths = thisFunctionality.arguments
                .findAll { it.type == "file" && it.direction == "input" }
                .collect { par ->
                  def val = combinedArgs.containsKey(par.name) ? combinedArgs[par.name] : []
                  def inputFiles = []
                  if (val == null) {
                    inputFiles = []
                  } else if (val instanceof List) {
                    inputFiles = val
                  } else if (val instanceof Path) {
                    inputFiles = [ val ]
                  } else {
                    inputFiles = []
                  }
                  // throw error when an input file doesn't exist
                  inputFiles.each{ file -> 
                    assert file.exists() :
                      "Error in module '${key}' id '${id}' argument '${par.name}'.\n" +
                      "  Required input file does not exist.\n" +
                      "  Path: '$file'.\n" +
                      "  Expected input file to exist"
                  }
                  inputFiles 
                } 
      
              // remove input files
              def argsExclInputFiles = thisFunctionality.arguments
                .findAll { it.type != "file" || it.direction != "input" }
                .collectEntries { par ->
                  def parName = par.name
                  def val = combinedArgs[parName]
                  if (par.multiple && val instanceof Collection) {
                    val = val.join(par.multiple_sep)
                  }
                  if (par.direction == "output" && par.type == "file") {
                    val = val.replaceAll('\\$id', id).replaceAll('\\$key', key)
                  }
                  [parName, val]
                }
      
              [ id ] + inputPaths + [ argsExclInputFiles, passthrough, resourcesDir ]
            }
            | processObj
            | map { output ->
              def outputFiles = thisFunctionality.arguments
                .findAll { it.type == "file" && it.direction == "output" }
                .indexed()
                .collectEntries{ index, par ->
                  out = output[index + 2]
                  // strip dummy '.exitcode' file from output (see nextflow-io/nextflow#2678)
                  if (!out instanceof List || out.size() <= 1) {
                    if (par.multiple) {
                      out = []
                    } else {
                      assert !par.required :
                          "Error in module '${key}' id '${output[0]}' argument '${par.name}'.\n" +
                          "  Required output file is missing"
                      out = null
                    }
                  } else if (out.size() == 2 && !par.multiple) {
                    out = out[1]
                  } else {
                    out = out.drop(1)
                  }
                  [ par.name, out ]
                }
              
              // drop null outputs
              outputFiles.removeAll{it.value == null}
      
              if (processArgs.auto.simplifyOutput && outputFiles.size() == 1) {
                outputFiles = outputFiles.values()[0]
              }
      
              def out = [ output[0], outputFiles ]
      
              // passthrough additional items
              if (output[1]) {
                out.addAll(output[1])
              }
      
              out
            }
            | debug(processArgs, "output")
      
          emit:
          output_
        }
      
        def wf = workflowInstance.cloneWithName(workflowKey)
      
        // add factory function
        wf.metaClass.run = { runArgs ->
          workflowFactory(runArgs)
        }
      
        return wf
      }
      
      // initialise default workflow
      myWfInstance = workflowFactory([:])
      
      // add workflow to environment
      ScriptMeta.current().addDefinition(myWfInstance)
      
      // anonymous workflow for running this module as a standalone
      workflow {
        if (params.containsKey("help") && params["help"]) {
          exit 0, thisHelpMessage
        }
        if (!params.containsKey("id")) {
          params.id = "run"
        }
        if (!params.containsKey("publishDir")) {
          params.publishDir = "./"
        }
      
        // fetch parameters
        def args = thisFunctionality.arguments
          .findAll { par -> params.containsKey(par.name) }
          .collectEntries { par ->
            if (par.multiple) {
              par_data = params[par.name]
              if (par_data instanceof List) {
                par_data = par_data.collect{it.split(par.multiple_sep)}.flatten()
              } else {
                par_data = par_data.split(par.multiple_sep)
              }
              // todo: does this work for non-strings?
            } else {
              par_data = [ params[par.name] ]
            }
            if (par.type == "file" && par.direction == "input") {
              par_data = par_data.collect{file(it)}.flatten()
            }
            if (!par.multiple) {
              assert par_data.size() == 1 : 
                "Error: argument ${par.name} has too many values.\n" +
                "  Expected amount: 1. Found: ${par_data.length}"
              par_data = par_data[0]
            }
            [ par.name, par_data ]
          }
                
        Channel.value([ params.id, args ])
          | view { "input: $it" }
          | myWfInstance.run(
            auto: [ publish: true ]
          )
          | view { "output: $it" }
      }

    dest: "main.nf"
  - type: "file"
    text: |
      manifest {
        name = 'maxquant'
        mainScript = 'main.nf'
        nextflowVersion = '!>=20.12.1-edge'
        version = 'main_build'
        description = 'Perform a MaxQuant analysis with mostly default parameters.'
        author = 'Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid: 0000-0003-3641-729X}'
      }
      
      // detect tempdir
      tempDir = java.nio.file.Paths.get(
        System.getenv('NXF_TEMP') ?:
          System.getenv('VIASH_TEMP') ?: 
          System.getenv('TEMPDIR') ?: 
          System.getenv('TMPDIR') ?: 
          '/tmp'
      ).toAbsolutePath()
      
      profiles {
        docker {
          docker.enabled         = true
          docker.userEmulation   = true
          docker.temp            = tempDir
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        singularity {
          singularity.enabled    = true
          singularity.autoMounts = true
          docker.enabled         = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        podman {
          podman.enabled         = true
          podman.temp            = tempDir
          docker.enabled         = false
          singularity.enabled    = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        shifter {
          shifter.enabled        = true
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          charliecloud.enabled   = false
        }
        charliecloud {
          charliecloud.enabled   = true
          charliecloud.temp      = tempDir
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
        }
      }

    dest: "nextflow.config"
  - type: "file"
    path: "settings"
  description: "Perform a MaxQuant analysis with mostly default parameters."
  usage: "maxquant --input file1.raw --input file2.raw --reference ref.fasta --output\
    \ out/"
  tests: []
  info: {}
  dummy_arguments: []
  set_wd_to_resources_dir: false
  enabled: true
platform:
  type: "nextflow"
  id: "nextflow"
  variant: "vdsl3"
  directives:
    accelerator: {}
    conda: []
    containerOptions: []
    label:
    - "highmem"
    - "highcpu"
    module: []
    pod: []
    publishDir: []
    queue: []
  auto:
    simplifyInput: true
    simplifyOutput: true
    transcript: false
    publish: false
  debug: false
  container: "docker"
platforms: []
info:
  config: "src/maxquant/maxquant/config.vsh.yaml"
  platform: "nextflow"
  output: "target/nextflow/maxquant/maxquant"
  viash_version: "0.5.13"
  git_commit: "65f07e027d6237818a16ccc7c0c9ff56df3c5e53"
  git_remote: "https://github.com/czbiohub/mspipelines"
